{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to MkDocs\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n\n\n\n12 factor apps\n\n\n12factor.net\n\n\n8 Fallacies of Distributed Systems\n\n\nUnderstanding the 8 Fallacies of Distributed Systems \n\n\noriginal pdf\n\n\nThe network is reliable\n\n\nit is not, and packets could be lost, so there is need of Error handling\n\nand retries after the packet lost and error occured\\connection loss\n\n\nLatency is zero\n\n\nlatency is a thing, and computers are not close to each other. Sometimes things could be mirrored in different regions.\n\nNumber of network requests need to be as little as possible, to minimize latency timings - the less requests goes through the network with unknown latency, the faster overall things would be\n\n\nBadnwidth is infinite\n\n\nBandwidth (channel width) is limited, and huge amount of data will take longer to transfer.\n\nAlso when service gets requests from lots of clients, some clients could send lots of requests which will be transformed in network being to busy, so other clients will get no space.\n\nIn this case some throtelling should be applied, and such clients at some moment need to be denied, so there will be network space for others to made a request.\nSend small payloads - do not send huge packets with requests, send small ones so they will get through physicall wire faster, kind of\n\n\nThe network is secure\n\n\nNetwork inside even a single cluster is Insecure, so even the services located in secured network should use authentication.\n\nEspecially in case when services are running some 3rd party code or libraries or stuff like that, which could send some requests over the network, when they should not be allowed to.\n\n\nTopology does not change\n\n\nOrchestrator (tool which will keep all the services ap and running, restarted and scaled) could change topology - some services could be spawned eleswhere, on the different pc, rack or even availability zone, this will affect the Topology\n\n\nThere is one administrator\n\n\nThere are lots of administrators - people and automated tools\\scripts, so something that is considered by you as Immutable (eg. IP:port) could be changed by someone w/o your knowlege.\n\nThings like that need to be monitored, and all the updates should be visible and all the notifications or even automated changes should be done.\n\n\nTransport cost is zero\n\n\nUsually transporting traffic in and out of datacenter costs a performance.\n\nAlso transport could cost money, as datacenters could charge users for that metric\n\n\nThe network is homogeneous\n\n\nThe network could consist of different hardware used such as routers, switches(manufacturer or model or age) and even the wire - fibre or copper etc.\n\nThis could be applied to datacenter itself, or segment between datacenters where user's services are mirrorred, or anywhere on the way\n\n\nMicroservices\n\n\nTo use a mircoservices we need to convert usual methods into network calls,\n\n together with parameters of the methods, which also need to be [de]serializable\n\nand result , returned by the method also need to be serializable.\n\n\nvar result = Method(arg1, arg2);\n\n\n\n\n\nresult\n and \nargs\n need to be serialized and deserialized byt client and server\n\nMethod need to be Explicit, Language agnostic, multiversion API contract.   \n\n\n\n\nWhich means name would be used on different sides(server and client), in different languages, and whole the signature will require lots of changes on both sides.  \n\n\nAlso the method signature will not be shown in IntelliSense anymore because it is not in referenced library anymore, it is just simple Network call (which could be wrapped into library but this will require extra work)\n\n\nSerializable narrows amount of Types we could use (only serializable ones),\n\n and the process of serialization of language-specific object into serializable data type, such as JSON, XML, Avro, YAML, so on, takes CPU and RAM\n\n\nSometimes serialization of types, especially if they support reflection, could use 90+% of the resources used by the service\\app.\n\n\nInprocess method call vs Network request\n\n\n\n\n\n\nPerformance - Worse:\n\nIncreases network congestion, means that it adds to network traffic\n\nUnpredictable timing, in-process call could take microseconds, and Network call could take seconds and could even fail on timeout - need to expect it and be ready to handle.\n\n\n\n\n\n\nUnreliable:\n\nRequires retries, timeouts and circuit breakers.\n\nCircuit breaker is when client tries to communicate with a server, and if it fails to connect\\communicate for X consecutive times(e.g. 5) it will no longer try for Y amount of time, or otherwise it will became a DDoS attacker of own server. This loop(try 5 times, wait 1 minute) will continute until server is available again - then circuit breaker will exit and allow traffic to move on.\n\nCircuit breaker - client logic mostly\n\nBut server also need to count whether it answered, and if yes - do not send same answers for requests made because of circuit breaker still not exited and sending more requests, that already have been answered\n\n\n\n\n\n\nSecurity:\n\nWhen method was in a single process, there was no need of security, authentication and encryption.\n\nBut now Parameters and Results are sent over the network, those need to be encrypted\n\nNow anyone could call this Method, which become Network call, so only authorized clients need to be served\n\n\n\n\n\n\nDiagnostics:\n\nCalls are now are done over the network, instead of being in-process before, so there is latency now.\n\nEvents and logs are now generated by all the tools and apps, like web servers, proxies, encryptors, virtual machines.\n\nCall stack is now split over the network and potentially lots of machines, you cant just view it in Debug mode like before, or go to Method implementation by F12(F11 in VS?)\n\nClocks are not fully synchorized, use UTC in the cluster. \n\nBut even with same UTC some machines could slow down or pace too fast, so there could be situation when Client sent a request at 00.01 by clien's time and server received it by 23.59 by server's time.\n\nSo during logs examination this need to be taken into account.\n\n\n\n\n\n\nIdempotent operation\n\n\nOperation which could be performed 2+ times witn no ill effect.\n\n\nExample:\n\n\n\n\nMethod call to the server, with an image as a parameter, and the server produces thumbnail(preview of the image) and returns it to the client.\n\nThis oparation could be performed 2+ times with the same exact image and the server will return the same exact thumbnail every time. Without any \nill\n effect.\n\n\n\n\nServices MUST implement operations idempotently\n\n\nServers need to make sure that operation is idempotent.\n\n\nExample:\n\n\n\n\nMethod adds $100 to an account by creating a request, it does this in a circuit breaker loop, but after server accepts the call, method sends another one. If server will accept all such calls - account will have more than $100.\n\nSo server should check whether the call was processed, and if so - deny all the same calls being done afterwards.\n\nWhich will be a State corruption.\n\n\n\n\nSo the Client need to send the requests in loop manner, to make sure message is sent and received.\n\n\nThe Server need to make sure idempotentcy - so every request received from the Client will be idempotent - Client could call method 2+ times \nwith no ill effect\n\n\nIdempotant CRUD operations\n\n\n\n\n\n\n\n\n\n\nOperation\n\n\nHTTP Verb\n\n\nWhat to do\n\n\n\n\n\n\n\n\n\n\nC\n\n\nid = Create()\n\n\nPOST\n\n\nsee pattern below\n\n\n\n\n\n\nR\n\n\ndata = Read(id)\n\n\nGET/HEAD/OPTIONS/TRACE\n\n\nNaturally idempotent\n\n\n\n\n\n\nU\n\n\nUpdate(id, data)\n\n\nPUT\n\n\nLast writer wins\n\n\n\n\n\n\nD\n\n\nDelete(id)\n\n\nDELETE\n\n\nif already done OK\n\n\n\n\n\n\n\n\nIdempotency pattern:\n\n\n\n\n\n\nClient: asks server to create unique ID \nor\n client(\ntrusted\n) creates ab ID\n\n\n\n\n\n\nsome unique ID like GUID is created by the server on clients request, this is more trustfull way, but adds network latency.\n\nClient could create ID on its own, w/o network communication, but it needs to be Unique, so Server need to \nTrust\n the client on that matter.\n\nThis could be retried, but only last ID matters, all other previous IDs should be thrown away\n\n\n\n\n\n\nClient: sends ID & desired operation to server\n\n\n\n\n\n\nClient then sends POST asking Server to add $100 to the account.\n\nThis also could be retried, as we are talking about idemtotency here\n\n\n\n\n\n\nServer: if ID is new, do the operation and log the ID, respond OK\n\n\n\n\n\n\nMust be transactioned(atomic) operation\n\nThis is important to keep idempotency: if Client sends operation +$100, and Server checked in the Log for the ID, start doing the operation, and Client sends the retry - Server will start 2nd operation, because 1st is still ongoing and ID is not yet logged.\n\nOn the other hand if Search and then Log the ID, server could crash on the execution of the operation, and will not perform the operation at all.\n\nSo the atomic 'Search-for-ID; Perform-operation; Log-the-ID' operation is important. Otherwise it will not guaranteed to be \nIdempotent\n.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-mkdocs",
            "text": "For full documentation visit  mkdocs.org .",
            "title": "Welcome to MkDocs"
        },
        {
            "location": "/#commands",
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.",
            "title": "Commands"
        },
        {
            "location": "/#project-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "Project layout"
        },
        {
            "location": "/#12-factor-apps",
            "text": "12factor.net",
            "title": "12 factor apps"
        },
        {
            "location": "/#8-fallacies-of-distributed-systems",
            "text": "Understanding the 8 Fallacies of Distributed Systems   original pdf",
            "title": "8 Fallacies of Distributed Systems"
        },
        {
            "location": "/#the-network-is-reliable",
            "text": "it is not, and packets could be lost, so there is need of Error handling \nand retries after the packet lost and error occured\\connection loss",
            "title": "The network is reliable"
        },
        {
            "location": "/#latency-is-zero",
            "text": "latency is a thing, and computers are not close to each other. Sometimes things could be mirrored in different regions. \nNumber of network requests need to be as little as possible, to minimize latency timings - the less requests goes through the network with unknown latency, the faster overall things would be",
            "title": "Latency is zero"
        },
        {
            "location": "/#badnwidth-is-infinite",
            "text": "Bandwidth (channel width) is limited, and huge amount of data will take longer to transfer. \nAlso when service gets requests from lots of clients, some clients could send lots of requests which will be transformed in network being to busy, so other clients will get no space. \nIn this case some throtelling should be applied, and such clients at some moment need to be denied, so there will be network space for others to made a request.\nSend small payloads - do not send huge packets with requests, send small ones so they will get through physicall wire faster, kind of",
            "title": "Badnwidth is infinite"
        },
        {
            "location": "/#the-network-is-secure",
            "text": "Network inside even a single cluster is Insecure, so even the services located in secured network should use authentication. \nEspecially in case when services are running some 3rd party code or libraries or stuff like that, which could send some requests over the network, when they should not be allowed to.",
            "title": "The network is secure"
        },
        {
            "location": "/#topology-does-not-change",
            "text": "Orchestrator (tool which will keep all the services ap and running, restarted and scaled) could change topology - some services could be spawned eleswhere, on the different pc, rack or even availability zone, this will affect the Topology",
            "title": "Topology does not change"
        },
        {
            "location": "/#there-is-one-administrator",
            "text": "There are lots of administrators - people and automated tools\\scripts, so something that is considered by you as Immutable (eg. IP:port) could be changed by someone w/o your knowlege. \nThings like that need to be monitored, and all the updates should be visible and all the notifications or even automated changes should be done.",
            "title": "There is one administrator"
        },
        {
            "location": "/#transport-cost-is-zero",
            "text": "Usually transporting traffic in and out of datacenter costs a performance. \nAlso transport could cost money, as datacenters could charge users for that metric",
            "title": "Transport cost is zero"
        },
        {
            "location": "/#the-network-is-homogeneous",
            "text": "The network could consist of different hardware used such as routers, switches(manufacturer or model or age) and even the wire - fibre or copper etc. \nThis could be applied to datacenter itself, or segment between datacenters where user's services are mirrorred, or anywhere on the way",
            "title": "The network is homogeneous"
        },
        {
            "location": "/#microservices",
            "text": "To use a mircoservices we need to convert usual methods into network calls, \n together with parameters of the methods, which also need to be [de]serializable \nand result , returned by the method also need to be serializable.  var result = Method(arg1, arg2);   result  and  args  need to be serialized and deserialized byt client and server \nMethod need to be Explicit, Language agnostic, multiversion API contract.      Which means name would be used on different sides(server and client), in different languages, and whole the signature will require lots of changes on both sides.    Also the method signature will not be shown in IntelliSense anymore because it is not in referenced library anymore, it is just simple Network call (which could be wrapped into library but this will require extra work)  Serializable narrows amount of Types we could use (only serializable ones), \n and the process of serialization of language-specific object into serializable data type, such as JSON, XML, Avro, YAML, so on, takes CPU and RAM  Sometimes serialization of types, especially if they support reflection, could use 90+% of the resources used by the service\\app.",
            "title": "Microservices"
        },
        {
            "location": "/#inprocess-method-call-vs-network-request",
            "text": "Performance - Worse: \nIncreases network congestion, means that it adds to network traffic \nUnpredictable timing, in-process call could take microseconds, and Network call could take seconds and could even fail on timeout - need to expect it and be ready to handle.    Unreliable: \nRequires retries, timeouts and circuit breakers. \nCircuit breaker is when client tries to communicate with a server, and if it fails to connect\\communicate for X consecutive times(e.g. 5) it will no longer try for Y amount of time, or otherwise it will became a DDoS attacker of own server. This loop(try 5 times, wait 1 minute) will continute until server is available again - then circuit breaker will exit and allow traffic to move on. \nCircuit breaker - client logic mostly \nBut server also need to count whether it answered, and if yes - do not send same answers for requests made because of circuit breaker still not exited and sending more requests, that already have been answered    Security: \nWhen method was in a single process, there was no need of security, authentication and encryption. \nBut now Parameters and Results are sent over the network, those need to be encrypted \nNow anyone could call this Method, which become Network call, so only authorized clients need to be served    Diagnostics: \nCalls are now are done over the network, instead of being in-process before, so there is latency now. \nEvents and logs are now generated by all the tools and apps, like web servers, proxies, encryptors, virtual machines. \nCall stack is now split over the network and potentially lots of machines, you cant just view it in Debug mode like before, or go to Method implementation by F12(F11 in VS?) \nClocks are not fully synchorized, use UTC in the cluster.  \nBut even with same UTC some machines could slow down or pace too fast, so there could be situation when Client sent a request at 00.01 by clien's time and server received it by 23.59 by server's time. \nSo during logs examination this need to be taken into account.",
            "title": "Inprocess method call vs Network request"
        },
        {
            "location": "/#idempotent-operation",
            "text": "Operation which could be performed 2+ times witn no ill effect.  Example:   Method call to the server, with an image as a parameter, and the server produces thumbnail(preview of the image) and returns it to the client. \nThis oparation could be performed 2+ times with the same exact image and the server will return the same exact thumbnail every time. Without any  ill  effect.   Services MUST implement operations idempotently  Servers need to make sure that operation is idempotent.  Example:   Method adds $100 to an account by creating a request, it does this in a circuit breaker loop, but after server accepts the call, method sends another one. If server will accept all such calls - account will have more than $100. \nSo server should check whether the call was processed, and if so - deny all the same calls being done afterwards. \nWhich will be a State corruption.   So the Client need to send the requests in loop manner, to make sure message is sent and received.  The Server need to make sure idempotentcy - so every request received from the Client will be idempotent - Client could call method 2+ times  with no ill effect",
            "title": "Idempotent operation"
        },
        {
            "location": "/#idempotant-crud-operations",
            "text": "Operation  HTTP Verb  What to do      C  id = Create()  POST  see pattern below    R  data = Read(id)  GET/HEAD/OPTIONS/TRACE  Naturally idempotent    U  Update(id, data)  PUT  Last writer wins    D  Delete(id)  DELETE  if already done OK     Idempotency pattern:    Client: asks server to create unique ID  or  client( trusted ) creates ab ID    some unique ID like GUID is created by the server on clients request, this is more trustfull way, but adds network latency. \nClient could create ID on its own, w/o network communication, but it needs to be Unique, so Server need to  Trust  the client on that matter. \nThis could be retried, but only last ID matters, all other previous IDs should be thrown away    Client: sends ID & desired operation to server    Client then sends POST asking Server to add $100 to the account. \nThis also could be retried, as we are talking about idemtotency here    Server: if ID is new, do the operation and log the ID, respond OK    Must be transactioned(atomic) operation \nThis is important to keep idempotency: if Client sends operation +$100, and Server checked in the Log for the ID, start doing the operation, and Client sends the retry - Server will start 2nd operation, because 1st is still ongoing and ID is not yet logged. \nOn the other hand if Search and then Log the ID, server could crash on the execution of the operation, and will not perform the operation at all. \nSo the atomic 'Search-for-ID; Perform-operation; Log-the-ID' operation is important. Otherwise it will not guaranteed to be  Idempotent .",
            "title": "Idempotant CRUD operations"
        },
        {
            "location": "/bash_page/",
            "text": "Intro\n\n\nscripts start from instruction which interpreter to use, first line always must\nbe the directive. Except if script is passed as param into interpreter, then\nthere is no need in that because there already an interpreter in place.\n\n\n#!/path/to/interpreter\n\n\n\ni.e.:\n\n\n#!/bin/bash\n\n\n\nShortcuts:\n  ctrl+a - move to start of line\n  ctrl+e - move to end of line\n  ctrl+b - move one char back\n  alt+b  - move one word back\n  ctrl+f - move one char forth\n  alt+b  - move one word forth\n  ctrl+d - delete one char under cursor\n  ctrl+u - delete from cursor to line start\n  ctrl+k - delete from cursor to line end\n  ctrl+w - delete from cursor to word start\n  alt+bcksp - delete previous word\n  ctrl+y - paste from clipboard(alt+bckspc deleted)\n  ctrl+l - clean screen\n  ctrl+r - reverse search in history\n  ctrl+j - edit found command in search\n  ctrl+p - previous command in history\n  ctrl+n - next command in history\n\n\nBash Scripts accepts data from stdin and its direct redirection or pipe\n\nNote: 'read' command need to be used.  \n\n\nExample: \n\n\n>script:  \n>read -p \"enter any *.txt\"  \n>echo \"${PWD}${REPLY}\"  \n$cd ~ && ls test.txt | ./read_and_expect_txt_files.sh  \n/home/dos/test.txt\n\n\n\nExample:  \n\n\n$echo \"test.txt\" > try.txt  \n$./read_and_expect_txt_files.sh < try.txt  \n/home/dos/test.txt\n\n\n\nDebugging:\n\n\n#!/bin/bash -x\n\n\n\nwill start debugging tracert - with display of all values expanded and\n  lines as bash sees them and will execute them\n\n\nset -x  \necho $var  \nset +x\n\n\n\nwill enable debugging tracert only between set -/+ x displaying only\n commands in between with such debug info\n\n\nshell variables:\n\n\n$FUNCNAME - contains name of the function being executed(like reflection)  \n\n\n$PS4 env var, is Promt String for debugging tracert\n\n$LINENO - is standard variable with line number\n\nPS4=\"$LINENO +\" - will display line number during debugging tracert  \n\n\n$OLDPWD - contains previous workind directory\n\n$RANDOM - contains random number from 1 to 32 767\n\n$$ - current PID of the programs\\script being executed  \n\n\nDefensive programming:\n\n\nMake sure everything is expanded and is executed in right place:\n\n Instead of:  \n\n\n$cd $path  \n$rm *\n\n\n\nUse secure defensive way:  \n\n\n[[ -d \"$path\" ]] && cd \"$path\" && echo rm *\n\n\n\nwhere:  \n\n\n\n\n\n\n-d - make sure path exists and is Directory  \n\n\n\"$path\" - in double quotes to avoid null expansion if var does not exist\n    it will be expanded in empty string in that way - \"\"  \n\n\n&& - will execute next command if previous returned 0 exit code  \n\n\necho rm * - will echo expanded by bash shell wildcard, displaying what\n\n    exactly is going to happen with actual 'rm *'  \n\n\n\n\n\n\nNaming Coventions:\n\n\n\n\nVariable Names:\n\n Lower-case, with underscores to separate words.\n\n Ex: my_variable_name  \n\n\nConstants and Environment Variable Names:\n\n All caps, separated with underscores, declared at the top of the file.\n\n Ex: MY_CONSTANT  \n\n\nGoogle naming convetions:  \n\n    https://google.github.io/styleguide/shell.xml#Naming_Conventions\n\n\n\n\nExecution:\n\n\nbash searches for entered command in \n\n\n\n\nbuilt-ins\n\n\nhashes\n\n\nbash history \n\n\nenv var PATH\n\n   4.1. PATH is split by ':' and every path is searched for the command  \n\n\nnew child process of bash created - fork(), and all ENV copied into it\n\n\nfound command is executed in the child copy of bash and replaces it - execve()  \n\n\nmain bash process executes wait() until the child process ends\n\n\nafter child process ends cleanup starts and destroys the process \n\n\n\n\nSo if script/binary is executed from some(i.e. current) directory and it is\n  not in path , it have to be 'source'd like:  \n\n\n source scriptname\n\n\n\n\n\nor:  \n\n\n\n\n . scriptname\n\n\n\n. is same to source    \n\n\n\n\nor:  \n\n\n\n\n ./scriptname\n\n\n\nthis seems to be \n/scriptname (so basically full path, which is also applies)  \n\n\nExpansion:\n\n\nExpansion is done by shell(i.e. bash) and expanded result is passed to shell\\command to be executed. If something is escaped by escape sequence, it will not be expanded by shell, and \ncould\n be expanded by the program receiving escaped sequence.\n\n expansion works before pass anything further, so:  \n\n\n cp * /some/dir\n\n\n\nwill copy everything from current dir into /some/dir\n\n  first argument * actually will be passed to cp as list of files matched \n  by wildcard, 'cp' will never see * itself\n\n  to check what * will return, execute \necho *\n  \n\n\nvariables expansion:\n\n\nExpansion starts when $ is encountered.\n\n  Escape sequences:\n\n  \\ - will escape 1 symbol to treat it as literal and not meta symbol\n\n  '' - everything between single quotes threated as literals too\n\n  \"\" - will expand variables, but escape spaces etc.\n\n   Variables are expanded before passing them to command so command will only\n  see var's value, not its name.\n\n   Bash creates variable when first mets it, so it will create even empty\n  variables, which could lead to errors:  \n\n\nfoo=\"/some/file\"  \nmv $foo $foo1  # error\n\n\n\n\n\nBash will create $foo1 variable , then mv will receive $foo1 value which\n  is null, and 'mv /some/file \n' will get error as only 1 param is passed\n\n   To rename file to \n'some/file1'\n use \n{}\n:  \n\n\n\n\n    mv $foo {$foo}1\n\n\n\nPositional parameters:\n\n\nBash supports positional params for the script.\n\n  default ones are $0-$9 , other params are listed by ${10} and so on:  \n\n\n\n\n\n\n$0 ---- contains full path of the script being executed\n\n----    does not count as parameter in list $#\n\n----    so [ $# -gt 0 ] will return false if $1 is empty  \n\n\n\n\n\n\n$1+  ---- are actual parameters  \n\n\n\n\n\n\nParameters are passed as usual\n\n\nExample:  \n\n\nscript.sh param1 param2\n\n\n\nin script.sh:  \n\n\necho $0  #/path/to/script.sh\necho $1  #param1\necho $2  #$param2\necho $3  # this would be empty uninitialized param with null value\n\n\n\nParaters could be passed using wildcards like\n\n\nExample:  \n\n\nscript.sh *\n\n\n\nwill pass everything matched, basically it would be a\n        list of files in PWD (same as \necho *\n)\n\n\nParams Arrays:\n\n\n\n\n$# - \n.Count\n, stores number of arguments passed into the script\n\n    does \nnot count $0\n in it, so is not 0 based\n\n\n$* - stores all arguments - ($1 $2 ...), if args has spaces those\n\n    would be treated as separate params, so params \n\n    (\"param1\" \"param with spaces\") would be\n\n\n$1=param1 $2=param $3=with $4=spaces\n  \n\n\n\"$*\" - all params expanded between (\"$1 $2\" ...), with delimiter of first\n    symbol in $IFS variable, it becomes one single parameter:\n\n    (\"param1\" \"param with spaces\") would be\n\n\n$1=\"param1 param with spaces\"\n (leaving quotes behind in the end)  \n\n\n$@ - same as $*  \n\n\n\"$@\" - stores all arguments in quotas (\"$1\" \"$2\" ...), thus handling \n    spaces in variables.\n\n    (\"param1\" \"param with spaces\") would be\n\n\n$1=\"param1\" $2=\"param with spaces\"\n \n\n    See 'variable substitution' for more use - ${@:2} (w/o \" works same)  \n\n\n\n\nshift:\n\n\nshifts params backwards so $2 would be $1 and old $1 would vanish\n  using this command each parameter could be handled one by one\n\n   Example:  \n\n\nwhile [[ $# -gt 0 ]]; do\n echo \"Parameter is $1\"\n shift\ndone\n\n\n\nThis will echo out all parameters given as per shift will consequentially\n  move all parameters into $1 position one by one\n\n\nbasename:\n\n\nreturns file name from latest node in the path\n\n   Example:  \n\n\n$/home/dos/scripts/some_script.sh\nin script >> echo basename $0\nwill return 'some_script.sh'\n\n\n\ndirname:\n\n\nreturns directory name (everything except latest nodein the path)\n\n  antipod of \nbasename\n\n   Example:\n\n\ncd /home/dos/directory/filename\ndirname $PWD\n> /home/dos/directory\n\n\n\nvariables:\n\n\nAKA Global Variables - are visible everywhere in shell environment\n   if 'export'ed - will be visible in subshells created from the shell\n\n   'local' variables are local for funcions - see below\n\n  is converted to 'String' class by default\n\n  Initializate variable by = sign w/o leading or trailing spaces\n\n  Access variable by $  \n\n\nAlso could be int, if used with 'declare':  \n\n\ndeclare -i variable=1\n\n\n\nCOuld be readonly:  \n\n\ndeclare -r READONLY=\"this is constant, in uppercase by convention\"\n\n\n\nVariables definitions:  \n\n\na=z                     # assign string \"z\" to var 'a'\nb=\"a string\"            # assign \"a string\" w/ space to var 'b'\nc=\"a string and $b\"     # expansion works in \"\" so 'c' value \n                        # is \"a string and a string\"\nd=$(ls -l foo.txt)      # result of command is assigned to 'd'\ne=$((5 * 7))            # result of ariphmetic result is assigned to 'e'\nf=\"\\t\\tastring \\n\"      # escaped sequences also works in \"\" here:\n                        # \\t - tab\n                        # \\n - new line\n\n\n\nMore than one variable could be declared in 1 line:  \n\n\na=5 b=\"a string\" # will create two variables with\n                 # values \"5\" and \"a string\", probably 5 is int.ToString()\n\n\n\nTo preserve spaces and stuff use quotas with string vars initialization\n\n   Example:  \n\n\nmy_str_var=\"string w/ spaces goes in quotas\"\necho \"stuff is: $my_str_var\"   # stuff is: string w/ spa.....\necho 'stuff is: $my_str_var'   # stuff is: $my_str_var\n\n\n\narrays:\n\n\nalso variables, indexed from 0, \n\n  could have empty indexes.\n\n   Example declaration:  \n\n\na[10]=Ten   # assign 'Ten' to 11th index in array length of 1\n\n\n\nPowershell-like thing(or vice versa), except w/o @   \n\n\ndays=(Mon Tue Wed Thu Fri Sat Sun)\ndays=([0]=Mon [1]=Tue ... )    # same as above but with strict indexing\nanimals=(\"a dog\" \"a cat\" \"a cow\")\n\n\n\nIf array addressed w/o index, index 0 is displayed, if it is null then nothing\n  displayed EVEN IF THERE ARE OTHER INDEXES\n\n   Example:  \n\n\narr=([1]=one [2]=two)\necho $arr       # nothing returned\narr[14]=14\necho $arr       # nothing returned\narr[0]=0\necho $arr       # 0 is returend\necho ${arr[@]}  # 0 one two 14 is returned\n\n\n\nExample2: assignment of array w/o index specification  \n\n\narr=(a b c d)\necho ${arr[@])  # a b c d - is returned\narr=AAA\necho ${arr[@]}  # AAA b c d - is returned\necho $arr       # AAA - is returned\n\n\n\nExample3: deletion of first index w/o index specification  \n\n\narr=(a b c d)\narr=\necho ${arr[@]}  # b c d - is returned\n\n\n\nIteration:\n\n\nArray variable substitution(see below for strings) arrays could be iterated\n  and counted and checked for empty indexes:\n\n   Iterate array:  \n\n\n${animals[*]} # returns list of values split by space\n${animals[@]} # same as above\n\n\n\nExample:  \n\n\narr3=(\"a first\" \"a second\" 'a third')      # \" or ' is fine\nfor i in ${arr3[*]}; do echo $i ; done\n\n>a          # separate variables\n>first      # also separated by spaces(split)\n>a          # will work fine if no Spaces\n>second\n>a\n>third\n\n\n\n\"${animals[*]}\" - JOIN joins all elements into 1 string\n\n Example:\n\n\n for i in \"${arr3[*]}\"; do echo $i ; done   # only \" , ' will not work\n >a first a second a third                  #joined string\n\n\n\nTODO: CONTINUE FROM HERE\n\n\nBest form to use:\n  \"${animals[@]}\" - returns list of values NOT split by space(real contents)\n    Example:\n   for i in \"${arr3[@]}\"; do echo $i; done\n\n\n\n\na first     # only different values are separated\na second        # spaces are in place\na third     # best form to use\n\n\n\n\nCounting:\n  Array count coun be calculated, and length of value in single index:\n\n\n.Count:\n  ${#array[@]}   \n\n   where:\n  array is var name\n   Example:\n  echo ${#arr3[@]}\n\n\n\n\n3            # there 3 elements 'a first' 'a second' 'a third'\n\n\n\n\narray[i].Length:\n  ${#array[i]}   \n\n   where:\n  array is var name and i is index to measure length of\n   Example:\n  echo ${#arr3[1]}\n\n\n\n\n8            # 'a first' is 8 characters long\n\n\n\n\nCheck indexes numbers:\n   Arrays could have uninitialized or null initialized elements under some\n  indexes.\n   The way of how to check which indexes are taken is:\n  ${!array[@]}      # works same w/ and w/o double quotes\n  ${!array[\n]}      # works same as above w/o quotes, \n            # but differently w/ quote, see example below in NOTE:\n   Example:\n  arr=([1]=\"a one\" [3]=\"a three\" [5]=\"a five\")\n  for i in ${!arr[@]}; do echo $i; done\n\n           \"${!arr[@]}\"         # will do the same\n           ${!arr[\n]}           # will do the same\n\n\n\n\n1\n3\n5\n\n\n\n\nNOTE: @ works same With or WithOut quotes:\n        ${!arr[@]} == \"${!arr[@]}\n   AND\n        * works same as above WithOut quotes\n   BUT\n\n        in double quotes it will JOIN indexes in one string with spaces separators\n   Example:\n  for i in \"${!arr[*]}\"; do echo $i; done\n\n\n\n\n1 3 5                # produces one line of indexes\n\n\n\n\narray.Add:\n  Arrays could have new elements added into the end of it.\n This is made as in Powershell(or powershell take it from bash) using +=\n  Example:\n arr=(a b c)        # echo \"${arr[@]}\" > a b c\n arr+=(f e g)       # echo \"${arr[@]}\" > a b c f e g\n\n\narray.Sort:\n  There is no default sort method, but it could be implemented with\n piping and regular 'sort' command, but with Iteration:\n  arr_sorted=$(for i in \"${arr[@]}\"; do echo $i; done | sort)\n    this will save soreted \"a b c e f g\" into arr_sorted\n   !!!! BUT THIS WILL NOT WORK    !!!\n   !!!!   echo \"${arr[@]}\" | sort !!!\n\n\nDelete array:\n  Deletion of array is done in the same way as deletion of variables\n using 'unset' command.\n DO Not Use '$' in unset.\n  Syntax:\n unset array_name\n\n\nExample:\n arr=(1 2 3)\n echo ${#arr[@]}    # check Count - returns 3\n echo ${arr[@]}     # check contents - returns 1 2 3\n unset arr      # deletes arr, NOTICE no '$' before 'arr' NAME\n echo ${#arr[@]}    # cehck Count - returns 0, as for anything unset\n            #+ because bash initializes on execution call\n echo ${arr[@]}     # returns nothing\n\n\nvariable substitution:\nsubstitutuin:\n   Bash supports various variables substitutions:\n  $a - will be substituted with 'a' value\n  ${a} - same as $a but could be concatenated w/ string w/o spaces:\n    ${a}.txt - will be expanded in a_value.txt\n  ${11} - 11th positional parameter given to script from shell\n  ${var:-word} - if 'variable' is set, the result will be its value\n    if 'variable' is unset - the result will be 'word'\n  $(var:=word} - if variable is set results in its value substituted\n    if variable is unset, it will be assigned to 'word'\n    such assignment will not work for positinal params(see 'shift')\n    and other special variables\n  ${var:?word} - if variable is unset error with reason 'word' will be\n    generated, exit code of such construct will be 1\n  ${var:+word} - if 'variable' is set, the result will be 'word',\n    (but variable's value will not be changed)\n    otherwise result will be EMPTY string\n  Example:\n    $ echo ${variable:-ls} - variable unset - ls used\n    > ls\n    $ export variable=1\n    $ echo ${variable:-ls} - variable is set- its value used\n    > 1\n    $ echo ${variable:+ls} - variable is set - ls used\n    > ls\n    $ echo ${variable1:+ls} - variable unset - empty line used\n    > \n  ${!prefix\n} or ${!prefix@} - returns NAMES of existing variables\n    that starts from 'prefix.\n   Example:\n    $ echo ${!BASH\n}\n    > BASH BASHOPTS BASHPID BASH_ALIASES BASH_ARGC BASH_ARGV BASH_CMDS\n\n\nstring variables substitution:\n  ${#var} - returns length of string in variable's value\n    Example:\n   $ var=123456789   #this could be interpreted as a string too now\n\n\n\n\n9           #string length is 9\n\n\n\n\n${#} or $# or ${#@} or ${#*} - returns number of positional parameters\n    of the script being executed\n\n\n${var:number} - return string from number to the end, spaces trimmed\n          variable is unchanged.\n    Example:\n   $ var=\"This string is to long.\"\n   $ echo ${var:5}         #returns string from 5th symbol\n\n\n\n\nstring is to long.\n    Example: spaces are trimmed:\n   $ echo ${var:5} | wc -c     #count chars\n   $ 19\n   $ echo ${var:4} | wc -c     #return starts from space\n   $ 19                #space is trimmed so same number of chars\n  ${var: -number} - return string from end to number, spaces trimmed\n            NOTE - space between ':' and '-' signs\n    Example:\n   $ echo ${var: -5}\nlong.\n  ${var:number:length} - return string from number till end of lenth\n    Example:\n   $ echo ${var:5:6}\nstring\n\n\n\n\n${var: -number: -length} - return string number between number(from the\n                 end) and length (also from the end)\n                 NOTE: number must be > than length\n    Example:\n   $ echo ${var: -18: -2}    #var is This string is to long.\n\n\n\n\nstring is to lon\n\n\n\n\n${@} - return all values of positional params \n     leaving spaces inside strings (like \"$@\" ) - bcs it know how \n     many arguments script has\n     ${*} is the same form, it seems\n\n\n${@:num} - displays values of positional params but from num\n     $(@:1) - works same as ${@}\n     ${@: -2} works , but starts from the end\n\n\n${@:num:length} - same as with strings but with positional params\n  ${@: -num: -length} - same as with strings but with positional params\n\n\n${param#pattern} - finds shortest match and deletes it (lazy match)\n    Example:\n   foo=\"file.txt.gz\"\n   ${foo#*.}\n\n\n\n\ntxt.gz\n  ${param##pattern} - finds longest match and deletes it (greedy match)\n   Example\n   ${foo##*.}\n.gz\n\n\n\n\n${param%pattern}  - same as #  but deletes from the end of the file\n   Example:\n   foo=file.txt.gz\n   ${foo%.\n}        - note .\n instead of *. in # example\n\n\n\n\nfile.txt\n  ${param%%pattern} - same as ##\n   ${foo%%.*}\nfile\n\n\n\n\nSearch and replace:\n\n\n\n${param/pattern/string} - replaces first occurance of pattern with string\n ${param//pattern/string} - replaces all occurances of pattern with string\n ${param/#pattern/string} - replaces only if at the beginning of the line\n ${param/%pattern/string} - replacesonly if at the end of the line\n\n\n$(())\n  Accepts any valid arithmetic expression \n   Pretty similar to (()) test construct which returns true when result >0\n Accepts any number system:\n\n\nDecimal\n   with base of 10\n  number\n   Example:\n  echo $((10)) - will return 10\n  echo $((10#10)) - will return 10\n\n\nOctal: \n   with base of 8 [0-7]\n  0number\n   Example:\n  echo $((010)) is 8 in decimal, where 10 is the whole base, which is 8\n  echo $((07)) is 7 in decimal\n  echo $((8#10)) is 8 in decimal\n  echo $((08)) - will return error 'value to great'\n        0-7; 10-17 and so on will work\n\n\nHex: Hexademical:\n   with base of 16 [0-9A-F]\n  0xnumber\n   Example:\n  $((0x10)) - is 16 in decimal, which is full base\n  $((16#10)) - is also 16, because of base of 16 numbers\n  $((16#100)) or ((0x100)) is 256 which is 16x16 or 16 in square(^2)\n  $((0xFF)) is F(15) full bases plus F(15) \n        which is ((15x15)==240)+15 == 255\n\n\nCustom:\n    custom base , could be any number [0-9A-Z] and some other]\n    maximum base is 64, seems like\n   base#number\n    Example:\n   echo $((17#10) - is 17, which is full base\n   echo $((64#10)) - is 64, which again is full base\n\n\nArithmetic expressions syntax $((arithmetics go here)):\n\n\nExample:\n\n\n\necho $((2+3)) >> 5\n\n\nIFS - variable that containes field separator, \n    by default is space/tab/new line\n    Could be changed\n\n\nTemporary variables:\n   Bash supports variables change/assignment before command execution\n  Variables assigned before command execution (on the same line) will be\n  changed only for env with which command will be executed:\n   IFS=\":\" read field1 field2 ... <<< \"$line_from_passwd\"\n    Equals to:\n   OLD_IFS=\"$IFS\"\n   IFS=\":\"\n   read field1 field2 .... <<< \"$line_from_passwd\"\n   IFS=\"$OLD_IFS\"\n  !Note:\n   it works weird - only in example from bood, need to research and update\n\n\nUnary operators:\n   Operators that require value on UNE side, contrary to binary operators\n   that reauire values on both sides:\n    + - is for positive numbes: +1\n    - - is for negative number: -1\n\n\nArithmetic operators: Binary operators:\n    there are more binary operators, but those below are used in Arithmetics\n  + - for addition\n  - - for substruction\n  * - for multiplication\n  / - for division\n  \n - for exponentiation (2\n2=4 3\n3=9 4*4=16 and so on, aka ^2 ^3 ^4 ...)\n  % - modulus division \n    5%2=1 which is what left after division - 2, 2 and 1\n      Example:\n    echo $(( 5%2 )) - will return 1\n\n\nAssignment in Arithmetic expressions:\n  echo $((foo=5))  - will assign 5 to $foo\n   NOTE: also could be used in tests like [[...]] and ((...))\n   NOTE: in 'test' or [...] used to \ncompare\n strings, BEWARE\n\n\nAssignment operators:\n   var = value \n    regular assignment operator\n   var += value \n    assignment with value addition      $var = $var + value\n   var -= value\n    assignment with value substruction  $var = $var - value\n   var *= value\n    assignment with value multiplication    $var = $var x value\n   var /= value\n    assignment with value division      $var = $var / value\n   var %= value\n    assignment with value modulus division  $var = $var % value\n\n\nAssignment with C-like style increment decrement:\n   Post - means show\\use first, change after it shown\\used\n  var++ - post increment            $var = $var + 1\n  var-- - post decrement            $var = $var - 1\n   Example:\n   foo=1; echo $((foo++)); echo $foo\n      1        2\n   Pre  - means change\\use first , show after it changed\\used\n  ++var - pre increment             $var = $var + 1\n  --var - pre decrement             $var = $var - 1\n   Example:\n   foo=1; echo $((++foo)); echo $foo\n      2        2\n\n\nBasic logical gates:\n   AND - returns true if both entry are true\n   OR - returs true if at least one (or both) entry is true \n   XOR - returns true if entry differs, \n     both true and both false return false\n   NOT - inverts entry - true is false and false is true\n   NAND - both false are true\n   NOR - returns true if at least one (or both) entry is false\n   XNOR - returns true if not differs,\n      both true or both false return true\n\n\nLogical operators in Arithmetics:\n   Comparison operators less, greater , less than and so on\n  <= - less or equal\n\n\n\n\n= - greater or equal\n  < - less\n- greater\n  == - equal\n  != - not equal\n  && - AND\n  || - OR\n\n\n\n\nexpr1:expr2?expr3 - trinary operator, \n              if expr1 is true \n              then expr2 executed\n              else expr3 is executed\n   Example:\n  a=0\n  ((a<1?++a:--1))   # a+=1 and a-=1 will not work\n  echo $a       # 1\n  ((a<1?(a+=1):(a-=1))) # but this will work just fine\n  echo $a       # 0\n\n\nlocal variables:\n  local variables are declared in functions, and visible only there\n  local variables overwrite globals with the same name\n   Syntax:\n  local variable_name[=optional_value]\n   values are optional , probably, in order to delete some global vars inside\n  a function(i.e. vars are used as standard in\\out params of a command)\n   Example:\n  foo=0\n  func() {\n   local foo\n   foo=1\n   echo \"in func $foo\"\n  }\n  func\n  echo \"in script $foo\"\n   Result:\n\n\n\n\nin func 1\nin script 0\n\n\n\n\nhere doc here docs here-document here script :\n  allows multiline input and it WILL expand even if in ''.\n   But will not expand if escaped - \\$\n   Syntax:\n  command << indicator\n  text\n  indicator\n\n\n<<-  - work as << but accepts TAB before enclosing 'indicator'\n    'space' will still return error\n\n\nwhere:\n  command is a command like, 'cat' or anything else (doesn't work with 'echo')\n  where text between 'indicators' is expanded by shell and sent to STDIN of\n  the command\n   Note that also <<- could be used, with it bash will ignore leading\n  tabs in 'indicator'\n   Note that seconds 'indicator' has to be on separate line and \n  Must have no spaces before or after it, otherwise bash interpreter will \n  conntinue to look for 'indicator' ignoring the one with leading/trailing \n  spaces\n   Value of 'indicator' could be any but 'EOF' is preffered\n\n\nExample:\n  $ foo=\"text\"\n  $ cat << \nEOF\n\n\n\n\n$foo\n\"$foo\"\n'$foo'\n\\$foo\n\nEOF\n\n   Result:\n  text\n  \"text\"\n  'text'\n  $foo\n\n\n\n\nExample of use with 'ftp'\n  ftp -n << EOF\n  open $FTP_SERVER_ADDRESS\n  user anonymous username@hostname\n  cd $FTP_PATH\n  hash\n  get $REMOTE_FILE\n  exit\n  EOF\n  ls -l $REMOTE_FILE - will display downloaded from ftp remote file, all the\n    commands are passed from inside the here-document\n\n\nhere-line:\n  same as here-doc but onliner:\n   $read field1 field2 <<< \"test in put\"\n   $echo $field1  ---> test\n   $echo $field2  ---> in put  < because put is 3rd field, and we have only 2\n                all excess fields goes into last 'field2'\n    Profid of here-line here is that commands with pipe\n   $echo \"test in put\" | read \n\n    will not work because subshell will be created for 'read' where $REPLY\n    will be created and assigned, but after that subshell ends, and\n    subshell can not change parent shell's env, such as REPLY value\n\n\nif:\n  Condition tests in IF should be inside square brackets - [], which are just a\n  reference for commant 'test'. see MAN page for TEST for details.\n   example:\n  if [ 100 -eq 100 ]; then - semicolon is needed in case of single line w/o /n\n   -or-\n  if [ 100 == 100 ] - seems like this also should work\n   -or-\n  if $(test $(echo \"$REPLY\" | grep '^[0-3]$'))\n    tests whether value of REPLY is mathced- if not, grep return 1(error)\n    then test returns 1 and 'if' will not work\n    if value matched then grep returns 0 (good), then test returns 0 and\n    'if' will work\n   -or-\n  if test $(echo \"$REPLY\" | grep '^[0-3]$')\n    same as above, but $() outside 'test' is unnecessary in fact\n    but works in both ways so keep both examples here\n   -or-\n  if [ \n ] && [ \n ] || [ \n ]\n  then\n    \n\n  elif [ \n ]      # else if, onliner gotta be ; ended\n  then\n    \n\n  else              # one liner gotta be ; ended\n    \n\n  fi\n\n\nlots info here:\n  https://ryanstutorials.net/bash-scripting-tutorial/bash-if-statements.php\n\n\n&& ||:\n  allowed inside bash shell logical AND and OR:\n   Example:\n  mkdir test && cd test\n   will create and than change dir to 'test'\n  [ -d test ] || mkdir test && cd test\n   will check whether 'test' exists, returns 0 or 1\n   || continues only if 1 was returned by 'test' and proceeds with mkdir\n   && continues only if 0 was returned by 'mkdir' and proceeds with cd\n\n\ntest:\n  Tests whether expression returns 0 or not\n  after evaluation command 'test' returns 1 if true and 0 if false\n   test 100 -eq 100 - test whether 100 equals 100\n  or [\n]\n   if [ 100 -eq 100 ]\n  used to test conditions\n\n\nLogical operators:\n   -a - AND\n   -o - OR\n   ! - NOT\n    Example:\n  [ ! ( FALSE -o TRUE )   -- (TRUE) but [ FALSE]\n   note the escaped parentheses\n   note that 'false' and 'true' command will behave differently , bcs returns\n  interegs 1 and 0 respectively\n\n\nRequires shell expansion symbols $ \n   Example:\n  if [ $num1 -lt $num2 ]; then echo 'int 1 less than int2'; fi\n\n\nBEWARE!!!!!!\n  shell creates empty variable if it is not defined\n  and $num1 -lt $num2 will return TRUE if both undefined\n\n\nCHECK YOUR VARIABLES BEFORE USE\n\n\nTest Strings\\Ints:\n  Operator  Description\n  ! \n  The EXPRESSION is false.\n  -n str    The length of STRING is greater than zero.\n  -z str    The lengh of STRING is zero (ie it is empty).\n  str1 == str2      STRING1 is equal to STRING2 (= also allowed)\n  str1 != str2      STRING1 is not equal to STRING2\n  str1 > str2       STRING1 is alphabetically greater than STRING2\n  INTEGER1 -eq INTEGER2     INTEGER1 is numerically equal to INTEGER2\n  -ne               integers are not equal\n  -le               are less or equal\n  -ge               greater or equal\n  INTEGER1 -gt INTEGER2     INTEGER1 is numerically greater than INTEGER2\n  INTEGER1 -lt INTEGER2     INTEGER1 is numerically less than INTEGER2\n\n\nTest Files:\n  FILE -ef FILE1 - both files are hard links (point to same inode)\n  FILE -nt FILE1 - FILE newer than FILE1\n  FILE -ot FILE1 - FILE older than FILE1\n  -e FILE   FILE exists.\n  -f FILE   FILE exists and is file\n  -s FILE   FILE exists and its size greater than 0\n  -L FILE   exists and is soft symblic link\n  -d FILE   FILE exists and is a directory.\n  -b FILE   exists and is Block device (i.e /dev/sda1)\n  -c FILE   exists and char device\n  -r FILE   FILE exists and the read permission is granted.\n  -s FILE   FILE exists and it's size is greater than zero (ie. not empty).\n  -w FILE   FILE exists and the write permission is granted.\n  -x FILE   FILE exists and the execute permission is granted.\n  -G FILE   exists and file belongs to existing group\n  -O FILE   exists and belongs to existing user\n\n\nNOTE:\n  == - does a string comparision\n  -eq - does numerical comparison\n\n\nEscaping:\n  symbols < > ( ) need to be escabed by \\ or be between ''\n  because [ ] is just command test, and bash will try to use them as its own \n  meta symbols (< > are stdout\\in redirecton, so could be a MESS)\n\n\n[[ ]]:\n  improved 'test' command but not POSIX compatible\n    Differences\\Features:\n\n\nDoes not need expansion symbol $, expands just fine w/o it\n\n\n\nUNLESS IS A STRING, then use \"$VAR\", to expand and prevent empty values\n    Example:\n   $ if [[ $num1 -lt num2 ]]; then echo \"$num1 less than $num2\"; fi\n\n\n\n\n1 less than 2\n    NOTE:\n   $num1 and num2 are both fine as per expansion works in some other way\n\n\n\n\nBEWARE OF UNDEFINED VARIABLES\n   IF BOTH ARE UNDEFINED 0 WILL BE RETURNED!!\n\n\nCan compare regex.\n\n\n\nSeems like gotta match full line, not just some part.\n   uses Extended Regex (ERE)\n    Requires $ symbol to expand variable value\n    use \"\" to prevent empty values\n   No need to escape bash meta symbols like \\ or *\n    Example:\n   $ if [[ \"$string_var\" =~ \\w+? ]]; then echo \"good\"; fi\n\n\n\n\ngood\n\n\n\n\nCan use wildcards , or similar to it..\n\n\n\nWhen using == equation\n    Example:\n   $ FILE=foo.txt\n   $ if [[ $FILE == foo.* ]]; then echo \"matches pattern\"; fi\n\n\n\n\nmatches pattern\n\n\n\n\nLogical operators:\n   && - AND\n   || - OR\n   ! - NOT\n    Example:\n   if [[ ! ( FALSE || TRUE )]]; --- in (will be TRUE) but ! will invert to FALSE\n    Note that 'false' and 'true' are not commands, commands return 1 and 0\n   and behavios incorrectly\n\n\n(( )):\n  Permits Arithmetic expansion and Evaluation for \n  Works only with Integers (no float dot like in Double or Float types)\n   Note:\n  bash syntax part, not a command\n  also does expansion w/o $ symbol before var name\n\n\nArithmetic expansion examples\n   a=$(( 5 + 3 )) or a=$((5+3)) - assign result of addition to variable 'a'\n   C style manipulations:\n   (( var++ )) or ((var++)) - display then increment by 1\n     $(( var++ )) - increments and saves var but also produses errors\n   (( ++var ))  - increment by 1 then display\n   (( --var ))  - same manner decrement\n   (( var-- ))  - display then decrement\n   Assignment:\n   $((b+=a)) - add 'a' to 'b' and assign result to 'b' - b=$a+$b\n\n\nEvaluation:\n   if ((10>=5)); then echo \"10 greater or equal 5\"; fi\n      ((var1>=var5))  - same byt with variables\n\n\nLogical operators:\n   same as in [[ ]] - && , || , !\n\n\n(()) returns 'true' if integer inside of it greater than 0\n   Example:\n  dos:bash$ if ((1)); then echo \"true\"; else echo \"false\"; fi\n\n\n\n\ntrue\n  dos:bash$ if ((0)); then echo \"true\"; else echo \"false\"; fi\nfalse\n\n\n\n\nLoops:\nfor:\n   Has two forms, first is foreach:\n    Syntax:\n  FOR var IN words; DO \n    commands\n  DONE\n   var - custom name as in regular foreach\n     usually i j k l m - thanks to Fortran where int vars must start\n     with those letters\n   words - is a collection, could be just 1 2 3 4 \n       or result of command\n       or result of wildcard matches list\n       or result of subshell execution\n       if omitted - args passed from commandline\n   do and done are the braces\n   Example:\n  for i in 1 2 3;  do echo \"print $i\"; done\n\n\n\n\nprint 1\nprint 2\nprint 3\n\n\n\n\nExample:\n  for file in test*.txt; do echo $file; done\n\n\n\n\nwill echo all files from current dir matched by globbed(wildcard)\n\n\ntest*.txt mask\n\n\n\n\n\nExample:\n  for i in $(some commands executed in subshell); do....\n\n\nIf optional 'words' component is omitted in 'for' it uses args(params) as\n  collection to iterate through\n\n\nExample:\n  for i; do echo \"command line param 1 is: $1\" shift; done\n   this will echo out all params of bash script one by one (shift command)\n\n\nSecond form of 'for' is c-like (like real 'for' in c# and not 'foreach'):\n    Syntax:\n   for (( expr1; expr2; expr3 )); do\n    commands\n   done\n\n\nwhere \n    expr1-2-3 - are arythmetic expressions\n\n\nExample:\n\n\n\nfor (( i=0; i>5; i++ )); do\n    echo \"i is $i\"      # will output $i from 0 to 4\n   done\n\n\nThis form is equialent to following construction:\n\n\n\n(( expr1 ))          # this is assigned before loop\n   while (( expr2 )); do    # loop goes as long as expr2 is true\n    commands\n    (( expr3 ))         # at the end of each iteration expr3 is\n                #+ reassigned/reevaluated\n   done\n\n\nwhile:\n  infinity loop while exit code of command\\expression is 0 \n  could read from file(stdin) line by line with STDIN redirect or pipe:\n   Syntax:\n  while true; do    | while [[ true ]]; do | while (( 1 )); do\n   commands\n  done\n    Example:\n   while read field1 field2 field3\n    ..\n   do < file_to_read_from.txt\n    Example:\n   cat file_to_read_from.txt | while read field1 field 2 field3; do ... ; done\n\n\nexample:\n\n\n\nwhile read host ...; do... - or like this if in 1 line\n  do\n    ping -c 3 $host\n    if [[ $call_continue ]]; then\n      continue           # will pass further iteration and start over\n    fi\n    if [[ $call_break ]]; then\n      break         # break loop and go out of it at all\n    fi\n  done < myhosts.txt\n\n\nwill read from myhosts.txt while there are lines, each line will reinit $host\n\n\nbreak: will stop loop from execution - just regular break as ususal\n  continue: will pass iteration and start next iteration - also standard one\n\n\nexit status:\n\nexit code:\n  bash scripts ALWAYS returns exit code. 'exit' is optional\n  commands ALWAYS return exit code too. \n  Functions ALWAYS return exit code too. 'return' is optional\n  seems like everything returns exit code.\n  Even bash shell 'exit'ed with number will return this number\n  to manually return exit code:\n  exit nnn - to return nnn\n    !!   nnn - MUST be an INTEGER in range 0-255 !!\n  return nnn - to return nnn but from Function\n  exit $? or exit or omitting the 'exit':\n   with no params exit code is taken from latest command executed in the script\n  Variable that contains exit code:\n  $? - contains latest exit code, overriden after any next execution\n    even 'echo $?' will override it with 0\n    Exit code values:\n  0-255 allowed\n    By convetion\n  0 - OK\n  1-255 - various errors \n\n\nto test Exit Code:\n  echo $? - will return exit code of previous command or value of previously\n        executed script \n  more reading:\n  http://tldp.org/LDP/abs/html/exit-status.html\n\n\ncase:\n  check whether one of condition matches the 'word' in case, if so it\n executes code in particular case and terminates\n  Syntax:\n case $word in      # value of $word will be compared against\n  \n|\n) cmd1 # the \n and if tru command1\n         cmd2   # and command2 will be executed\n         ;;     # this ends particular case block\n  \n)         exit 1     # * match anything, last case 'catch'\n             ;;     # ends 'catch' block values of '$word' either\n            #+ matched by previous cases or this one\n            #+ nothing is left 'case' block unmatched this way\n esac           # this ends whole case, after 1 of the cases \n            #+ matched, or none matched\n  Example:\n while [[ -n $1 ]]; do      #while param is not null\n  case $1 in            #check param\n    -f | --file)    shift   #take $2 as $1 , if -f, \n                #to get file path given with -f option\n            filename=$1\n                #save file path to $filename\n            ;;\n  esac\n Case uses wildcards as pattern:\n ? - one character\n * - anything of any length\n [ABC] - 1 letter - either A, B or C\n [0-9] - 1 number in 0 - 9 range - any digin with length of 1\n a - letter 'a'\n \n.txt - anything that ends with '.txt'\n\n\nGood practice is \n\n\ntrue:\n  command that always return 0\n   Example:\n  exit $(true)   - will return 0 exit code\n   or Function:\n  return $(true) - will return 0 after Function\n   or:\n  true       - will return 0 exit code\n   or Funcion:\n  test() {\n   true\n  }\n  test       - this will return 0 in script, and if last line - script\n           will return this same 0 to caller (i.e. bash shell)\n   Same works for False\n\n\nfalse: \n  command that always return 1\n\n\nhelp:\n  lists all built-in commands\n   help \n\n  will return help for that command\n\n\nshopt:\n  SHell OPTions - built-in command to set\\view shell options\n\n\nshell types\n  See details:\n  https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work\n  Even more detailed:\n  https://askubuntu.com/questions/879364/differentiate-interactive-login-and-non-interactive-non-login-shell\n there are 4 shell types\n  Interactive Login shell\n    either shell opened via SSH\n    or via ctrl+alt+F1(-F6)\n  Interactive Non-Login shell\n    Opening terminal\n    executing 'bash'\n  Non-interactive Non-Login\n    scripts execution\n    or 'bash -c'\n  Non-interactive Login\n    rare stuff, complex.. should be ssh launched w/o command and STDIN of the\n    ssh should has no TTY - echo command | ssh server\n    or 'bash -l -c command'\n\n\nHow to check which shell is:\n   echo $-\n     if output has 'i' - its Interactive:\n\n\n\n\nhimBH\n\n\n\n\n\n\nbash -c 'echo $-'\n\n\n\n\nhBc\n     NOTE: that double quotes - bash -c \"echo $-\" still will start interactive\n    shell.. probably due to Shell Expansion of variables\n\n\n\n\nshopt login_shell\n\n\n\n\nlogin_shell off\n    will mean that it is NOT login shell\nlogin_shell on\n    will mean that is IS login shell\n\n\n\n\nExample for each type:\n\n\nInteractive, non-login shell. Regular terminal\n\n\n$ echo $-; shopt login_shell\nhimBHs\nlogin_shell     off\n\n\nInteractive login shell\n\n\n$ bash -l\n$ echo $-; shopt login_shell\nhimBHs\nlogin_shell     on\n\n\nNon-interactive, non-login shell\n\n\n$ ssh localhost 'echo $-; shopt login_shell'\nhBc\nlogin_shell     off\n\n\nNon-interactive login shell\n\n\n$ echo 'echo $-; shopt login_shell' | ssh localhost\nPseudo-terminal will not be allocated because stdin is not a terminal.\nhBs\nlogin_shell     on    \n\n\nfunctions:\n  shell could have functions, funcions MUST BE declared before its call,\n  otherwise shell will interpret funcion names as regular commands\n   Functions could be executed and calld from shell, added to scripts or \n  bashrc/profile files so will be available in cmd as regular command\n  'return' command in optional\n   Function MUST has at least one command, in order to mock\\stab 'return'\n  command could be used.\n   Basically every script could be converted into a function just by\n  copuint all the contents after she-bang in between function declaration\n  which is in Syntax below. And replace 'exit' with 'return' keywords\n   Variables in the Function could be set as 'local' overriding outer\n  global variables.\n   $0 in fuction contains name of the script being executed\n   $FUNCNAME - contains name of the function being executed\n\n\nSyntax:\n  function name {\n    local VAR\n    local VAR2=value\n    \n\n    return\n  }\n   Or equivalent:\n  name() {\n   \n\n   return\n  }\n\n\nExample:\n\n\n\nscript.sh:\n  function func {\n   echo \"step 2\"\n   return\n  }\n  echo \"step 1\"\n  func\n  echo \"step 3\"\n\n\nResult:\n\n\n\n\nstep 1\nstep 2\nste 3\n\n\n\n\nExample .bashrc:\n  ds() {\n   echo \"disk space utilization for $HOSTNAME\"\n   df -h\n  }\n\n\nResult after shell restarted \n   (after source .bashrc ds looped and crashed shell):\n  $ ds\n\n\n\n\ndisk space utili....\n...output of df -h....\n\n\n\n\n|:\n  pipe character\n  creates new subshell, executes what goes after pipe there, then returns\n  back to parent shell. subshell can not change parent shell - so 'read' command\n  will not work in pipe\n\n\nbreaking line; new line:\n use \\ at the end of the line.\n  NO SPACES OR ANYTHING AFTER THE BACK SLASH.\n Guideline: split the long(80 chars) line into two lines.\n Also break the line everytime there is an &&, | or || characters.\n Example:\ncommand1 \\\n && command2 \\\n || command3 \\\n | command4\n\n\nread:\n  reads line from keyboard input, accepts here-strings, does not work with |\n  By default saves entered value into $REPLY global variable\n   Synopsis:\n  read [-params] [variable1 variable2 variable3 ...]\n\n\nAccepts more than 1 variable to save keyboard input\n   Default separator is space or tab or new line(\\n)\n  If less variables are given to save data, all excessive data (delimited \n  fields) will be saved in last variable (like msbuild do when too many params)\n  When too many variables is given - unpopulated vars remain empty string\n\n\nParams:\n  -a - array, saves input into the array \n  -d \n - delimiter, by default space\\tab\\newline\n  -e - use Readline, behaves like keyboard input in bash\n  -n \n - read num symbols from keyboard input\n  -p \n - display promt befor cursor on the same line (like bash promt)\n  -r - do not inerpret \\ symbols\n  -s - silent, do not display entered data, like in password field\n  -t - time out for wait of entry\n  -u \n - use file with given descriptor as STDIN\n\n\nCommand grouping:\ncommand groups:\n Commands could be groupped, there are two types:\n  Groups:\n   { command1; command2; [command3; ... ] }\n  Subshell:\n   (command1; command2 [; command3 ... ])\n\n\nCommands in Groups are executed in current shell, so ENV variables \n chaned during commands execution are saved into current Shell ENV\n  Subshells are executed in sub shell and current ENV could not be \n modified.\n\n\nExamples:\n ls -l > output.txt\n echo \"listing of foo.txt\" > output.txt\n cat foo.txt > outpu.txt\n  ==\n { ls -l; echo \"listing of foo.txt\"; cat foo.txt; } > output.txt\n  or\n (ls -l; echo \"listing of foo.txt\"; cat foo.txt) > output.txt\n\n\nNOTICE:\n  Groups must have spaces next to {}, and last command should end with ;\n  SUbshells do not need extra spaces and ;\n BUT:\n  subshells are taking longer to be executed because of extra subshell\n need to be created. Also more memory is required. \n  Also no parent shell ENV modifications is possible\n\n\nExample with pipe:\n  { ls-l; echo \"listing of foo.txt\"; cat foo.txt; } | lpr\n this will redirect output through pipe into 'lpr' which is print command.\n  So all the output of 3 commands are redirected at once into 'lpr'\n which is not the same with \n  ls -l | echo \"...\" | cat foo.txt | lpr - will send only foo.txt to\n print\n\n\nProcesses substitution:\n <(commands list)  - processes sending output to STDOUT\n\n\n\n\n(commands list)  - processes getting input to STDIN\n\n\n\n\nSubshell output is interpreted as regular file, and could be redirected\n into another commands as usual.\n\n\nExample:\n  read < <(echo \"foo\")\n  echo $REPLY - will return 'foo'\n where:\n  'read' reads from STDIN, and saves what is read into $REPLY by default\n     if executed in subshell $REPLY will not be available to \n     parent shell\n        echo \"foo\" | read - will not work\n  <     - STDIN redirection, so STDOUT from <() is redirected as STDIN\n      of 'read' command\n  <(echo \"foo\") - Process substitution, where 'ehco' executed in \n          subshell Group () sends its output to STDOUT.\n        In other words output of this part is represented as\n        usual file, which is redirected into 'read' STDIN\n  echo $REPLY - proves that $REPLY is updated in current shell with\n        value redirected out of subshell \n        due to process substitution\n\n\nResult of substituted shell is stored as regular file and could be \n found line this:\n  echo <(echo \"foo\")\n\n\n\n\n/dev/fd/63\n   Its contents could be listed:\n  cat <(echo \"foo\")\nfoo\n\n\n\n\ntrap:\n  Event subscription for system signals (SIG..) like SIGKILL etc.\n there are 64 signals, so some variety of situations.\n  In general this is the way to trigger script when some system sent some\n particular event to the script\n\n\nSyntax:\n trap \n signal [signal ...]\n  Where command is what will be executed in case 'signal' is sent to\n the executing script\n !  Also could be a function.\n\n\nExample:\n #!/bin/bash\n trap \"echo 'i am ignoring you'\" SIGTERM SIGINT\n\n\nthis will return 'i am ...' if SIGTERM (kill -9) or SIGINT (CTRL+C) are\n sent to the script.\n\n\nAlso works with functions, so like lambda\\delegates\n\n\nExample:\n  exit_on_SIGINT () {\n   echo \"script Interrupted\" 2>&1\n   exit 0\n  }\n\n\ntrap exit_on_SIGINT SIGINT\n\n\nthis will call exit_on_SIGINT() function when CTRL+C is received\n   and will redirect 'echo' into both STDERR and STDOUT\n  After event is handled, exit 0, will ensure exit of the script, as it\n  is expected by the SIGnal\n\n\ntemp files:\nrandom:\n  event handling (trap) usually used to delete temp files generated \n during script work. Such files could contain some secure info and stuff.\n  To secure from 'temp race attack' - when temp files being searched\n by 3rd party and read, files need to be named in unobvious way\n which means use random gibberish stuff to mask particular files.\n\n\nRandom could be used like $RANDOM\n $RANDOM var containes random number from 1 to 32 767, but this is not\n  really huge number\n\n\nmktemp:\n  command that creates temporary file with name based on pattern\n  or directory\n   creates \n   Syntax:\n  mktemp \n\n  mktemp [OPTION]... [TEMPLATE]\n\n\nCreate a temporary file or directory, safely, and print its name.\n\n  TEMPLATE must contain at least 3 consecutive 'X's in last  component. \n\n  If TEMPLATE is not specified, use tmp.XXXXXXXXXX, and --tmpdir is \n  implied. \n   Files are created u+rw, and directories  u+rwx,  minus  umask\n\n  restrictions.\n\n\nthis will create file at given path, with every X being replaced\n  with random number or letter(random case)\n   NOTICE: in general convers 4+ X into random letters\\numbers\n  and only at the end of the file name\n   except if it has extention (like .txt)\n\n\nExample:\n  #!/bin/bash\n  tempfile=$(mktemp ~/.tmp/XXX-$(basename $0).$$.XXXXXXXXX) \n  echo $tempfile\n   which will produce:\n  script output: >XXX-test.17306.PRsr7hXuU\n  $ ll XXX*\n  -rw------- 1 dos dos 0 \u043a\u0432\u0456  2 03:07 XXX-test.17306.PRsr7hXuU\n\n\nGenerally it is more safe to use custom temp directory instead of /tmp\n  Example:\n [[ -d ~/.tmp ]] || mkdir ~/.tmp\n  this will check if ~/.tmp exists, if not will create it\n  But in general mktemp also could create directories.. \n\n\nAsynchronous\nAsync async\n Bash could run async commands and tools.\n To run something in background is should ends with &\n  Example:\n\n\n\n\ncopy_script.sh &\n  copyscript will now run in background\n $! - returns PID of last started in background process\n  Example:\npid=$!   - will save PID of running copy_script.sh into pid variable\n\n\n\n\nwait:\n waits for given process to finish, before continue execution\n  Example:\n wait $pid - will wait for copy_script.sh to finish before continue\n\n\nPipe\npipe\n Pipes are the files created in special pipefs filesystem\n  by default unnamed pipe are created for every | symbol in command chain\n in following command 2 pipes will be created:\n  ls -l | grep foo | less\n output of 'ls -l' will be redirected into pipeA\n input into 'grep' will be reead from pipeA\n STDOUT of 'grep' will be written into pipeB\n STDIN of 'less' will be read from pipeB\n\n\nevery command will be executed in subshell, need to clarify more\n\n\nmkfifo:\n  creates Named pipe, which is stored as a file in regular file system\n   Syntax:\n  mkfifo /path/to/file\n   To write into named pipe:\n  command1 > named_pipe\n   To read from named pipe:\n  command2 < named_pipe\n\n\nfile will have attribute 'p':\n      prw-r-----\n   Example:\n  1st Terminal:\n   mkfifo pipe1\n   ls -l > pipe1 - will halt until everything is read out from pipe1\n  2nd Terminal\n   cat < pipe1 - will read everything from pipe1 and display, 1st terminal\n        will continue its work afterwards",
            "title": "Bash page"
        },
        {
            "location": "/bash_page/#intro",
            "text": "scripts start from instruction which interpreter to use, first line always must\nbe the directive. Except if script is passed as param into interpreter, then\nthere is no need in that because there already an interpreter in place.  #!/path/to/interpreter  i.e.:  #!/bin/bash  Shortcuts:\n  ctrl+a - move to start of line\n  ctrl+e - move to end of line\n  ctrl+b - move one char back\n  alt+b  - move one word back\n  ctrl+f - move one char forth\n  alt+b  - move one word forth\n  ctrl+d - delete one char under cursor\n  ctrl+u - delete from cursor to line start\n  ctrl+k - delete from cursor to line end\n  ctrl+w - delete from cursor to word start\n  alt+bcksp - delete previous word\n  ctrl+y - paste from clipboard(alt+bckspc deleted)\n  ctrl+l - clean screen\n  ctrl+r - reverse search in history\n  ctrl+j - edit found command in search\n  ctrl+p - previous command in history\n  ctrl+n - next command in history  Bash Scripts accepts data from stdin and its direct redirection or pipe \nNote: 'read' command need to be used.    Example:   >script:  \n>read -p \"enter any *.txt\"  \n>echo \"${PWD}${REPLY}\"  \n$cd ~ && ls test.txt | ./read_and_expect_txt_files.sh  \n/home/dos/test.txt  Example:    $echo \"test.txt\" > try.txt  \n$./read_and_expect_txt_files.sh < try.txt  \n/home/dos/test.txt",
            "title": "Intro"
        },
        {
            "location": "/bash_page/#debugging",
            "text": "#!/bin/bash -x  will start debugging tracert - with display of all values expanded and\n  lines as bash sees them and will execute them  set -x  \necho $var  \nset +x  will enable debugging tracert only between set -/+ x displaying only\n commands in between with such debug info",
            "title": "Debugging:"
        },
        {
            "location": "/bash_page/#shell-variables",
            "text": "$FUNCNAME - contains name of the function being executed(like reflection)    $PS4 env var, is Promt String for debugging tracert \n$LINENO - is standard variable with line number \nPS4=\"$LINENO +\" - will display line number during debugging tracert    $OLDPWD - contains previous workind directory \n$RANDOM - contains random number from 1 to 32 767 \n$$ - current PID of the programs\\script being executed",
            "title": "shell variables:"
        },
        {
            "location": "/bash_page/#defensive-programming",
            "text": "Make sure everything is expanded and is executed in right place: \n Instead of:    $cd $path  \n$rm *  Use secure defensive way:    [[ -d \"$path\" ]] && cd \"$path\" && echo rm *  where:      -d - make sure path exists and is Directory    \"$path\" - in double quotes to avoid null expansion if var does not exist\n    it will be expanded in empty string in that way - \"\"    && - will execute next command if previous returned 0 exit code    echo rm * - will echo expanded by bash shell wildcard, displaying what \n    exactly is going to happen with actual 'rm *'",
            "title": "Defensive programming:"
        },
        {
            "location": "/bash_page/#naming-coventions",
            "text": "Variable Names: \n Lower-case, with underscores to separate words. \n Ex: my_variable_name    Constants and Environment Variable Names: \n All caps, separated with underscores, declared at the top of the file. \n Ex: MY_CONSTANT    Google naming convetions:   \n    https://google.github.io/styleguide/shell.xml#Naming_Conventions",
            "title": "Naming Coventions:"
        },
        {
            "location": "/bash_page/#execution",
            "text": "bash searches for entered command in    built-ins  hashes  bash history   env var PATH \n   4.1. PATH is split by ':' and every path is searched for the command    new child process of bash created - fork(), and all ENV copied into it  found command is executed in the child copy of bash and replaces it - execve()    main bash process executes wait() until the child process ends  after child process ends cleanup starts and destroys the process    So if script/binary is executed from some(i.e. current) directory and it is\n  not in path , it have to be 'source'd like:     source scriptname   or:      . scriptname  . is same to source       or:      ./scriptname  this seems to be  /scriptname (so basically full path, which is also applies)",
            "title": "Execution:"
        },
        {
            "location": "/bash_page/#expansion",
            "text": "Expansion is done by shell(i.e. bash) and expanded result is passed to shell\\command to be executed. If something is escaped by escape sequence, it will not be expanded by shell, and  could  be expanded by the program receiving escaped sequence. \n expansion works before pass anything further, so:     cp * /some/dir  will copy everything from current dir into /some/dir \n  first argument * actually will be passed to cp as list of files matched \n  by wildcard, 'cp' will never see * itself \n  to check what * will return, execute  echo *",
            "title": "Expansion:"
        },
        {
            "location": "/bash_page/#variables-expansion",
            "text": "Expansion starts when $ is encountered. \n  Escape sequences: \n  \\ - will escape 1 symbol to treat it as literal and not meta symbol \n  '' - everything between single quotes threated as literals too \n  \"\" - will expand variables, but escape spaces etc. \n   Variables are expanded before passing them to command so command will only\n  see var's value, not its name. \n   Bash creates variable when first mets it, so it will create even empty\n  variables, which could lead to errors:    foo=\"/some/file\"  \nmv $foo $foo1  # error   Bash will create $foo1 variable , then mv will receive $foo1 value which\n  is null, and 'mv /some/file  ' will get error as only 1 param is passed \n   To rename file to  'some/file1'  use  {} :         mv $foo {$foo}1",
            "title": "variables expansion:"
        },
        {
            "location": "/bash_page/#positional-parameters",
            "text": "Bash supports positional params for the script. \n  default ones are $0-$9 , other params are listed by ${10} and so on:      $0 ---- contains full path of the script being executed \n----    does not count as parameter in list $# \n----    so [ $# -gt 0 ] will return false if $1 is empty      $1+  ---- are actual parameters",
            "title": "Positional parameters:"
        },
        {
            "location": "/bash_page/#parameters-are-passed-as-usual",
            "text": "Example:    script.sh param1 param2  in script.sh:    echo $0  #/path/to/script.sh\necho $1  #param1\necho $2  #$param2\necho $3  # this would be empty uninitialized param with null value",
            "title": "Parameters are passed as usual"
        },
        {
            "location": "/bash_page/#paraters-could-be-passed-using-wildcards-like",
            "text": "Example:    script.sh *  will pass everything matched, basically it would be a\n        list of files in PWD (same as  echo * )",
            "title": "Paraters could be passed using wildcards like"
        },
        {
            "location": "/bash_page/#params-arrays",
            "text": "$# -  .Count , stores number of arguments passed into the script \n    does  not count $0  in it, so is not 0 based  $* - stores all arguments - ($1 $2 ...), if args has spaces those \n    would be treated as separate params, so params  \n    (\"param1\" \"param with spaces\") would be  $1=param1 $2=param $3=with $4=spaces     \"$*\" - all params expanded between (\"$1 $2\" ...), with delimiter of first\n    symbol in $IFS variable, it becomes one single parameter: \n    (\"param1\" \"param with spaces\") would be  $1=\"param1 param with spaces\"  (leaving quotes behind in the end)    $@ - same as $*    \"$@\" - stores all arguments in quotas (\"$1\" \"$2\" ...), thus handling \n    spaces in variables. \n    (\"param1\" \"param with spaces\") would be  $1=\"param1\" $2=\"param with spaces\"   \n    See 'variable substitution' for more use - ${@:2} (w/o \" works same)",
            "title": "Params Arrays:"
        },
        {
            "location": "/bash_page/#shift",
            "text": "shifts params backwards so $2 would be $1 and old $1 would vanish\n  using this command each parameter could be handled one by one \n   Example:    while [[ $# -gt 0 ]]; do\n echo \"Parameter is $1\"\n shift\ndone  This will echo out all parameters given as per shift will consequentially\n  move all parameters into $1 position one by one",
            "title": "shift:"
        },
        {
            "location": "/bash_page/#basename",
            "text": "returns file name from latest node in the path \n   Example:    $/home/dos/scripts/some_script.sh\nin script >> echo basename $0\nwill return 'some_script.sh'",
            "title": "basename:"
        },
        {
            "location": "/bash_page/#dirname",
            "text": "returns directory name (everything except latest nodein the path) \n  antipod of  basename \n   Example:  cd /home/dos/directory/filename\ndirname $PWD\n> /home/dos/directory",
            "title": "dirname:"
        },
        {
            "location": "/bash_page/#variables",
            "text": "AKA Global Variables - are visible everywhere in shell environment\n   if 'export'ed - will be visible in subshells created from the shell \n   'local' variables are local for funcions - see below \n  is converted to 'String' class by default \n  Initializate variable by = sign w/o leading or trailing spaces \n  Access variable by $    Also could be int, if used with 'declare':    declare -i variable=1  COuld be readonly:    declare -r READONLY=\"this is constant, in uppercase by convention\"  Variables definitions:    a=z                     # assign string \"z\" to var 'a'\nb=\"a string\"            # assign \"a string\" w/ space to var 'b'\nc=\"a string and $b\"     # expansion works in \"\" so 'c' value \n                        # is \"a string and a string\"\nd=$(ls -l foo.txt)      # result of command is assigned to 'd'\ne=$((5 * 7))            # result of ariphmetic result is assigned to 'e'\nf=\"\\t\\tastring \\n\"      # escaped sequences also works in \"\" here:\n                        # \\t - tab\n                        # \\n - new line  More than one variable could be declared in 1 line:    a=5 b=\"a string\" # will create two variables with\n                 # values \"5\" and \"a string\", probably 5 is int.ToString()  To preserve spaces and stuff use quotas with string vars initialization \n   Example:    my_str_var=\"string w/ spaces goes in quotas\"\necho \"stuff is: $my_str_var\"   # stuff is: string w/ spa.....\necho 'stuff is: $my_str_var'   # stuff is: $my_str_var",
            "title": "variables:"
        },
        {
            "location": "/bash_page/#arrays",
            "text": "also variables, indexed from 0,  \n  could have empty indexes. \n   Example declaration:    a[10]=Ten   # assign 'Ten' to 11th index in array length of 1  Powershell-like thing(or vice versa), except w/o @     days=(Mon Tue Wed Thu Fri Sat Sun)\ndays=([0]=Mon [1]=Tue ... )    # same as above but with strict indexing\nanimals=(\"a dog\" \"a cat\" \"a cow\")  If array addressed w/o index, index 0 is displayed, if it is null then nothing\n  displayed EVEN IF THERE ARE OTHER INDEXES \n   Example:    arr=([1]=one [2]=two)\necho $arr       # nothing returned\narr[14]=14\necho $arr       # nothing returned\narr[0]=0\necho $arr       # 0 is returend\necho ${arr[@]}  # 0 one two 14 is returned  Example2: assignment of array w/o index specification    arr=(a b c d)\necho ${arr[@])  # a b c d - is returned\narr=AAA\necho ${arr[@]}  # AAA b c d - is returned\necho $arr       # AAA - is returned  Example3: deletion of first index w/o index specification    arr=(a b c d)\narr=\necho ${arr[@]}  # b c d - is returned  Iteration:  Array variable substitution(see below for strings) arrays could be iterated\n  and counted and checked for empty indexes: \n   Iterate array:    ${animals[*]} # returns list of values split by space\n${animals[@]} # same as above  Example:    arr3=(\"a first\" \"a second\" 'a third')      # \" or ' is fine\nfor i in ${arr3[*]}; do echo $i ; done\n\n>a          # separate variables\n>first      # also separated by spaces(split)\n>a          # will work fine if no Spaces\n>second\n>a\n>third  \"${animals[*]}\" - JOIN joins all elements into 1 string \n Example:   for i in \"${arr3[*]}\"; do echo $i ; done   # only \" , ' will not work\n >a first a second a third                  #joined string  TODO: CONTINUE FROM HERE  Best form to use:\n  \"${animals[@]}\" - returns list of values NOT split by space(real contents)\n    Example:\n   for i in \"${arr3[@]}\"; do echo $i; done   a first     # only different values are separated\na second        # spaces are in place\na third     # best form to use   Counting:\n  Array count coun be calculated, and length of value in single index:  .Count:\n  ${#array[@]}    \n   where:\n  array is var name\n   Example:\n  echo ${#arr3[@]}   3            # there 3 elements 'a first' 'a second' 'a third'   array[i].Length:\n  ${#array[i]}    \n   where:\n  array is var name and i is index to measure length of\n   Example:\n  echo ${#arr3[1]}   8            # 'a first' is 8 characters long   Check indexes numbers:\n   Arrays could have uninitialized or null initialized elements under some\n  indexes.\n   The way of how to check which indexes are taken is:\n  ${!array[@]}      # works same w/ and w/o double quotes\n  ${!array[ ]}      # works same as above w/o quotes, \n            # but differently w/ quote, see example below in NOTE:\n   Example:\n  arr=([1]=\"a one\" [3]=\"a three\" [5]=\"a five\")\n  for i in ${!arr[@]}; do echo $i; done \n           \"${!arr[@]}\"         # will do the same\n           ${!arr[ ]}           # will do the same   1\n3\n5   NOTE: @ works same With or WithOut quotes:\n        ${!arr[@]} == \"${!arr[@]}\n   AND\n        * works same as above WithOut quotes\n   BUT \n        in double quotes it will JOIN indexes in one string with spaces separators\n   Example:\n  for i in \"${!arr[*]}\"; do echo $i; done   1 3 5                # produces one line of indexes   array.Add:\n  Arrays could have new elements added into the end of it.\n This is made as in Powershell(or powershell take it from bash) using +=\n  Example:\n arr=(a b c)        # echo \"${arr[@]}\" > a b c\n arr+=(f e g)       # echo \"${arr[@]}\" > a b c f e g  array.Sort:\n  There is no default sort method, but it could be implemented with\n piping and regular 'sort' command, but with Iteration:\n  arr_sorted=$(for i in \"${arr[@]}\"; do echo $i; done | sort)\n    this will save soreted \"a b c e f g\" into arr_sorted\n   !!!! BUT THIS WILL NOT WORK    !!!\n   !!!!   echo \"${arr[@]}\" | sort !!!  Delete array:\n  Deletion of array is done in the same way as deletion of variables\n using 'unset' command.\n DO Not Use '$' in unset.\n  Syntax:\n unset array_name  Example:\n arr=(1 2 3)\n echo ${#arr[@]}    # check Count - returns 3\n echo ${arr[@]}     # check contents - returns 1 2 3\n unset arr      # deletes arr, NOTICE no '$' before 'arr' NAME\n echo ${#arr[@]}    # cehck Count - returns 0, as for anything unset\n            #+ because bash initializes on execution call\n echo ${arr[@]}     # returns nothing  variable substitution:\nsubstitutuin:\n   Bash supports various variables substitutions:\n  $a - will be substituted with 'a' value\n  ${a} - same as $a but could be concatenated w/ string w/o spaces:\n    ${a}.txt - will be expanded in a_value.txt\n  ${11} - 11th positional parameter given to script from shell\n  ${var:-word} - if 'variable' is set, the result will be its value\n    if 'variable' is unset - the result will be 'word'\n  $(var:=word} - if variable is set results in its value substituted\n    if variable is unset, it will be assigned to 'word'\n    such assignment will not work for positinal params(see 'shift')\n    and other special variables\n  ${var:?word} - if variable is unset error with reason 'word' will be\n    generated, exit code of such construct will be 1\n  ${var:+word} - if 'variable' is set, the result will be 'word',\n    (but variable's value will not be changed)\n    otherwise result will be EMPTY string\n  Example:\n    $ echo ${variable:-ls} - variable unset - ls used\n    > ls\n    $ export variable=1\n    $ echo ${variable:-ls} - variable is set- its value used\n    > 1\n    $ echo ${variable:+ls} - variable is set - ls used\n    > ls\n    $ echo ${variable1:+ls} - variable unset - empty line used\n    > \n  ${!prefix } or ${!prefix@} - returns NAMES of existing variables\n    that starts from 'prefix.\n   Example:\n    $ echo ${!BASH }\n    > BASH BASHOPTS BASHPID BASH_ALIASES BASH_ARGC BASH_ARGV BASH_CMDS  string variables substitution:\n  ${#var} - returns length of string in variable's value\n    Example:\n   $ var=123456789   #this could be interpreted as a string too now   9           #string length is 9   ${#} or $# or ${#@} or ${#*} - returns number of positional parameters\n    of the script being executed  ${var:number} - return string from number to the end, spaces trimmed\n          variable is unchanged.\n    Example:\n   $ var=\"This string is to long.\"\n   $ echo ${var:5}         #returns string from 5th symbol   string is to long.\n    Example: spaces are trimmed:\n   $ echo ${var:5} | wc -c     #count chars\n   $ 19\n   $ echo ${var:4} | wc -c     #return starts from space\n   $ 19                #space is trimmed so same number of chars\n  ${var: -number} - return string from end to number, spaces trimmed\n            NOTE - space between ':' and '-' signs\n    Example:\n   $ echo ${var: -5}\nlong.\n  ${var:number:length} - return string from number till end of lenth\n    Example:\n   $ echo ${var:5:6}\nstring   ${var: -number: -length} - return string number between number(from the\n                 end) and length (also from the end)\n                 NOTE: number must be > than length\n    Example:\n   $ echo ${var: -18: -2}    #var is This string is to long.   string is to lon   ${@} - return all values of positional params \n     leaving spaces inside strings (like \"$@\" ) - bcs it know how \n     many arguments script has\n     ${*} is the same form, it seems  ${@:num} - displays values of positional params but from num\n     $(@:1) - works same as ${@}\n     ${@: -2} works , but starts from the end  ${@:num:length} - same as with strings but with positional params\n  ${@: -num: -length} - same as with strings but with positional params  ${param#pattern} - finds shortest match and deletes it (lazy match)\n    Example:\n   foo=\"file.txt.gz\"\n   ${foo#*.}   txt.gz\n  ${param##pattern} - finds longest match and deletes it (greedy match)\n   Example\n   ${foo##*.}\n.gz   ${param%pattern}  - same as #  but deletes from the end of the file\n   Example:\n   foo=file.txt.gz\n   ${foo%. }        - note .  instead of *. in # example   file.txt\n  ${param%%pattern} - same as ##\n   ${foo%%.*}\nfile   Search and replace:  ${param/pattern/string} - replaces first occurance of pattern with string\n ${param//pattern/string} - replaces all occurances of pattern with string\n ${param/#pattern/string} - replaces only if at the beginning of the line\n ${param/%pattern/string} - replacesonly if at the end of the line  $(())\n  Accepts any valid arithmetic expression \n   Pretty similar to (()) test construct which returns true when result >0\n Accepts any number system:  Decimal\n   with base of 10\n  number\n   Example:\n  echo $((10)) - will return 10\n  echo $((10#10)) - will return 10  Octal: \n   with base of 8 [0-7]\n  0number\n   Example:\n  echo $((010)) is 8 in decimal, where 10 is the whole base, which is 8\n  echo $((07)) is 7 in decimal\n  echo $((8#10)) is 8 in decimal\n  echo $((08)) - will return error 'value to great'\n        0-7; 10-17 and so on will work  Hex: Hexademical:\n   with base of 16 [0-9A-F]\n  0xnumber\n   Example:\n  $((0x10)) - is 16 in decimal, which is full base\n  $((16#10)) - is also 16, because of base of 16 numbers\n  $((16#100)) or ((0x100)) is 256 which is 16x16 or 16 in square(^2)\n  $((0xFF)) is F(15) full bases plus F(15) \n        which is ((15x15)==240)+15 == 255  Custom:\n    custom base , could be any number [0-9A-Z] and some other]\n    maximum base is 64, seems like\n   base#number\n    Example:\n   echo $((17#10) - is 17, which is full base\n   echo $((64#10)) - is 64, which again is full base  Arithmetic expressions syntax $((arithmetics go here)):  Example:  echo $((2+3)) >> 5  IFS - variable that containes field separator, \n    by default is space/tab/new line\n    Could be changed  Temporary variables:\n   Bash supports variables change/assignment before command execution\n  Variables assigned before command execution (on the same line) will be\n  changed only for env with which command will be executed:\n   IFS=\":\" read field1 field2 ... <<< \"$line_from_passwd\"\n    Equals to:\n   OLD_IFS=\"$IFS\"\n   IFS=\":\"\n   read field1 field2 .... <<< \"$line_from_passwd\"\n   IFS=\"$OLD_IFS\"\n  !Note:\n   it works weird - only in example from bood, need to research and update  Unary operators:\n   Operators that require value on UNE side, contrary to binary operators\n   that reauire values on both sides:\n    + - is for positive numbes: +1\n    - - is for negative number: -1  Arithmetic operators: Binary operators:\n    there are more binary operators, but those below are used in Arithmetics\n  + - for addition\n  - - for substruction\n  * - for multiplication\n  / - for division\n    - for exponentiation (2 2=4 3 3=9 4*4=16 and so on, aka ^2 ^3 ^4 ...)\n  % - modulus division \n    5%2=1 which is what left after division - 2, 2 and 1\n      Example:\n    echo $(( 5%2 )) - will return 1  Assignment in Arithmetic expressions:\n  echo $((foo=5))  - will assign 5 to $foo\n   NOTE: also could be used in tests like [[...]] and ((...))\n   NOTE: in 'test' or [...] used to  compare  strings, BEWARE  Assignment operators:\n   var = value \n    regular assignment operator\n   var += value \n    assignment with value addition      $var = $var + value\n   var -= value\n    assignment with value substruction  $var = $var - value\n   var *= value\n    assignment with value multiplication    $var = $var x value\n   var /= value\n    assignment with value division      $var = $var / value\n   var %= value\n    assignment with value modulus division  $var = $var % value  Assignment with C-like style increment decrement:\n   Post - means show\\use first, change after it shown\\used\n  var++ - post increment            $var = $var + 1\n  var-- - post decrement            $var = $var - 1\n   Example:\n   foo=1; echo $((foo++)); echo $foo\n      1        2\n   Pre  - means change\\use first , show after it changed\\used\n  ++var - pre increment             $var = $var + 1\n  --var - pre decrement             $var = $var - 1\n   Example:\n   foo=1; echo $((++foo)); echo $foo\n      2        2  Basic logical gates:\n   AND - returns true if both entry are true\n   OR - returs true if at least one (or both) entry is true \n   XOR - returns true if entry differs, \n     both true and both false return false\n   NOT - inverts entry - true is false and false is true\n   NAND - both false are true\n   NOR - returns true if at least one (or both) entry is false\n   XNOR - returns true if not differs,\n      both true or both false return true  Logical operators in Arithmetics:\n   Comparison operators less, greater , less than and so on\n  <= - less or equal   = - greater or equal\n  < - less\n- greater\n  == - equal\n  != - not equal\n  && - AND\n  || - OR   expr1:expr2?expr3 - trinary operator, \n              if expr1 is true \n              then expr2 executed\n              else expr3 is executed\n   Example:\n  a=0\n  ((a<1?++a:--1))   # a+=1 and a-=1 will not work\n  echo $a       # 1\n  ((a<1?(a+=1):(a-=1))) # but this will work just fine\n  echo $a       # 0  local variables:\n  local variables are declared in functions, and visible only there\n  local variables overwrite globals with the same name\n   Syntax:\n  local variable_name[=optional_value]\n   values are optional , probably, in order to delete some global vars inside\n  a function(i.e. vars are used as standard in\\out params of a command)\n   Example:\n  foo=0\n  func() {\n   local foo\n   foo=1\n   echo \"in func $foo\"\n  }\n  func\n  echo \"in script $foo\"\n   Result:   in func 1\nin script 0   here doc here docs here-document here script :\n  allows multiline input and it WILL expand even if in ''.\n   But will not expand if escaped - \\$\n   Syntax:\n  command << indicator\n  text\n  indicator  <<-  - work as << but accepts TAB before enclosing 'indicator'\n    'space' will still return error  where:\n  command is a command like, 'cat' or anything else (doesn't work with 'echo')\n  where text between 'indicators' is expanded by shell and sent to STDIN of\n  the command\n   Note that also <<- could be used, with it bash will ignore leading\n  tabs in 'indicator'\n   Note that seconds 'indicator' has to be on separate line and \n  Must have no spaces before or after it, otherwise bash interpreter will \n  conntinue to look for 'indicator' ignoring the one with leading/trailing \n  spaces\n   Value of 'indicator' could be any but 'EOF' is preffered  Example:\n  $ foo=\"text\"\n  $ cat <<  EOF   $foo\n\"$foo\"\n'$foo'\n\\$foo EOF \n   Result:\n  text\n  \"text\"\n  'text'\n  $foo   Example of use with 'ftp'\n  ftp -n << EOF\n  open $FTP_SERVER_ADDRESS\n  user anonymous username@hostname\n  cd $FTP_PATH\n  hash\n  get $REMOTE_FILE\n  exit\n  EOF\n  ls -l $REMOTE_FILE - will display downloaded from ftp remote file, all the\n    commands are passed from inside the here-document  here-line:\n  same as here-doc but onliner:\n   $read field1 field2 <<< \"test in put\"\n   $echo $field1  ---> test\n   $echo $field2  ---> in put  < because put is 3rd field, and we have only 2\n                all excess fields goes into last 'field2'\n    Profid of here-line here is that commands with pipe\n   $echo \"test in put\" | read  \n    will not work because subshell will be created for 'read' where $REPLY\n    will be created and assigned, but after that subshell ends, and\n    subshell can not change parent shell's env, such as REPLY value  if:\n  Condition tests in IF should be inside square brackets - [], which are just a\n  reference for commant 'test'. see MAN page for TEST for details.\n   example:\n  if [ 100 -eq 100 ]; then - semicolon is needed in case of single line w/o /n\n   -or-\n  if [ 100 == 100 ] - seems like this also should work\n   -or-\n  if $(test $(echo \"$REPLY\" | grep '^[0-3]$'))\n    tests whether value of REPLY is mathced- if not, grep return 1(error)\n    then test returns 1 and 'if' will not work\n    if value matched then grep returns 0 (good), then test returns 0 and\n    'if' will work\n   -or-\n  if test $(echo \"$REPLY\" | grep '^[0-3]$')\n    same as above, but $() outside 'test' is unnecessary in fact\n    but works in both ways so keep both examples here\n   -or-\n  if [   ] && [   ] || [   ]\n  then\n     \n  elif [   ]      # else if, onliner gotta be ; ended\n  then\n     \n  else              # one liner gotta be ; ended\n     \n  fi  lots info here:\n  https://ryanstutorials.net/bash-scripting-tutorial/bash-if-statements.php  && ||:\n  allowed inside bash shell logical AND and OR:\n   Example:\n  mkdir test && cd test\n   will create and than change dir to 'test'\n  [ -d test ] || mkdir test && cd test\n   will check whether 'test' exists, returns 0 or 1\n   || continues only if 1 was returned by 'test' and proceeds with mkdir\n   && continues only if 0 was returned by 'mkdir' and proceeds with cd  test:\n  Tests whether expression returns 0 or not\n  after evaluation command 'test' returns 1 if true and 0 if false\n   test 100 -eq 100 - test whether 100 equals 100\n  or [ ]\n   if [ 100 -eq 100 ]\n  used to test conditions  Logical operators:\n   -a - AND\n   -o - OR\n   ! - NOT\n    Example:\n  [ ! ( FALSE -o TRUE )   -- (TRUE) but [ FALSE]\n   note the escaped parentheses\n   note that 'false' and 'true' command will behave differently , bcs returns\n  interegs 1 and 0 respectively  Requires shell expansion symbols $ \n   Example:\n  if [ $num1 -lt $num2 ]; then echo 'int 1 less than int2'; fi  BEWARE!!!!!!\n  shell creates empty variable if it is not defined\n  and $num1 -lt $num2 will return TRUE if both undefined  CHECK YOUR VARIABLES BEFORE USE  Test Strings\\Ints:\n  Operator  Description\n  !    The EXPRESSION is false.\n  -n str    The length of STRING is greater than zero.\n  -z str    The lengh of STRING is zero (ie it is empty).\n  str1 == str2      STRING1 is equal to STRING2 (= also allowed)\n  str1 != str2      STRING1 is not equal to STRING2\n  str1 > str2       STRING1 is alphabetically greater than STRING2\n  INTEGER1 -eq INTEGER2     INTEGER1 is numerically equal to INTEGER2\n  -ne               integers are not equal\n  -le               are less or equal\n  -ge               greater or equal\n  INTEGER1 -gt INTEGER2     INTEGER1 is numerically greater than INTEGER2\n  INTEGER1 -lt INTEGER2     INTEGER1 is numerically less than INTEGER2  Test Files:\n  FILE -ef FILE1 - both files are hard links (point to same inode)\n  FILE -nt FILE1 - FILE newer than FILE1\n  FILE -ot FILE1 - FILE older than FILE1\n  -e FILE   FILE exists.\n  -f FILE   FILE exists and is file\n  -s FILE   FILE exists and its size greater than 0\n  -L FILE   exists and is soft symblic link\n  -d FILE   FILE exists and is a directory.\n  -b FILE   exists and is Block device (i.e /dev/sda1)\n  -c FILE   exists and char device\n  -r FILE   FILE exists and the read permission is granted.\n  -s FILE   FILE exists and it's size is greater than zero (ie. not empty).\n  -w FILE   FILE exists and the write permission is granted.\n  -x FILE   FILE exists and the execute permission is granted.\n  -G FILE   exists and file belongs to existing group\n  -O FILE   exists and belongs to existing user  NOTE:\n  == - does a string comparision\n  -eq - does numerical comparison  Escaping:\n  symbols < > ( ) need to be escabed by \\ or be between ''\n  because [ ] is just command test, and bash will try to use them as its own \n  meta symbols (< > are stdout\\in redirecton, so could be a MESS)  [[ ]]:\n  improved 'test' command but not POSIX compatible\n    Differences\\Features:  Does not need expansion symbol $, expands just fine w/o it  UNLESS IS A STRING, then use \"$VAR\", to expand and prevent empty values\n    Example:\n   $ if [[ $num1 -lt num2 ]]; then echo \"$num1 less than $num2\"; fi   1 less than 2\n    NOTE:\n   $num1 and num2 are both fine as per expansion works in some other way   BEWARE OF UNDEFINED VARIABLES\n   IF BOTH ARE UNDEFINED 0 WILL BE RETURNED!!  Can compare regex.  Seems like gotta match full line, not just some part.\n   uses Extended Regex (ERE)\n    Requires $ symbol to expand variable value\n    use \"\" to prevent empty values\n   No need to escape bash meta symbols like \\ or *\n    Example:\n   $ if [[ \"$string_var\" =~ \\w+? ]]; then echo \"good\"; fi   good   Can use wildcards , or similar to it..  When using == equation\n    Example:\n   $ FILE=foo.txt\n   $ if [[ $FILE == foo.* ]]; then echo \"matches pattern\"; fi   matches pattern   Logical operators:\n   && - AND\n   || - OR\n   ! - NOT\n    Example:\n   if [[ ! ( FALSE || TRUE )]]; --- in (will be TRUE) but ! will invert to FALSE\n    Note that 'false' and 'true' are not commands, commands return 1 and 0\n   and behavios incorrectly  (( )):\n  Permits Arithmetic expansion and Evaluation for \n  Works only with Integers (no float dot like in Double or Float types)\n   Note:\n  bash syntax part, not a command\n  also does expansion w/o $ symbol before var name  Arithmetic expansion examples\n   a=$(( 5 + 3 )) or a=$((5+3)) - assign result of addition to variable 'a'\n   C style manipulations:\n   (( var++ )) or ((var++)) - display then increment by 1\n     $(( var++ )) - increments and saves var but also produses errors\n   (( ++var ))  - increment by 1 then display\n   (( --var ))  - same manner decrement\n   (( var-- ))  - display then decrement\n   Assignment:\n   $((b+=a)) - add 'a' to 'b' and assign result to 'b' - b=$a+$b  Evaluation:\n   if ((10>=5)); then echo \"10 greater or equal 5\"; fi\n      ((var1>=var5))  - same byt with variables  Logical operators:\n   same as in [[ ]] - && , || , !  (()) returns 'true' if integer inside of it greater than 0\n   Example:\n  dos:bash$ if ((1)); then echo \"true\"; else echo \"false\"; fi   true\n  dos:bash$ if ((0)); then echo \"true\"; else echo \"false\"; fi\nfalse   Loops:\nfor:\n   Has two forms, first is foreach:\n    Syntax:\n  FOR var IN words; DO \n    commands\n  DONE\n   var - custom name as in regular foreach\n     usually i j k l m - thanks to Fortran where int vars must start\n     with those letters\n   words - is a collection, could be just 1 2 3 4 \n       or result of command\n       or result of wildcard matches list\n       or result of subshell execution\n       if omitted - args passed from commandline\n   do and done are the braces\n   Example:\n  for i in 1 2 3;  do echo \"print $i\"; done   print 1\nprint 2\nprint 3   Example:\n  for file in test*.txt; do echo $file; done",
            "title": "arrays:"
        },
        {
            "location": "/bash_page/#will-echo-all-files-from-current-dir-matched-by-globbedwildcard",
            "text": "test*.txt mask   Example:\n  for i in $(some commands executed in subshell); do....  If optional 'words' component is omitted in 'for' it uses args(params) as\n  collection to iterate through  Example:\n  for i; do echo \"command line param 1 is: $1\" shift; done\n   this will echo out all params of bash script one by one (shift command)  Second form of 'for' is c-like (like real 'for' in c# and not 'foreach'):\n    Syntax:\n   for (( expr1; expr2; expr3 )); do\n    commands\n   done  where \n    expr1-2-3 - are arythmetic expressions  Example:  for (( i=0; i>5; i++ )); do\n    echo \"i is $i\"      # will output $i from 0 to 4\n   done  This form is equialent to following construction:  (( expr1 ))          # this is assigned before loop\n   while (( expr2 )); do    # loop goes as long as expr2 is true\n    commands\n    (( expr3 ))         # at the end of each iteration expr3 is\n                #+ reassigned/reevaluated\n   done  while:\n  infinity loop while exit code of command\\expression is 0 \n  could read from file(stdin) line by line with STDIN redirect or pipe:\n   Syntax:\n  while true; do    | while [[ true ]]; do | while (( 1 )); do\n   commands\n  done\n    Example:\n   while read field1 field2 field3\n    ..\n   do < file_to_read_from.txt\n    Example:\n   cat file_to_read_from.txt | while read field1 field 2 field3; do ... ; done  example:  while read host ...; do... - or like this if in 1 line\n  do\n    ping -c 3 $host\n    if [[ $call_continue ]]; then\n      continue           # will pass further iteration and start over\n    fi\n    if [[ $call_break ]]; then\n      break         # break loop and go out of it at all\n    fi\n  done < myhosts.txt  will read from myhosts.txt while there are lines, each line will reinit $host  break: will stop loop from execution - just regular break as ususal\n  continue: will pass iteration and start next iteration - also standard one  exit status: \nexit code:\n  bash scripts ALWAYS returns exit code. 'exit' is optional\n  commands ALWAYS return exit code too. \n  Functions ALWAYS return exit code too. 'return' is optional\n  seems like everything returns exit code.\n  Even bash shell 'exit'ed with number will return this number\n  to manually return exit code:\n  exit nnn - to return nnn\n    !!   nnn - MUST be an INTEGER in range 0-255 !!\n  return nnn - to return nnn but from Function\n  exit $? or exit or omitting the 'exit':\n   with no params exit code is taken from latest command executed in the script\n  Variable that contains exit code:\n  $? - contains latest exit code, overriden after any next execution\n    even 'echo $?' will override it with 0\n    Exit code values:\n  0-255 allowed\n    By convetion\n  0 - OK\n  1-255 - various errors   to test Exit Code:\n  echo $? - will return exit code of previous command or value of previously\n        executed script \n  more reading:\n  http://tldp.org/LDP/abs/html/exit-status.html  case:\n  check whether one of condition matches the 'word' in case, if so it\n executes code in particular case and terminates\n  Syntax:\n case $word in      # value of $word will be compared against\n   | ) cmd1 # the   and if tru command1\n         cmd2   # and command2 will be executed\n         ;;     # this ends particular case block\n   )         exit 1     # * match anything, last case 'catch'\n             ;;     # ends 'catch' block values of '$word' either\n            #+ matched by previous cases or this one\n            #+ nothing is left 'case' block unmatched this way\n esac           # this ends whole case, after 1 of the cases \n            #+ matched, or none matched\n  Example:\n while [[ -n $1 ]]; do      #while param is not null\n  case $1 in            #check param\n    -f | --file)    shift   #take $2 as $1 , if -f, \n                #to get file path given with -f option\n            filename=$1\n                #save file path to $filename\n            ;;\n  esac\n Case uses wildcards as pattern:\n ? - one character\n * - anything of any length\n [ABC] - 1 letter - either A, B or C\n [0-9] - 1 number in 0 - 9 range - any digin with length of 1\n a - letter 'a'\n  .txt - anything that ends with '.txt'  Good practice is   true:\n  command that always return 0\n   Example:\n  exit $(true)   - will return 0 exit code\n   or Function:\n  return $(true) - will return 0 after Function\n   or:\n  true       - will return 0 exit code\n   or Funcion:\n  test() {\n   true\n  }\n  test       - this will return 0 in script, and if last line - script\n           will return this same 0 to caller (i.e. bash shell)\n   Same works for False  false: \n  command that always return 1  help:\n  lists all built-in commands\n   help  \n  will return help for that command  shopt:\n  SHell OPTions - built-in command to set\\view shell options  shell types\n  See details:\n  https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work\n  Even more detailed:\n  https://askubuntu.com/questions/879364/differentiate-interactive-login-and-non-interactive-non-login-shell\n there are 4 shell types\n  Interactive Login shell\n    either shell opened via SSH\n    or via ctrl+alt+F1(-F6)\n  Interactive Non-Login shell\n    Opening terminal\n    executing 'bash'\n  Non-interactive Non-Login\n    scripts execution\n    or 'bash -c'\n  Non-interactive Login\n    rare stuff, complex.. should be ssh launched w/o command and STDIN of the\n    ssh should has no TTY - echo command | ssh server\n    or 'bash -l -c command'  How to check which shell is:\n   echo $-\n     if output has 'i' - its Interactive:   himBH    bash -c 'echo $-'   hBc\n     NOTE: that double quotes - bash -c \"echo $-\" still will start interactive\n    shell.. probably due to Shell Expansion of variables   shopt login_shell   login_shell off\n    will mean that it is NOT login shell\nlogin_shell on\n    will mean that is IS login shell   Example for each type:",
            "title": "will echo all files from current dir matched by globbed(wildcard)"
        },
        {
            "location": "/bash_page/#interactive-non-login-shell-regular-terminal",
            "text": "$ echo $-; shopt login_shell\nhimBHs\nlogin_shell     off",
            "title": "Interactive, non-login shell. Regular terminal"
        },
        {
            "location": "/bash_page/#interactive-login-shell",
            "text": "$ bash -l\n$ echo $-; shopt login_shell\nhimBHs\nlogin_shell     on",
            "title": "Interactive login shell"
        },
        {
            "location": "/bash_page/#non-interactive-non-login-shell",
            "text": "$ ssh localhost 'echo $-; shopt login_shell'\nhBc\nlogin_shell     off",
            "title": "Non-interactive, non-login shell"
        },
        {
            "location": "/bash_page/#non-interactive-login-shell",
            "text": "$ echo 'echo $-; shopt login_shell' | ssh localhost\nPseudo-terminal will not be allocated because stdin is not a terminal.\nhBs\nlogin_shell     on      functions:\n  shell could have functions, funcions MUST BE declared before its call,\n  otherwise shell will interpret funcion names as regular commands\n   Functions could be executed and calld from shell, added to scripts or \n  bashrc/profile files so will be available in cmd as regular command\n  'return' command in optional\n   Function MUST has at least one command, in order to mock\\stab 'return'\n  command could be used.\n   Basically every script could be converted into a function just by\n  copuint all the contents after she-bang in between function declaration\n  which is in Syntax below. And replace 'exit' with 'return' keywords\n   Variables in the Function could be set as 'local' overriding outer\n  global variables.\n   $0 in fuction contains name of the script being executed\n   $FUNCNAME - contains name of the function being executed  Syntax:\n  function name {\n    local VAR\n    local VAR2=value\n     \n    return\n  }\n   Or equivalent:\n  name() {\n    \n   return\n  }  Example:  script.sh:\n  function func {\n   echo \"step 2\"\n   return\n  }\n  echo \"step 1\"\n  func\n  echo \"step 3\"  Result:   step 1\nstep 2\nste 3   Example .bashrc:\n  ds() {\n   echo \"disk space utilization for $HOSTNAME\"\n   df -h\n  }  Result after shell restarted \n   (after source .bashrc ds looped and crashed shell):\n  $ ds   disk space utili....\n...output of df -h....   |:\n  pipe character\n  creates new subshell, executes what goes after pipe there, then returns\n  back to parent shell. subshell can not change parent shell - so 'read' command\n  will not work in pipe  breaking line; new line:\n use \\ at the end of the line.\n  NO SPACES OR ANYTHING AFTER THE BACK SLASH.\n Guideline: split the long(80 chars) line into two lines.\n Also break the line everytime there is an &&, | or || characters.\n Example:\ncommand1 \\\n && command2 \\\n || command3 \\\n | command4  read:\n  reads line from keyboard input, accepts here-strings, does not work with |\n  By default saves entered value into $REPLY global variable\n   Synopsis:\n  read [-params] [variable1 variable2 variable3 ...]  Accepts more than 1 variable to save keyboard input\n   Default separator is space or tab or new line(\\n)\n  If less variables are given to save data, all excessive data (delimited \n  fields) will be saved in last variable (like msbuild do when too many params)\n  When too many variables is given - unpopulated vars remain empty string  Params:\n  -a - array, saves input into the array \n  -d   - delimiter, by default space\\tab\\newline\n  -e - use Readline, behaves like keyboard input in bash\n  -n   - read num symbols from keyboard input\n  -p   - display promt befor cursor on the same line (like bash promt)\n  -r - do not inerpret \\ symbols\n  -s - silent, do not display entered data, like in password field\n  -t - time out for wait of entry\n  -u   - use file with given descriptor as STDIN  Command grouping:\ncommand groups:\n Commands could be groupped, there are two types:\n  Groups:\n   { command1; command2; [command3; ... ] }\n  Subshell:\n   (command1; command2 [; command3 ... ])  Commands in Groups are executed in current shell, so ENV variables \n chaned during commands execution are saved into current Shell ENV\n  Subshells are executed in sub shell and current ENV could not be \n modified.  Examples:\n ls -l > output.txt\n echo \"listing of foo.txt\" > output.txt\n cat foo.txt > outpu.txt\n  ==\n { ls -l; echo \"listing of foo.txt\"; cat foo.txt; } > output.txt\n  or\n (ls -l; echo \"listing of foo.txt\"; cat foo.txt) > output.txt  NOTICE:\n  Groups must have spaces next to {}, and last command should end with ;\n  SUbshells do not need extra spaces and ;\n BUT:\n  subshells are taking longer to be executed because of extra subshell\n need to be created. Also more memory is required. \n  Also no parent shell ENV modifications is possible  Example with pipe:\n  { ls-l; echo \"listing of foo.txt\"; cat foo.txt; } | lpr\n this will redirect output through pipe into 'lpr' which is print command.\n  So all the output of 3 commands are redirected at once into 'lpr'\n which is not the same with \n  ls -l | echo \"...\" | cat foo.txt | lpr - will send only foo.txt to\n print  Processes substitution:\n <(commands list)  - processes sending output to STDOUT   (commands list)  - processes getting input to STDIN   Subshell output is interpreted as regular file, and could be redirected\n into another commands as usual.  Example:\n  read < <(echo \"foo\")\n  echo $REPLY - will return 'foo'\n where:\n  'read' reads from STDIN, and saves what is read into $REPLY by default\n     if executed in subshell $REPLY will not be available to \n     parent shell\n        echo \"foo\" | read - will not work\n  <     - STDIN redirection, so STDOUT from <() is redirected as STDIN\n      of 'read' command\n  <(echo \"foo\") - Process substitution, where 'ehco' executed in \n          subshell Group () sends its output to STDOUT.\n        In other words output of this part is represented as\n        usual file, which is redirected into 'read' STDIN\n  echo $REPLY - proves that $REPLY is updated in current shell with\n        value redirected out of subshell \n        due to process substitution  Result of substituted shell is stored as regular file and could be \n found line this:\n  echo <(echo \"foo\")   /dev/fd/63\n   Its contents could be listed:\n  cat <(echo \"foo\")\nfoo   trap:\n  Event subscription for system signals (SIG..) like SIGKILL etc.\n there are 64 signals, so some variety of situations.\n  In general this is the way to trigger script when some system sent some\n particular event to the script  Syntax:\n trap   signal [signal ...]\n  Where command is what will be executed in case 'signal' is sent to\n the executing script\n !  Also could be a function.  Example:\n #!/bin/bash\n trap \"echo 'i am ignoring you'\" SIGTERM SIGINT  this will return 'i am ...' if SIGTERM (kill -9) or SIGINT (CTRL+C) are\n sent to the script.  Also works with functions, so like lambda\\delegates  Example:\n  exit_on_SIGINT () {\n   echo \"script Interrupted\" 2>&1\n   exit 0\n  }  trap exit_on_SIGINT SIGINT  this will call exit_on_SIGINT() function when CTRL+C is received\n   and will redirect 'echo' into both STDERR and STDOUT\n  After event is handled, exit 0, will ensure exit of the script, as it\n  is expected by the SIGnal  temp files:\nrandom:\n  event handling (trap) usually used to delete temp files generated \n during script work. Such files could contain some secure info and stuff.\n  To secure from 'temp race attack' - when temp files being searched\n by 3rd party and read, files need to be named in unobvious way\n which means use random gibberish stuff to mask particular files.  Random could be used like $RANDOM\n $RANDOM var containes random number from 1 to 32 767, but this is not\n  really huge number  mktemp:\n  command that creates temporary file with name based on pattern\n  or directory\n   creates \n   Syntax:\n  mktemp  \n  mktemp [OPTION]... [TEMPLATE]  Create a temporary file or directory, safely, and print its name. \n  TEMPLATE must contain at least 3 consecutive 'X's in last  component.  \n  If TEMPLATE is not specified, use tmp.XXXXXXXXXX, and --tmpdir is \n  implied. \n   Files are created u+rw, and directories  u+rwx,  minus  umask \n  restrictions.  this will create file at given path, with every X being replaced\n  with random number or letter(random case)\n   NOTICE: in general convers 4+ X into random letters\\numbers\n  and only at the end of the file name\n   except if it has extention (like .txt)  Example:\n  #!/bin/bash\n  tempfile=$(mktemp ~/.tmp/XXX-$(basename $0).$$.XXXXXXXXX) \n  echo $tempfile\n   which will produce:\n  script output: >XXX-test.17306.PRsr7hXuU\n  $ ll XXX*\n  -rw------- 1 dos dos 0 \u043a\u0432\u0456  2 03:07 XXX-test.17306.PRsr7hXuU  Generally it is more safe to use custom temp directory instead of /tmp\n  Example:\n [[ -d ~/.tmp ]] || mkdir ~/.tmp\n  this will check if ~/.tmp exists, if not will create it\n  But in general mktemp also could create directories..   Asynchronous\nAsync async\n Bash could run async commands and tools.\n To run something in background is should ends with &\n  Example:   copy_script.sh &\n  copyscript will now run in background\n $! - returns PID of last started in background process\n  Example:\npid=$!   - will save PID of running copy_script.sh into pid variable   wait:\n waits for given process to finish, before continue execution\n  Example:\n wait $pid - will wait for copy_script.sh to finish before continue  Pipe\npipe\n Pipes are the files created in special pipefs filesystem\n  by default unnamed pipe are created for every | symbol in command chain\n in following command 2 pipes will be created:\n  ls -l | grep foo | less\n output of 'ls -l' will be redirected into pipeA\n input into 'grep' will be reead from pipeA\n STDOUT of 'grep' will be written into pipeB\n STDIN of 'less' will be read from pipeB  every command will be executed in subshell, need to clarify more  mkfifo:\n  creates Named pipe, which is stored as a file in regular file system\n   Syntax:\n  mkfifo /path/to/file\n   To write into named pipe:\n  command1 > named_pipe\n   To read from named pipe:\n  command2 < named_pipe  file will have attribute 'p':\n      prw-r-----\n   Example:\n  1st Terminal:\n   mkfifo pipe1\n   ls -l > pipe1 - will halt until everything is read out from pipe1\n  2nd Terminal\n   cat < pipe1 - will read everything from pipe1 and display, 1st terminal\n        will continue its work afterwards",
            "title": "Non-interactive login shell"
        },
        {
            "location": "/devops_conf_2019/",
            "text": "[Gianluca Abezzano] DevOps never sleeps. What we learned from influxDB v1 to v2\n\n\n\n\nDocker is pretty layered system, which is split even more recently after ContainerD is moved into OpenSource by Docker development team.\n\n\nThe layers if Docker infrastructure are:\n\n\n\n\nCLI\n\n\ncommand line interface for accessing Docker daemon  \n\n\n\n\n\n\nREST\n\n\nrestfull interface for accessing daemon. TODO: check whether CLI also uses it\n\n\n\n\n\n\ndockerd\n\n\ndocker service that accepts CLI and REST  calls and translates them further tocontainerd service(daemon)\n\n\nDocker Engine\n\n\n\n\n\n\ncontainerd\n\n\nservice that is moved into separate project. main purpose is to made this \nCore\n service to be fully crossplatform. \ncontainerd daemon exposes its API that is  used by dockerd and others parties interested in communication witn containerd in order to use services it provides.\nIt is currently used everywhere where containers and docker are  used - AWS, Azure and all the cloud providers that adopted docker as engine for containerization. \nAlso this is the core that runs on multiple OS such as: Linux, MacOS, Windows etc.  \n\n\n\n\n\n\nRunC\n\n\nTODO: write RunC description, and probably add more layers..\n\n\n\n\n\n\n\n\nTODO: which of above uses Linux Socket\\TCP Socket in order of communication with outer world.\n\n\nJenkins and k8s\n\n\nMain point in usage of docker infrastructure by '3rd parties' is in acquiring access to docker socket.\n\nIt could be illustrated by starting docker container with docker(sub) running inside it.\n\nSub docker could manipulate parent docker if it has access to parent docker's socker, for instance by mounting socket file to sub docker's container:  \n\n\ndocker run -v /var/run/docker.sock:/var/run/docker.sock -ti docker bash\n\n\n\nin this example docker socket is passed from Host system into the container so docker running in the container could manipulate Host's one.\n\n\nThis is  how Jenkins plugin and k8s work.\n\n\nTracing and effective logs monitoring\n\n\nIn order to trace different important events out of all the system information, unimportant warnings and errors information need to be correctly logged, aggregated and filtered.\n\nMetrics need to be used and calculated, and tracking of events throught different logs need to be applied to that. \n\n\nAfter all the logs are gathered, together with metrics and tracking applied to it, some particular thing could be Traced.\n\n\nMetrics alone is just like slap to the face, it is sudden and only  has shock effect.\n\n\nLogs in raw view are useless, the sence comes after aggregation and ability to filter it.\n\n\nState of the system.\n\n\nMonitoring of the system could be done by monitoring its State:\n\n\n\n\nHardware charasteritsitcs like CPU usage\n\n\nprocesses rununing and resource usage\n\n\nI\\O activities \n\n\netc\n\n\n\n\nBut just snapshot is usually not enough. \n\nLike 'ps -lte' - it could return actual and correct info but it will not gave any meaningful info in order which process works ineffectively or wrong.\n\nFor more efficient monitoring 'top' program will give much more info about processes and how they are utilizing system's resources.\n\nBut this gives realtime results and requres constant attention, it order to find which process does what.\n\nSo for effective monitor we need a tool that will save changes into history and ability to filter and even visualize the history.\n\n\nBut w/o any context even atual, filtered and visualized info makes no sence:\n\n\n\n\nNormal State.\n\n\n\n\nWhen everything in the system runs OK, and everybody is satisfied we could call it the Normal State of the system.\n\nThis state need to be saved with all the metrics calculated and attached, to be used later when need to determine how system works at given point of time.\n\n\n\n\nCurrent State\n\n\n\n\nAt any point of time we could apply all the filters to logs, calculate metrics and visualize statistics (even of some time period). This will give us the Current State.\n\nComparing this \nCurrent State\n with a 'golden standard' like \nNormal State\n will give the answer to the main question:\n\n\nWhether everything is fine with our system Now?\n\n\nDesign applications correctly in order to monitor them later\n\n\nIn order to monitor system state effectively we need the info in first place.\n\nInfo is the logs generated by apps which state we want to monitor.  \n\n\nWhen Application is developed logs generation should be keep in mind. In ideal way app should has API to access logging functionality from outside by other systems: \n\n\n[App [[Logs func + stuff ] API for logs access\\creation]]\n\n\n\nin that way some logs\\events storage facility could use API to ask app for some particular logs an particular point of time.\n\n\nCode Instrumentation (hello Bullseye)\n\n\nExisting code  also could be instrumented (i.e. during compilation)\n\n\nLater on during runtime this added parts will fire and write logs\\events somewhere ( i.e. into some stream). Also this added part could provide API access for other tools.  \n\n\nTools \\ Links\n\n\n\n\nELK(B)\n\n\nPrometheus(google)\n\n\nJaeger\n\n\nbullseye. not related to article, just example\n\n\nOpenCensus(google\\open source)\n\n\nmount-bind docker socket\n\n\n\n\n\n\n[Philipp Krenn - Elastic] Hands-On ModSecurity and Logging\n\n\nBasically this talk is about security breaches that could be logged and monitored uneffectively, thus alerting user too late and with lack of general information.\n\nTool that is demonstrated in order to prevent both breaches and bad monitoring is ModSecurity\n\n\nSecurity issues\n\n\n\n\nthe link\n\n\nThere could be gaps in app with security\n\n\nGaps could be covered outside the app with tools, that filter out traffic analizing its inner parts, like requests (i.e. request having 'update' sql directive will be filtered out in some case, and 'delete' almost everytime)\n\n\nSome traffic could be sniffed by the \nModSecurity\n, and it will make a decision of what to do , whether block traffic completely (nevertheless notifying the logs \n) or just log info\\warning\\error.  \n\n\n\n\nModSecurity is an \nopen source\n, \ncross-platform\n web application firewall(WAF) module.\n\n\nIt enables web pplication defenders to gain visibility into HTTP(S) traffic and provides a power \nrules language\n and \nAPI\n to implement advanced protections.\n\n\n\n\nSo in case ModSec decided that traffic package has some malwarish things this request to the web server could be blocked preventing anything from happening and end user will get 503 error or something similar to it\n\n\nLogging an monitoring issues\n\n\nThe app itself could write logs, the instrumented app could write logs, and even apps running atof of main app could add logging\n\n\nLogs are generated by ModSecutiry, then are gathered by  File Beat and fed into Logstash, which applies filters to match particular fields of the log to be filtered by or displayed in Kibana.  \n\n\nBasically after ModSecurity is turned on and set up, it generates logs which could be viewed in ELK, containing all the info from where package came from, when, what it had, and in general full statistics could be gathered to understand whole picture.\n\n\n\n\nLogs Enreachment\n\n\n\n\nLike taking IP from the logs and adding where it is from (country) to add meaning and context fo the logged info, w/o which it is pretty useless.  \n\n\nTools \\ Links\n\n\n\n\nElastic Stack: Beats\n\n\nSqlmap\n - tool to check website db for various vulnurabilities  \n\n\nModSecurity\n  \n\n\nModSecurity & Logging demo\n  \n\n\nterraform+ansible+aws+apache+modsecurity\n\n\n\n\n\n\npresentation\n local place: img/devops_conf_2019/presentations/ModSecLogging.pdf\n\n\n\n\n\n\n[Nikita Procenko - Netflix] Infrastructure-as-code: bringing the gap between Devs and Ops\n\n\nthe talk mostly about approaches, and terraform:\n\n\nImmutability of infrastructure\n\n\nTODO: write down about immutability - easier to recreate instancec than patch it, to avoid regression and stuff\n\n\nDeclarative vs Imperative\n\n\nImperative\n approach is when everything is explicitly designed. This is the list of detailed instructions of HOW TO do something.  \n\n\n\n\nbash, aws cli\n\n\n\n\nDeclarative\n on the other hand is abstract form where only END STATE is given. Basically Declarative approah is just a wrapper around Imperative code. Imperative part here is incapsulated out of sight of regular user.\n\nTake Ansible - the only user do is state that Foo program need to be on the system. And Ansible will ensure it - install, update, or do nothing if Foo already existts.  \n\n\n\n\nterraform, ansible\n\n\n\n\nExample:\n\n\nImperative C#:\n\n\nList<int> collection = new List<int> { 1, 2, 3, 4, 5 };\n\n# Imperative: get only Odd numbers\nList<int> results = new List<int>();\nforeach(var num in collection)\n{\n    if (num % 2 != 0)\n          results.Add(num);\n}\n\n# The code below has detailed instructions of what to do step-by-step\n\n# Declarative: get only Odd numbers  \nvar results = collection.Where( num => num % 2 != 0);\n\n# or even more Declarative:\nfrom item in collection where item%2 != 0 select item\n\n\n\n\n\ndeclarative vs imperative is more about \ndeclaring what you want to have\n happen vs. explaining exactly \nhow it needs to occur\n.\n\n\n\n\nTerraform, the same as Ansible are the higher level abstraction tools that are Declarative.\n\nThe user only declares what he wants - tool inistalled or EC2 Instance created from specific image within specific subnet with a specific name etc. and the Tool will ensure for the User that this happened.\n\n\nBash scripts on the other hand are Imperative. \n\nWhere everything is explicitly and in a detailed way is explained to the interpreter which will go step by step doing what it is told, and at the and we need to ensure that everything is in place as we Wanted it.\n\n\nread more\n\n\nIaC Demo:\n\n\nhttps://github.com/iac-demo\n\n\n\n\nOther at all:\n\n\ntech interview presentation\n starting #10 slide",
            "title": "Devops conf 2019"
        },
        {
            "location": "/devops_conf_2019/#gianluca-abezzano-devops-never-sleeps-what-we-learned-from-influxdb-v1-to-v2",
            "text": "Docker is pretty layered system, which is split even more recently after ContainerD is moved into OpenSource by Docker development team.  The layers if Docker infrastructure are:   CLI  command line interface for accessing Docker daemon      REST  restfull interface for accessing daemon. TODO: check whether CLI also uses it    dockerd  docker service that accepts CLI and REST  calls and translates them further tocontainerd service(daemon)  Docker Engine    containerd  service that is moved into separate project. main purpose is to made this  Core  service to be fully crossplatform. \ncontainerd daemon exposes its API that is  used by dockerd and others parties interested in communication witn containerd in order to use services it provides.\nIt is currently used everywhere where containers and docker are  used - AWS, Azure and all the cloud providers that adopted docker as engine for containerization. \nAlso this is the core that runs on multiple OS such as: Linux, MacOS, Windows etc.      RunC  TODO: write RunC description, and probably add more layers..     TODO: which of above uses Linux Socket\\TCP Socket in order of communication with outer world.",
            "title": "[Gianluca Abezzano] DevOps never sleeps. What we learned from influxDB v1 to v2"
        },
        {
            "location": "/devops_conf_2019/#jenkins-and-k8s",
            "text": "Main point in usage of docker infrastructure by '3rd parties' is in acquiring access to docker socket. \nIt could be illustrated by starting docker container with docker(sub) running inside it. \nSub docker could manipulate parent docker if it has access to parent docker's socker, for instance by mounting socket file to sub docker's container:    docker run -v /var/run/docker.sock:/var/run/docker.sock -ti docker bash  in this example docker socket is passed from Host system into the container so docker running in the container could manipulate Host's one.  This is  how Jenkins plugin and k8s work.",
            "title": "Jenkins and k8s"
        },
        {
            "location": "/devops_conf_2019/#tracing-and-effective-logs-monitoring",
            "text": "In order to trace different important events out of all the system information, unimportant warnings and errors information need to be correctly logged, aggregated and filtered. \nMetrics need to be used and calculated, and tracking of events throught different logs need to be applied to that.   After all the logs are gathered, together with metrics and tracking applied to it, some particular thing could be Traced.  Metrics alone is just like slap to the face, it is sudden and only  has shock effect.  Logs in raw view are useless, the sence comes after aggregation and ability to filter it.",
            "title": "Tracing and effective logs monitoring"
        },
        {
            "location": "/devops_conf_2019/#state-of-the-system",
            "text": "Monitoring of the system could be done by monitoring its State:   Hardware charasteritsitcs like CPU usage  processes rununing and resource usage  I\\O activities   etc   But just snapshot is usually not enough.  \nLike 'ps -lte' - it could return actual and correct info but it will not gave any meaningful info in order which process works ineffectively or wrong. \nFor more efficient monitoring 'top' program will give much more info about processes and how they are utilizing system's resources. \nBut this gives realtime results and requres constant attention, it order to find which process does what. \nSo for effective monitor we need a tool that will save changes into history and ability to filter and even visualize the history.  But w/o any context even atual, filtered and visualized info makes no sence:   Normal State.   When everything in the system runs OK, and everybody is satisfied we could call it the Normal State of the system. \nThis state need to be saved with all the metrics calculated and attached, to be used later when need to determine how system works at given point of time.   Current State   At any point of time we could apply all the filters to logs, calculate metrics and visualize statistics (even of some time period). This will give us the Current State. \nComparing this  Current State  with a 'golden standard' like  Normal State  will give the answer to the main question:  Whether everything is fine with our system Now?",
            "title": "State of the system."
        },
        {
            "location": "/devops_conf_2019/#design-applications-correctly-in-order-to-monitor-them-later",
            "text": "In order to monitor system state effectively we need the info in first place. \nInfo is the logs generated by apps which state we want to monitor.    When Application is developed logs generation should be keep in mind. In ideal way app should has API to access logging functionality from outside by other systems:   [App [[Logs func + stuff ] API for logs access\\creation]]  in that way some logs\\events storage facility could use API to ask app for some particular logs an particular point of time.",
            "title": "Design applications correctly in order to monitor them later"
        },
        {
            "location": "/devops_conf_2019/#code-instrumentation-hello-bullseye",
            "text": "Existing code  also could be instrumented (i.e. during compilation)  Later on during runtime this added parts will fire and write logs\\events somewhere ( i.e. into some stream). Also this added part could provide API access for other tools.",
            "title": "Code Instrumentation (hello Bullseye)"
        },
        {
            "location": "/devops_conf_2019/#tools-links",
            "text": "ELK(B)  Prometheus(google)  Jaeger  bullseye. not related to article, just example  OpenCensus(google\\open source)  mount-bind docker socket",
            "title": "Tools \\ Links"
        },
        {
            "location": "/devops_conf_2019/#philipp-krenn-elastic-hands-on-modsecurity-and-logging",
            "text": "Basically this talk is about security breaches that could be logged and monitored uneffectively, thus alerting user too late and with lack of general information. \nTool that is demonstrated in order to prevent both breaches and bad monitoring is ModSecurity",
            "title": "[Philipp Krenn - Elastic] Hands-On ModSecurity and Logging"
        },
        {
            "location": "/devops_conf_2019/#security-issues",
            "text": "the link  There could be gaps in app with security  Gaps could be covered outside the app with tools, that filter out traffic analizing its inner parts, like requests (i.e. request having 'update' sql directive will be filtered out in some case, and 'delete' almost everytime)  Some traffic could be sniffed by the  ModSecurity , and it will make a decision of what to do , whether block traffic completely (nevertheless notifying the logs \n) or just log info\\warning\\error.     ModSecurity is an  open source ,  cross-platform  web application firewall(WAF) module.  It enables web pplication defenders to gain visibility into HTTP(S) traffic and provides a power  rules language  and  API  to implement advanced protections.   So in case ModSec decided that traffic package has some malwarish things this request to the web server could be blocked preventing anything from happening and end user will get 503 error or something similar to it",
            "title": "Security issues"
        },
        {
            "location": "/devops_conf_2019/#logging-an-monitoring-issues",
            "text": "The app itself could write logs, the instrumented app could write logs, and even apps running atof of main app could add logging  Logs are generated by ModSecutiry, then are gathered by  File Beat and fed into Logstash, which applies filters to match particular fields of the log to be filtered by or displayed in Kibana.    Basically after ModSecurity is turned on and set up, it generates logs which could be viewed in ELK, containing all the info from where package came from, when, what it had, and in general full statistics could be gathered to understand whole picture.   Logs Enreachment   Like taking IP from the logs and adding where it is from (country) to add meaning and context fo the logged info, w/o which it is pretty useless.",
            "title": "Logging an monitoring issues"
        },
        {
            "location": "/devops_conf_2019/#tools-links_1",
            "text": "Elastic Stack: Beats  Sqlmap  - tool to check website db for various vulnurabilities    ModSecurity     ModSecurity & Logging demo     terraform+ansible+aws+apache+modsecurity    presentation  local place: img/devops_conf_2019/presentations/ModSecLogging.pdf",
            "title": "Tools \\ Links"
        },
        {
            "location": "/devops_conf_2019/#nikita-procenko-netflix-infrastructure-as-code-bringing-the-gap-between-devs-and-ops",
            "text": "the talk mostly about approaches, and terraform:",
            "title": "[Nikita Procenko - Netflix] Infrastructure-as-code: bringing the gap between Devs and Ops"
        },
        {
            "location": "/devops_conf_2019/#immutability-of-infrastructure",
            "text": "TODO: write down about immutability - easier to recreate instancec than patch it, to avoid regression and stuff",
            "title": "Immutability of infrastructure"
        },
        {
            "location": "/devops_conf_2019/#declarative-vs-imperative",
            "text": "Imperative  approach is when everything is explicitly designed. This is the list of detailed instructions of HOW TO do something.     bash, aws cli   Declarative  on the other hand is abstract form where only END STATE is given. Basically Declarative approah is just a wrapper around Imperative code. Imperative part here is incapsulated out of sight of regular user. \nTake Ansible - the only user do is state that Foo program need to be on the system. And Ansible will ensure it - install, update, or do nothing if Foo already existts.     terraform, ansible   Example:  Imperative C#:  List<int> collection = new List<int> { 1, 2, 3, 4, 5 };\n\n# Imperative: get only Odd numbers\nList<int> results = new List<int>();\nforeach(var num in collection)\n{\n    if (num % 2 != 0)\n          results.Add(num);\n}\n\n# The code below has detailed instructions of what to do step-by-step\n\n# Declarative: get only Odd numbers  \nvar results = collection.Where( num => num % 2 != 0);\n\n# or even more Declarative:\nfrom item in collection where item%2 != 0 select item   declarative vs imperative is more about  declaring what you want to have  happen vs. explaining exactly  how it needs to occur .   Terraform, the same as Ansible are the higher level abstraction tools that are Declarative. \nThe user only declares what he wants - tool inistalled or EC2 Instance created from specific image within specific subnet with a specific name etc. and the Tool will ensure for the User that this happened.  Bash scripts on the other hand are Imperative.  \nWhere everything is explicitly and in a detailed way is explained to the interpreter which will go step by step doing what it is told, and at the and we need to ensure that everything is in place as we Wanted it.  read more",
            "title": "Declarative vs Imperative"
        },
        {
            "location": "/devops_conf_2019/#iac-demo",
            "text": "https://github.com/iac-demo   Other at all:  tech interview presentation  starting #10 slide",
            "title": "IaC Demo:"
        },
        {
            "location": "/interview/",
            "text": "Question: Do you have any question:\n\n\nMy Possible questions:\n\n\nLooks Good  \n\n\n\n\n\n\nthings\\skills that team is missing that manager is going to fill with me  \n\n\n\n\n\n\nare there documentation\n\n  how it is created and updated\n\n  how is it maintained up to date\n\n\n\n\n\n\nwhat is the current biggest point\\priority\n  what is the biggest challanges team is going to face in the next year\n\n\n\n\n\n\nwhat is the career growth paths in the company\n\n\n\n\n\n\nwhat does success look like for this position\n\n\n\n\n\n\nLooks normal  \n\n\n\n\n\n\nwhat in your opinion is DevOps \\ what DevOps is to them\n\n  how aligned the company with the said opinion\n\n\n\n\n\n\nhow team deals with production incidents\n\n\n\n\n\n\nwhat is favorite thing working here besides the peole\n\n\n\n\n\n\nwhat frsutrates you about working here\n\n\n\n\n\n\nGeneral questions:\n\n\n\n\nhow is performance feedback done\n\n  how is 1 on 1\n\n\n\n\nBetter not to ask:",
            "title": "Interview"
        },
        {
            "location": "/interview/#question-do-you-have-any-question",
            "text": "My Possible questions:  Looks Good      things\\skills that team is missing that manager is going to fill with me      are there documentation \n  how it is created and updated \n  how is it maintained up to date    what is the current biggest point\\priority\n  what is the biggest challanges team is going to face in the next year    what is the career growth paths in the company    what does success look like for this position    Looks normal      what in your opinion is DevOps \\ what DevOps is to them \n  how aligned the company with the said opinion    how team deals with production incidents    what is favorite thing working here besides the peole    what frsutrates you about working here    General questions:   how is performance feedback done \n  how is 1 on 1   Better not to ask:",
            "title": "Question: Do you have any question:"
        },
        {
            "location": "/linux_cheatsheet/",
            "text": "==========TODO:\nadd lshw lspci and whole setup and recheck of wifi. iwlwifi driver and \nway to ensure there are drivers installed in the kernel\n\n\ncreate bash script that will export variables to:\n1. current session\n2. user .bashrc file so it will be available in other sessions\n3. system-wide\n\n\ncheck distribution and update according files for\n1. user-wide\n2. system-wide \n\n\ndo the same for Bash Aliases\n\n\nSave this script into this repo\n\n\n===\nidentify myself on git\n==========todo\n\n\nContents\n===General\n===Env Vars / Environment variables:\n===User\\Groups Management\n=====User\\Groups Management General info\n=====Manipulate Users And Groups\n=====Super User\n===Processes and Services Management\n===Package management\n=====Debian\\Ubuntu\n=======Uninstallation\n=======Repository setup\n=======List all installed packages:\n=====RedHat RHEL\\Centos\n=======Manage repositories\n=======Uninstallation\n=======List all installed packages:\n===File Permissions / Ownership\n===IPTables / Linux kernel Firewall(not d)\n===Bash\n=====Bash command line shortcuts\n=====Bash Shell\n=====Bash Scripting\n===Nginx\n===Networking\n====Ports checks\n\n\n===General\ncheck linux distro:\n  uname -a - returns info about kernel and stuff\n  cat /etc/*-release - return info about distro\n  lsb_release - command that returns info about distro\n  cat /proc/version - file with linux version info\n\n\nfacter:\n  tool to get info about the system, hardware, network, user, distro..\n  121 property in total\n  Useful for Ansible, to get different facts like os name.\n   Ubuntu: sudo apt-get install facter\n   CentOS: yum install facter # enable EPEL repo\n\n\ndirectories structure:\n / - root\n /bin - binaries for system boot, goes with distro\n /boot - kernel boot files and RAM disc with drv \n  for boot, and boot program itself\n  /boot/grub/grub.conf - boot config\n  /boot/vmlinuz - kernel\n /dev - all devices are here\n /etc - configs all here\n  /ect/crontab - cron config\n  /etc/fstab - mounts config\n  /etc/passwd - users list\n /home - contains all users' dirs\n  ~/bin - bins of particular user, add to PATH to use\n /lib - shared by various programs libraries(DLLs), probably for /bin's\n /lost+found - if HDD feels bad, some stuff could appear here\n /media - user-related mount points for flash, hdd, cd etc \n   which are mounted manually\n /opt - optional installed software and other stuff\n   mostly used for commercial software\n /proc - files to peek at kernel things\n /root - root's home dir\n /sbin - same as /bin\n /tmp - temp used by various programs, could be cleaned at boot\n /usr - programs and files used by users\n /usr/bin - programs from distro, almost everythign already installed goes here\n /usr/lib - shared libraries for /usr/bin programs\n /usr/local - empty by default, available for all users on system\n  /usr/local/bin - locally compiled\\created bins should go here\n  /usr/local/sbin - locally created system bins for administration\n /usr/sbin - System bins, come with distro, i suppose\n /usr/share - shared stuff(configs, files, images) except shared libs\n /usr/share/doc - mans for installed soft go here\n /var - variables, most changed dir, contains DBs, buffers, emails\n /var/log - all logs go here\n\n\nman:\n  https://www.howtogeek.com/663440/how-to-use-linuxs-man-command-hidden-secrets-and-basics/\n  man [section] \n\n  man sections:\n   1. General commands\n   2. System calls\n   3. C library functions\n   4. Special files (usually devices, those found in /dev) and drivers\n   5. File formats and conventions\n   6. Games and screensavers\n   7. Miscellanea\n   8. System administration commands and daemons\n  Example:\n   man 2 stat - will display manual for hashed 'stat'\n    man has stat(1) and stat(2), which are different commands..\n  -f \n  - search for all entries of command and return \nwhatis\n\n    Example:\n   man -f man\n    will return man(1) and man(7) and short whatis of both\n  -k \n   - same as apropos - search through man, return line matched\n     if . given - it will match EVERYthing\n\n\nCount all man pages:\n    $ man -k . | wc -l\n    > 6659 \n      number of manpages on the system\n\n\napropos:\n  searches through man by keyword and returs matched in format:\n   article (secion) - line matched\n\n\necho:\n  accepts input from params or STDIN and returns in expanded to STDOUT\n  accepts multyline input, and will return it in the same manner:\n   echo \"this is firstline\n    this is second line\n    this is third line\"\n   Params:\n  -n - supress new line after output, like '.Write()' vs '.WriteLine();\n  -e - enable interpretation of \\ escaped stuff\n\n\nwhatis:\n  returns first line from man of given command\n   Example:\n  dos:~$ whatis ls\n\n\n\n\nls (1)               - list directory contents\n\n\n\n\ninfo:\n  same as 'man' for GNU software\n\n\npwd:\n  print working directory \n  returns string with current path location\n   Example:\n  pwd  - will return path where you are now\n   Example inside linked path:\n    https://unix.stackexchange.com/questions/63580/resolving-symbolic-links-pwd\n   $ type -a pwd\n   pwd is a shell builtin\n   pwd is /bin/pwd\n   $ mkdir a\n   $ ln -s a b\n   $ cd b\n   $ pwd           <<< this will display path as if there is no link\n   /home/michas/b\n   $ /bin/pwd\n   /home/michas/a  <<< this will display real path including link redirection\n\n\ncd:\n  change directory\n   Params:\n  \n - changes dir to the folder\n  -/\n   - changes to current homedir\n  ~username - changes to username homedir\n\n\ntree:\n  lists directories as trees, very useful\n   https://www.ostechnix.com/view-directory-tree-structure-linux/\n  sudo apt install tree\n\n\nExample:\n  tree .terraform/\n   will show everything inside .terraform directory in tree-view\n  tree -h --du .terraform/\n   will show sizes in human readable, and for directories too\n\n\nls:\n  list files in given directory or current one\n   Params:\n  -l - long format (list basically)\n    Columns:\n    access rights | hard links qty | owner | owner group | \n    | size | data changed | name\n  -a - show all files including hidden \n  -d - show only current directory, if given another directory - shows only it\n       if given a wildcard that will match directories only - will return only\n       the directories, w/o -d it will return every directory and its contents\n    Example:\n     ls -lda \n/ - will return list of directories\n    Example:\n     ls -la \n/ - will return contents of each directory within a separate list\n  -r - reverse sorting order\n  -S - sort by size\n  -t - sort by date changed\n  -F - adds slash to dirs in list, * to g+r files and stuff\n  -p - appens slash to dirs\n  -h - human readable sizes (mb, gb etc)\n  -i - inode number, address where contents is located, same for all hard links\n  -I - ignore; dont list entries matching shell pattern\n\n\nExample:\n  ls -lSrh - sory by size, in reverse order with human readable sizes\n\n\nlocale:\n  displays all Locale related env variables, like\n   LANG - set up during installation, picks install language\n   LC_MONETARY - currency? \n   etc.\n\n\ntime:\n  seems like calculates time of how long command takes to execute\n  like StopWatch\n   Syntax:\n  time \n\n   Example:\n  time some_script.sh\n\n\n\n\nreal 0m14.169s  #i spend 14 seconds (script uses 'read')\nuser 0m0.005s   #this two seems to calculate actual time of\nsys  0m0.005s   #+ calculations required, except manual intput\n\n\n\n\ndate:\n  returns current date - weekday, day, month, year, time, time-zone\n   Could be formatted like:\n  date +\"%x %r %Z\"\n   where\n   x - short date in American format like '20.02.2019' - dot delimited\n   r - time hh:mm:ss (dunno whether its 12 hours or 24 hours format)\n   Z - short name of time zone like 'EET' for Eastern European Time\n\n\ncal:\n  calendar, spans for several lines , looks like ok calendar\n\n\nfile:\n short info about a file, file type etc.\n  Linux does not have extensions for files so this is \n the sure way to acquire such info\n\n\ngrep:\n  Global Regular Expression Print\n  filters output using regular expressions\n  grep \n \n \n\n  -v - inverts results\n   ps aux | grep bash |grep -v grep - will return only real bash w/o grep \n    process listed\n   '[b]ash' will also do the trick, dunno why\n    why it works: \n     https://askubuntu.com/questions/153419/how-does-this-tricky-bracket-expression-in-grep-work\n TL;DR - ps returns process of grep with a parameter '[f]irefox', and\n grep it its turn searches for 'firefox' this pattert will not match string\n from ps output with displayed grep params that includes braces\n So basically string 'grep [f]irefox'(returned by ps aux) will not be \n matched by regular expression '[f]irefox' (executed by grep)\n Params:\n  -i  - case INsensitive\n  \n - is not required if used in pipe\n  -c - Count matched lines, return number of lines matched\n  [] = search for a character\n     i.e.: grep [abzfh] file - will search for appearance of any character \n        between square brackets in the file, standard regex\n  -f \n - pass contents of filename as a pattern\n  -l - search inside file contents and returns names of files found\n  -L - like -v but for -l - show only not matched files\n  -n - line number for matches\n  -h - no file names in output \n  -r - recurse search in a directory\n      Example:\n    grep -lr cron /etc \n       will return all file names which content has\n       'cron' pattern in whole /etc directory\n  -E - extended regular expressions, same as 'egrep'\n  -F - Fixed grep, same as 'fgrep'\n   Search in file:\n   Example:\n  grep pattern filename.txt\n   Recurse Example:\n  grep -R pattern *\n\n\nfgrep:\n  interprets regex special symbols as usual ones, and searches text with it\n  searches FAST and simple\n   fgrep ^hello file.txt - will find for literal '^hello' and not for hello at\n            the beginning of the line in the file file.txt\n\n\negrep:\n  allows to use Extended regular expressions\n   accepts same Parameters as regular 'grep'\n  it allows signs as:\n   | ? + .* {} \n   {3} - match 3 times exactly\n    Example:\n  egrep 'AAA|BBB|CCC'   - will match either AAA BBB or CCC\n  egrep '^(AAA|BBB|CCC)'   - will match either AAA BBB or CCC on the beginning\n    of the line\n\n\nstat: \n  returns status info about given file, if it dir, or file, when it was changed,  its size and stuff\n   man 2 stat - for details\n\n\nfind:\n  find \n \n \n\n   !!!  ORDER IS IMPORTANT\n   Example:\n  find / -type f -name \"one.tx*\" <-ls>\n\n\nOperations:\n  -ls - will output results as in 'ls -l'\n  -delete - delete found file. Better check first with -print, then -delete\n  -print - prints full path to stdout, Default operation\n  -print0 - changes delimiter to null character (0 code in ASCII), useful for \n    filenames with spaces, when need to use another delimiter. \n    Together with 'xargs' with --null, will go fine:\n     find -name '\n.jpg' -print0 | xargs --null ls -l\n    Example:\n    Effective:\n   find /u01/app/oracle/product/db11g/rdbms/audit/ -type f -print0 | xargs -0 rm\n    Ineffective:\n   find /u01/app/oracle/product/db11g/rdbms/audit/ -type f -name '\n' -mtime +1 -exec rm {} \\; &\n     Note: -name \"\n\" is also redundant - default behavior\n    xargs will take a number of items up max command length and then run it. \n     So in the previous example rm would run once for each file. With xargs it \n    would run a couple of time with many files on the rm command at onces.\n     The find -print0 and xargs -0 option uses the nul character instead of \n    the default newline character to terminate items. This means it will work with file names with spaces or other white space.\n  -quit - quits after first match\n  -regex - will search with regex, whole path need to be matched in order\n    to find it:\n     Example:\n    find . -regex '.\n[^-\n./0-9a-zA-Z].\n'\n     will match whole path(.\n at both sides), where files have spaces\n     (spaces are not in [^....] group - files in the group are acceptable\n     in a filename)\n  -ok - ask before execution, same as exec, see below\n  -exec command '{}' \\; - executes a command for _EVERY\n match, so 100 matches\n    is 100 times executed commands, substitutes found value in {}\n    with ; as a command end delimiter.\n    {} and ; are special for Bash, so need to be escaped by '' or \\ \n     Use + instead of ; in the end to concatenate results and execute \n    command just once\n     Also instead of + 'xargs' command could be used\n     This could be helpful as command line has limit on arguments (xargs\n    restarts when arguments limit is reached until everything done)\n    Example:\n   find . -name \"test\n\" -exec stat '{}' \\;\n    will find files and folders and display results of hashed 'stat' for \n    every matched item\n    Example:\n   find . -name \"test\n\" -exec stat '{}' +\n    will do the same as above, but 'stat' will be executed only once with list\n    of found files - stat testmatch1 testmatch2 testmatch3\n     Example mass rename:\n    find . -depth -name \"\n\" -exec sh -c 'f=\"{}\"; mv -- \"$f\" \"${f%}.txt\"' \\;\n     rename files w/o extention into files with .txt extention\n     Example:\n   find . -name \"test\n\" | xargs stat\n     Example:\n    Effective remove:\n    instead of this:\n   find /u01/app/oracle/product/db11g/rdbms/audit/ -type f -name '*' -mtime +1 -exec rm {} \\;\n    use this:\n   find /u01/app/oracle/product/db11g/rdbms/audit/ -type f -print0 | xargs -0 rm\n     Details:\n    xargs will take a number of items up max command length and then run it. \n     So in the previous example rm would run once for each file. With xargs it \n    would run a couple of time with many files on the rm command at onces.\n     The find -print0 and xargs -0 option uses the nul character instead of \n    the default newline character to terminate items. This means it will work \n    with file names with spaces or other white space.\n\n\nChecks:\n  -type f:\n    d - dir\n    f - file\n    l - sym link\n    b - block device (like /dev/sda)\n    c - char device (also in /dev , dunno what it is tho)\n  -name \"name\n\" - name or wildcard in \"\" - to prevent bash expansion\n  -iname \"Name\n\" - case Insensitive\n  -size nM:\n    b - block, default,            512 bytes\n    c - char(byte)           1 byte\n    w - words (two bytes)        2 bytes\n    k - kilobytes, block         1 024 bytes\n    M - megagytes, block     1 048 576 bytes\n    G - gigabytes, block 1 073 741 824 bytes\n    Example:\n   find . -size +10M  - will find with size more than 10 megabytes\n   find . -size 10M - will find exactly of 10 megabytes\n   find . -size -10M - will find with size less than 10 megabytes\n   find . -size +1G -exec mv {} delme/. \\; - move everything more than 1Gb into another dir 'delme'\n  -cmin n - files\\dirs with attrs\\contents changed exactly n minutes ago\n    supports +\\ - like -size\n  -mmin n - files\\dirs whos contents changed n minutes ago\n  -cnewer name: files\\dirs with attrs\\contents changed later than \n    given file name\n  -newer name: files\\dirs with contents changed later than given\n    file name\n  -ctime n - same as cmin, but n*24 hours is used\n  -mtime n - same as mmin\n  -empty - will match empty files and dirs\n  -inum n - search for files with inode n, useful to search for hard links\n  -samefile name - works same with -inum\n  -user name - files\\dirs owned by given user\n  -nouser - files\\dirs owned by noone, like deleted user\n  -group group_name - will search for files\\dirs owned by given group\n  -nogroup - same but for groups\n  -perm 777 - search by permission\n\n\nOperands:\n\n\n\nOperands glues Checks\\Operands and Groups of Checks:\n  -and-a - logical AND - default value\n  -or-o - OR\n  -not-! - NOT\n  ( ) - group of Checks. Need to be Escaped, as per is special for bash.\n    By default checks goes from left to right, so\n    ORDER IS IMPORTANT\n   Example: \n  find . ( -type l -or -type d ) -a (-name \"this\" -or -name \"that\" )\n   Example:\n  find . -type f -name \"\n.bak\" -print\n   all files with name \n.bak will be printed\n  find . -print -type f -name \"*.bak\"\n   print everytghint\n\n\nParams:\n\n\n\n-depth - process files first, then directories, Default for -delete\n  -maxdepth levels_number - how deep to dig\n  -mindepth levels_humber - go that deep before perform search\n    seems like value 2 will ignore current dir, and 1 will not\n  -mount - do not search in mounted directories\n    but still searches if explicitly set to search inside mounted dir\n  -noleaf - do not optimize search assuming its Unix filesystem\n    required for DOS\\WIndows CD-ROM systems\n    But works fine without in on NTFS, probably required for exactly CD?\n\n\ncould search by: \n   -date:\n      https://www.cyberciti.biz/faq/linux-unix-osxfind-files-by-date/\n   Ranges example:\n      -m\\a\\ctime - modification\\acces\\creatioin? time <+\\ -days> \n      - m\\a\\cmin - minutes i.e.:\n      find . -mtime -1 -ls - find files modified less than day ago\n      find . -mtime +1 -ls - all filed modified more than 1 day ago\n      find . -amin 1 -ls - all files accessed exactly 1 minute ago\n      -newerXY ; where XY could be:\n        a \u2013 The access time of the file reference\n        B \u2013 The birth time of the file reference\n        c \u2013 The inode status change time of reference\n        m \u2013 The modification time of the file reference\n        t \u2013 reference is interpreted directly as a time\n       i.e.:\n       find . -type f -newermt 2017-09-24 -ls\n        will find all files modified on 24/sep/2017\n\n\nby type\n f - file\n d - directory\n l - symlink and others\nby user:\n      TODO\nby group:\n -group <name_of_group>\n\n    by permissions:\n\n\n\n-perm - find by permission like Read\\Write\\Execute\n   it seems to be pretty similar to just '-executable' flag(switch?)\n    find /usr -type f -executable  - like this\n  details here http://www.tutonics.com/2012/12/find-files-based-on-their-permissions.html\n  i.e.:\n    find / -perm 644 - will match EXACT permission files, only that have 644\n    find / -perm -644 - will match files with at least 644 permission\n     -perm -u+rw,g+r,o+r - same as above, will match 654 and dont 634\n    find / -perm /644 - will match all files that have at least one of 3 sets\n     /222 - match will occur if either the owner, the group, or other have their            \"write\" bit set.\n     -perm /u+w,g+w,o+w - same as above, where u-user, g-group, o-owner\n     -perm /a+w - same as above where a-all + or = is same so: /a=w \n\n\ncould execute commands on what found:\n   find ~ -iname test.txt -exec du -h {} \\;\n    search in home dir for test.txt, case Insensitive, execute du -h for each\n    found file. SEE '-exec' in man\n    {} - is where the each find result will be piped.\n    find and copy:\n    !use with caution!  find . -name \"*.pdf\" -type f -exec cp {} ./pdfsfolder \\;\n  !see man for lots of details\n  NOTE: with Permission denied erros, and probably others\n   clear output of found stuff could be redirected to file. probably STDOUT\n   while STDERR will keep appear on terminal window\n\n\nxargs:\n  takes arguments for given command from stdin and executes given command with\n   that arguments.\n  If amount of arguments are more than a limit of command line xargs will \n   restart given command with what left of arguments list from stdin until \n   handle everything.\n    Params:\n  --show-limits - will display various limits on the system - command length,\n    argument length, how many take the env vars, buffer size, etc.\n  --null - accepts null symbol as delimiter, useful when filenames have\n    spaces, and need to use another delimiter\n\n\nExample:\n  find . -name \"test*\" | xargs ls -l\n   will construct ls -l test1 test2 test3 .... and execute it\n\n\nlocate:\n  find files by name.\n   NOTE: not all distros have it preinstalled\n  call 'updatedb' before use, to update database used by locate\n  locate \n - will return path to file if found\n   i.e.\n  locate kernel | grep /usr - will find any file contains 'kernel' in its name\n   and grep /usr so it will show up only those which are in /usr dir(matched by)\n    Params:\n  --regex - search with regex\n    Example:\n   locate --regex '(^/|^/usr/)[s]?bin/(dz|gz|zip)'\n    will search in /usr/bin or sbin or /bin or sbin for files having dz or gz or\n   zip in their names\n\n\nupdate files:\n touch \n - will update access time of a file, or create new if none\n echo \"\" > \n - create\\rewrite filename with blank line\n echo \"text\" >> \n - append existing file with line 'text'\n\n\nupdate-alternatives:\n  manages symbolic links i.e.:\n   sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 1 - python3.5 as first list item\n   sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 2 - python3.6 as second list item\n   sudo update-alternatives --config python3 - will print the list and ask\n    for number to point to for default usage, 2 will pick python3.6 for\n    python3 entry\n  See here: \n   http://ubuntuhandbook.org/index.php/2017/07/install-python-3-6-1-in-ubuntu-16-04-lts/\n\n\nshuf:\n shuffles the deck lol\n generages random output for range\n -i num-num - input range\n -n num - number of items from the list to display\n  Example:\n shuf -i 20-25\n  will output 5 numbers in range in random order\n shuf -i 20-25 -n 1\n  will output 1 number from the range\n\n\nsort:\n  sorts by alphabet by default\n   Synopsis:\n  Sort [option] file\n   Params:\n  -r - reverse\n  -R - random sort\n  -n - sort by numbers\n  -b - ignore leading blanks i.e. '   a'\n  -f - ignore case\n  -h - sor human readable format, like 1M 1K 1G\n  -k - key i.e. -k 1, or --key=1\n    -k 5 (or --key=5) - use column 5\n     Example: ls -l /usr/bin | sort -k 5 -rn\n    will sort files by field 5 (size) by Numberic and Reverse\n    --key=1,1 - column from 1 to 1\n    --key=2nr  - use second column to sort groups after first key\n            accepts short params after column num: b, n, r\n     Example: sort --key=1,1 --key=2n file.txt\n    sorts by column 1(diapason is important), then sorts by second field\n    numerically - will resort each just sorted group by second field:\n     abc 1   - w/o --key=2n this will be second line\n     abc 10\n    --key=3.7 - column number.symbol number. Use b, just in case\n     Example: sort -k 3.7nbr > t.txt ... dates\n    will sort by third column and 7th symbol in it, by year 01.01.2006 in \n    this dd.mm.yyyy format\n  -m - merge sorted files i.e. sort -m file1 file2 file3 > merged_file\n  -o - output to file i.e. -o=file\n  -t - change separator, default is 'space' and 'tab'\n     Example:\n    sort -t ':' -k 7 /etc/passwd\n    will sort by 7th column using : as field separator\n\n\nuniq:\n  deletes duplicates from SORTED output\n   Params: \n  -d - displays only duplicets (kinda inversion)\n  -c - displays number of matches and line (displays all lines) \n     Example:\n    sort test.txt | uniq -c\n    will return lines in format '\n \n'\n  -f n - ignore(forward to) n fields in each line, \n    delimiter is as in 'sort' but CANNOT be changed\n  -i - ignore case\n  -s n - skip n symbols in line\n  -u - unique lines only\n\n\n!Note: to get comparable output use 'sort' otherwice could display stuff in a\n    way whcih will be not comparable. see ls examples below \n\n\nExample:\n  ls /bin /usr/bin | sort | uniq | less - will display combined unique files in\n    both directories, in sorted order\n  ls /bin /usr/bin | sort | uniq -d | less - will display only duplicated files\n    in both dirs, in sorted order\n\n\ncat:\n  concatenate, return file contents on screen. GNU\n  cat \n \n - will return contents of both to screen\n  cat \n \n >> \n - create new\\append old file with joined\n   contents of file1 and 2\n  cat file.* > file - will combine lots of files\n  cat > newfile - will create a new file and promt for input \n   Params:\n  -A - shows all hidden\\meta symbols\n    ^I - tab\n    $ - end of the line\n  -n - show line numbers\n  -s - squeeze blank lines, leaves only 1 blank if 1+ exists concequently\n\n\nless:\n  pager, upgraded version of 'more' command\n  lists output in separate 'window' does not adding history in the console\n  ALSO could be used like 'unzip' to view archives\n\n\nsplit:\n  splits file into another files, like with collection variable\n  split -l 2 \n -  will split the file by 2 Lines into new files named\n                xaa, xab, xac, etc.\n\n\nnl: \n  number lines, \n  number lines by pages(number from 1 on second page)\n  number only regex matched lines (-b regex)\n  add header(\\:\\:\\:) and footer(\\:) between body(\\:\\:)\n\n\nfold:\n  split lines in two if length is exceeded\n   Params:\n  -w n - n length of w until move to another line\n   Example:\n  echo \"long line of text and stuff\" | fold -w 6\n\n\n\n\nlong line \nof text an\nd stuff\n\n\n\n\nfmt:\n  format text, like 'fold' but saves emtpy lines, spaces and tabs\n   Params:\n  -w n - how long line should be (same to fold) , but screws up spaces and tabs\n  -c - unscrews up spaces and tabs\n   Example:\n  fmt -cw 50 text.txt\n   will format file with lines of 50 long, saving empty lines and tabs\n\n\nprintf:\n  format string, like String.format(). has no new line \\n, gotta add it manually\n  % - conversion specificator:\n   s - string\n     Example:\n    printf \"variable: %s; and another %s\\n\" var ano\n    >variable: var; and another ano\n   d - digit negative or positive (1 or -1)\n   f - float, with comma - (1,1 > 1,100000)\n   o - octal number (1-8, like 8 bites in 1 byte, or chmod 777)\n    Example:\n     \"octal 1 is %0 octal 9 is %0)\" 1 9 > 1 .... 11\n   x - hex number (1-9a-f < lowercase)\n   % - escapes % (printf \"%%\")\n   %50s - formated string max 50 chars wide\n\n\nwc: \n  word count, returns number of lines, chars\\bytes and words\n   -l - count lines\n\n\nhead:\n  returs top of the file - 10 lines by default\n   Params:\n  -n \n - change number of returned lines\n\n\ntail:\n  returns bottom of the file - 10 lines by default\n   Params:\n  -n \n - number of returned lines\n  -n +\n - output starting from line, could cut off header lines\n  -f  - follow, will update file if any\n\n\nExample:\n   ls -l | tail -n +2\n    will return only listing fo files, w/o header lines:\n   $ ls -l\n\n\n\n\ntotal 1140\n-rw-rw-r-- 1 dos dos 0 feb 3 01:02 1.txt\n   ...\n    And:\n   $ ls -l | tail -n +2\n-rw-rw-r-- 1 dos dos 0 feb 3 01:02 1.txt\n    WHich is usefull for automation\n    Same goes with 'ps'\n\n\n\n\nTroubleshooting:\n Debian:\n  /var/log/syslog\n  /var/log/auth.log\n RHEL:\n  /var/log/messages\n  /var/log/secure\n /var/log/syslog too large\n some single event spammed and keep spamming syslog, and it grows in size\n DO NOT DELETE /var/log/syslog\n Find the cause:\n   watch tail -f /var/log/syslog\n    or\n   tail -100 /var/log/syslog\n instead clean the file:\n   sudo cat /dev/null > /var/log/syslog\n\n\nWifi adapter search for errors:\n  lshw -C network\n  uname -rv\n  lspci -knn | grep -EiA2 net\n  dmesg|grep -Ei 'wlan|firmw|dhc'\n  sudo cat /var/log/syslog | grep -Ei 'net|wpa|dhc'\n  sudo rfkill list\n  https://www.linuxquestions.org/questions/linux-newbie-8/asus-pce-ac51-wlan-adapter-driver-doesn%27t-install-properly-4175619526/\n\n\nNetwork troubleshooting:\n  nmcli device      # display all devices \n   or: nmcli device show eth0 - will show info about the interface(eth0) including dns, gateway, address, mac etc.\n\n\n\n\nDEVICE  TYPE      STATE      CONNECTION\n\nwlo1    wifi      connected  homenetwork \nlo      loopback  unmanaged  --   \n\n    # connected in wifi type device means it is working and connected to \n    # wireless router\n  lshw -C network   # display all info related about network devices\n   Output:\n    *-network             \n\n       description: Wireless interface\n       product: Intel Corporation\n       vendor: Intel Corporation\n       physical id: c\n       bus info: pci@0000:00:0c.0\n       logical name: wlo1\n       version: 03\n       serial: a0:a4:c5:3f:67:0e\n       width: 64 bits\n       clock: 33MHz\n       capabilities: pm msi pciexpress msix bus_master cap_list ethernet physical wireless\n       configuration: broadcast=yes driver=iwlwifi driverversion=4.15.0-46-generic firmware=34.0.0 ip=192.168.0.101 latency=0 link=yes multicast=yes wireless=IEEE 802.11\n       resources: irq:44 memory:a1114000-a1117fff\n\n\n\n\nWhere: \n  description - if device is set up incorrect will have 'Intel Corporation' or\n   something similar to manufacturer, otherwice 'Wireless interface'\n  configuration - the configuration of device, also has info about driver for \n   the device, ip address\n\n\n!NOTE:\n  If there are no info about drivers this could mean that kernel does not \n  have proper driver for the device, need to check what exact manufacturer and \n  network controller it is and then look for drivers on the manufacturer \n  website\n    ALso make sure that kernel version is newer than network adapter, because\n  then kernel could not yet has drivers for the device. As option make sure \n  that using newest kernel available\n\n\nlspci          # lists all pci devices on the machine\n  make sure something related to Network is there:\n   00:0c.0 Network controller: Intel Corporation Device 31dc (rev 03)\n  the part with 'Intel Corporation Device 31dc' is the device that need to \n  has correct drivers. \n\n\nNetworkManager:\n  network management daemon\n\n\nnmcli:\n  network manager command line interface\n   Synopsys:\n  nmcli [PARAMS] OBJECT {COMMAND | help}\n   OBJECTs:\n  g[eneral]       NetworkManager's general status and operations\n  n[etworking]    overall networking control\n  r[adio]         NetworkManager radio switches\n  c[onnection]    NetworkManager's connections\n  d[evice]        devices managed by NetworkManager\n  a[gent]         NetworkManager secret agent or polkit agent\n  m[onitor]       monitor NetworkManager changes\n   Example:\n  nmcli device - will display all network devices\n\n\nifconfig:\n  displays current network interfaces\n  part of the 'net-tools' package\n   /etc/network/interfaces - for interfaces(5) changes\n  -a - will display MAC also\n\n\nlshw:\n  list hardware, displays all the hardware detected by kernel and its state\n  and properties\n   -short  - short list of all devices found and their Classes\n   -businfo  - Better: than above, more humanfirendly, same list output\n   -class \n - detailed info about class of devices (-short for classes)\n   -C \n - same as above\n    Example:\n   lshw -C network - will display all info about network adapters and their\n                    state, together with drivers and info related\n\n\nhdparm:\n  lists HDD info\n  -I - get long list of HDD info\n\n\nlspci:\n  list pci devices - network , audio, usb controllers\n   Example:\n  lspci -knn | grep -EiA2 net\n   search for network controllers, and this is drivers used by them:\n    Kernel driver in use: rtl8821ae\n    Kernel modules: rtl8821ae\n\n\nuname:\n  system and kernel information\n   -r   - kernel info\n   -v   - Distro info\n   Example:\n  uname -rv\n\n\nBoot and Startup:\n  Boot when loads:\n  * BIOS\\UEFI\n  * MBR\n  * GRUB\n  * kernel is loaded into initial ramdisk(initrd or initramfs)\n  * systemd started\n  Startup:\n  * syslogd or rsyslogd is not running yet\n  * all logs go into \nring buffer\n\n  * to read \nring buffer\n use \ndmesg\n\n\ndmesg:\n  https://www.howtogeek.com/449335/how-to-use-the-dmesg-command-on-linux/\n  reads logs from Kernel Ring buffer \n   kernel ring - place where logs stored during boot and OS loading\n    BIOS\\UEFI, MBR, GRUB\n   During system run also stores all events like new hardware etc.\n  Example:\n   dmesg | grep -Ei 'wlan|firmw|dhc'\n    check drivers loading for wifi adapter\n  sudo remove:\n   sudo sysctl -w kernel.dmesg_restrict=0\n    ANY USER will be able to use \ndmesg\n now\n  -L   - color output\n  --color=always  - always use color (pipe redirection mb?\n  -H   - human timestamps + \nless\n is used for output\n         seconds are labeled with date\n    Example:\n   dmesg -H\n    [Apr 5 00:41] [UFW BLOCK] IN=wlp2s0 OUT= MAC=01:00\n    [  +9.499881] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e\n    [  +2.499111] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e\n    [  +0.126267] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e\n    [  +1.818683] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e\n    [  +2.920082] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e\n  -T   - human readable timestamps:\n    Example:\n   dmesg -T\n    [Sun Apr  5 00:45:25 2020] [UFW BLOCK] IN=wlp2s0 OUT=\n    [Sun Apr  5 00:45:29 2020] [UFW BLOCK] IN=wlp2s0 OUT= \n    [Sun Apr  5 00:47:30 2020] [UFW BLOCK] IN=wlp2s0 OUT= \n  --follow  - like \ntail -f\n\n  -l \n   -  Log levels\n    emerg: System is unusable.\n    alert: Action must be taken immediately.\n    crit: Critical conditions.\n    err: Error conditions.\n    warn: Warning conditions.\n    notice: Normal but significant condition.\n    info: Informational.\n    debug: Debug-level messages.\n\n\nExample:\n\n\n\nsudo dmesg -l debug,notice\n\n\n-f \n  - Facility categories\n    kern: Kernel messages.\n    user: User-level messages.\n    mail: Mail system.\n    daemon: System daemons.\n    auth: Security/authorization messages.\n    syslog: Internal syslogd messages.\n    lpr: Line printer subsystem.\n    news: Network news subsystem.\n\n\nExample:\n\n\n\ndmesg -f syslog,daemon\n\n\n-x  - decode. show the facility and level as human-readable prefixes \n   to each line.\n\n\nsed:\n  text stream editor, using RegEx for search\\edit in stream(STDIN) line by line,\n  in a non-interactive way(all decisions made\\given in command params)\n  Reads from file or STDIN\n  Writes into STDOUT or to file if given 'w \n' param or redirected STDOUT\n  Does not change original file content (-i will edit it)\n  Does only 1 change per line by default, 'g' - global, will match more than \n  first occurance\n  Also can just write something(matched) from stream into a new file\n=   sed '' file.txt - will just print everything, same as 'cat'\n\n\nSee More: https://www.digitalocean.com/community/tutorials/the-basics-of-using-the-sed-stream-editor-to-manipulate-text-in-linux\nhttps://www.digitalocean.com/community/tutorials/intermediate-sed-manipulating-streams-of-text-in-a-linux-environment\n\n\nsed \n '\n/pattern/replacement/\n' \n\n  -n - supress default ouput\n  '/p' - print , works with 's'\n    sed -n 'p' file - will print every line of file\n    sed 'p' file - will print every line of file TWICE\n  '1p' - print first line (address range)\n    Example:\n    sed -n '1p' /path/to/file    - will show only first line\n  '1,5p' - print from first to fifth line (address range) \n  '1,+4p' - print 4 lines from first(inclusively) (address range)\n  '1~3d' - print every other line: first then skip until 3rd line and again\n    print 1st line(drop counter)\n  '/pattern/,/pattern/' - address range matched by patterns\n    '/^START$/,/^END$/d' - will delete everything between lines START and\n    END, and do that again for another 'START' (until EOF or 'END' line)\n  'd' - delete\n  -i - edits IN-place - edits given file\n   -i.bak - creates baskup of original Edited IN-place file\n  's/pattern/replace/' - Substitute given RegEx pattern with given word\n    's_pattern_replace_' - will work if need to use slash inside fields\n      or any other character as far as all 3 are consistent\n  's/pattern/(&)/' - & is match result, so it will replace matched text with\n    () around this same text\n  's/(matchGroup)/\\1/ - ( is escaped to work as matching group, \\1 is the\n    referenced matching group\n  '/w \n' - Write changed lines into file - param2 group\n  '/g' - Global, will not go next line until first match\n   '/2' - Instead of 'g' will match only SECOND appearance of pattern\n  '/i' - Ignore case\n  '/G' - inserts blank line after each line\n  '=' - inserts line with a number of line after each line\n  '/pattern/s/' - matches and substitutes only in the line matched by pattern\n    Pattern could be complex too i.e. '/^$/d' will delete all empty lines\n     where ^ - beginning of line immediately followed by $ - end of line\n  '/pattern/!' - matches everything EXCEPT pattern\n    '/^$/!d' - will delete everything except blank lines\n\n\n Example:\n\n\n\n=   sed 's/parttime/fulltime/gw promotions.txt' team - will Substitute \n    everything found by pattern 'parttime' to new value 'fulltime' more than\n    once a line and will Write lines\n    that were changed into 'promotions.txt' in file 'team'\n=   sed 'fulltime/w fulltime.txt' team - will write all matched by 'fulltime'\n    pattern lines from file 'team' into file 'fulltime.txt'\n=   sed -n '/pattern/p' annoying.txt - will print only line containing pattern\n=   sed .... ... > /dev/null - will supress STDOUT, actually redirecting it \n    into void\n=   sed '0,/parttime/s/parttime/promotion/' team - will replace First occurance\n    of 'parttime' to 'promotion' in 'team' file\n=   sed 's/<[^>]\n>//' team - will substitute matched by expression pattern with\n    nothing - // it is empty, so nothing. \n    Regex: <[^>]\n> - matches HTML tags\n=   sed -n 's/on/forward/2p' file - will print out only lines where replace \n    was done, and replacing only 2nd match in line\n=   sed 's/^.*at/(&)/' annoying.txt - puts () around matched text\n=   sed 's/and/\\&/;s/people/horses' annoying.txt - substitues and to escaped &\n    then substitutes people to horses in annoying.txt\n=   sed '/^$/d' file - will delete empty lines(beginning of line followed by end\n    of line) from a 'file'\n\n\ntee:\n  reads from STDIN and writes to STDOUT and File\n  Overrides existing file by default , to append use -a\n   Good for saving in the middle of pipe\n  tee \n file1 file2\n  -a - append file\n    Example:\n  ls dir1 | tee file1 \n       will display ls on dir1 and write output to file1\n  ls dir2 | tee -a file1 | less \n       display dir2 and append ls output to file1\n       then moe along a pipe and 'less' with the same result as in file1\n\n\nnl:\n  number lines in text file. only numbering not empty lines\n\n\nmkdir:\n  creates dir\n  mkdir \n \n\n  -p - creates every unexisting dir along the path, return no errors\n   mkdir -p /not_exist1/not_exist2 - will create both not existing dirs \n  mkdir \n \n \n - will create 3 separate directories\n\n\nwildcards(not a command:\n * - any number of symbols\n ? - any symbol\n [ab1,] - one symbold from a list (same to regex)\n [!ab1,] - none of listed symbols([^] in regex)\n [[:class:]] - one symbol from a class\n  :alnum: - digits and letters (\\w in regex)\n  :alpha: - letters (similar to  Unicode character property class \\p{L} in regex)\n  :digit: - digits (\\d in regex)\n  :lower: - lowercase\n  :upper: - uppercase\n\n\ncp:\n  copy stuff\n  cp \n \n \n\n  cp -rf ../source/\n .\n   -r - recursive copy\n   -f - overwrite everything \n   -i - will ask for overwrite\n   -a - archive - keep original access rights and owners\n   -u - if overwrite - will copy only newer files\n   -v - verbose\n   . - current directory with saving all the names and paths from source\n  NOTE: to copy hidden files, those star with . , need to escape dot(.) as it is\n  regular expression:\n  cp /etc/skel/.\n . - will copy all hidden files to current directory\n  ............/.\n . - will not work\n  ............/.\n.* . - will copy '..' which is parent directory to current one\n\n\nmv: \n  see cp: above , pretty same, except it moves\\renames files\n\n\nrm:\n  remove stuff, does not delete dirs and not empty dirs w/o additinal params\n  rm \n \n\n  rm -r \n\n   -r - recursive, also deletes directory at the end\n   -d - deletes EMPTY directory\n   -f - force\n   -i - interactive - will ask before do\n   -v - verbose\n  To rm link:\n   rm link_name   -- just as regular file, source of link is not affected\n\n\nunlink:\n  delete symlink\n   Example:\n  unlink symlink_name\n\n\nFind broken link:\n   find /path/to/dir -xtype l\n  Delete:\n   find /path/to/directory -xtype l -delete\n\n\nln:\n  create hard link by default\n  ln \n \n \n\n  -s - create soft link\n   Example:\n  ln -s /src/file /dest/symlink\n    first param is where the file is \n    second param where to put symlink\n   Example, symlink from dir to dir:\n  ln -s /full/path/to/dir /full/path/to/symlink\n  ln -s ~/ansible/buoy ~/opt/tideworks\n   this will create 'tideworks -> /home/dos/ansible/buoy/'\n   where 'tideworks' is a link which points to another directory ansible/buoy\n  https://www.shellhacks.com/symlink-create-symbolic-link-linux/\n\n\nHard link: every file has 1 , create another one in different place\n   it will has size of its name, and will point to place on HDD for original file\n   File can not be deleted until all Hard links deleted.\n   Note: ls -i will return memory block of a place where link points\n    so file and is hard links will all point in the same place, thus having \n    same number next to it.\n   ONly can be created in the same filesystem (disk drive)\n\n\nSoft link: upgraded hard link, could be created anywhere, orig file can be\n   deleted > link will become broken\n\n\ntype: \n  returns how given command is interpreted\n   Example:\n  dos:~$ type ls\n\n\n\n\nls is aliased to `ls --color=auto'\n   Example:\n  dos:~$ type cd\ncd is a shell builtin\n   Example:\n  dos:~$ type apt\napt is /usr/bin/apt\n\n\n\n\ncurl: vs wget:\n  If I wanted to download content from a website and have the tree-structure \n  of the website searched recursively for that content, I\u2019d use wget.\n\n\nIf I wanted to interact with a remote server or API, and possibly download \n  some files or web pages, I\u2019d use curl. Especially if the protocol was one \n  of the many not supported by wget.\n\n\nwget:\n  non interactive network downloader\n   Params:\n  -q - quiet, turns off wget output\n  -i file / --input-file=file - reads URLs from a file\n   Example:\n  wget -q https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb - will download .deb file in current directory w/o any output\n\n\ncurl:\n  web requests issuer - HTTP[S] requests( see --version protocol list).\n   CAREFUL: binary download to stdout in terminal could exploit injections\n   https://www.howtogeek.com/447033/how-to-use-curl-to-download-files-from-the-linux-command-line/\n  Or could download web pages and files and stuff\n   -o - output, redirects output to a file, usefull when downloading files\n    sudo curl -o /usr/local/bin/ecs-cli https://s3.amazonaws.com/amazon-ecs-cli/ecs-cli-linux-amd64-latest\n     will download ecs-cli binary file from amazon storage into /usr/... path\n    > - will also workd:\n      Example:\n     curl https://www.bbc.com  > bbc.html\n     When redirected, \ncurl\n detects it and display download bar, columns:\n     % Total: The total amount to be retrieved.\n     % Received: The percentage and actual values of the data retrieved so far.\n     % Xferd: The percent and actual sent, if data is being uploaded.\n     Average Speed Dload: The average download speed.\n     Average Speed Upload: The average upload speed.\n     Time Total: The estimated total duration of the transfer.\n     Time Spent: The elapsed time so far for this transfer.\n     Time Left: The estimated time left for the transfer to complete\n     Current Speed: The current transfer speed for this transfer.\n\n\n-#  - display progress bar instead of full statistics\n\n\n-C    - continue from offset\n      -  - continue from last downloaded offset: simply continue download\n     Example:\n     curl -C - --output file.name http://example.com/some_partially_dwnld.file\n      Note line:\n    \n** Resuming transfer from byte position 168435712\n\n\n-I - Headers!\n     Example:\n    curl -I https://www.bbc.com\n\n\ndownload list of files:\n* save list urls into file; new line delimited\n* use xargs to read a file; each line as an argument\n* pass to curl:\n xargs -n 1 curl -O < urls-to-download.txt\n  -n 1  - treat each line in file as a separate parameter\n\n\n\n-O - save file with same name from remote server\n\n\n-s - silent, no progress or error output. BUT still displays url contents\n    echo \"$(curl -s https://s3.amazonaws.com/amazon-ecs-cli/ecs-cli-linux-amd64-latest.md5) /usr/local/bin/ecs-cli\" | md5sum -c -\n     will curl display md5 from amazon storage, w/o any other output, then\n      will display path to local ecs-cli, which will go into pipe to md5 sum\n      which will compare given md5 and md5 from file under given path, and will\n      return OK if everything if fine\n  Example: Generate crumb(token) and use it as a header to POST request\n to the server restfull call\n    crumb=$(curl -u \"builder:1234\" -s 'http://jenkins.local:8080/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)')\n    curl -u \"builder:1234\" -H \"$crumb\" -X POST http://jenkins.local:8080/job/ENV/build?delay=0sec\n  -H - header of a request(contains Token)\n  -u - \"user:password\"; Works with FTP too\n  -X - Request type (GET or POST, PUT, DELETE etc) \n   default: GET\n\n\ndu:\n  disk usage\n  du \n \n\n   Does not accept input from pipe, seems like\n    use 'xargs'\n  du -hs dirname\n   -h - human readable\n    Example:\n       ls -d \n/ | xargs du -hs  - will display size of all subdirs of of \n                current dir\n    Or:\n       ls -d \n/ | xargs du -hs | sort -hr | head -20 - top 20 biggest dirs\n    Or:\n       du -sh \n/ | sort -hr  - du also accepts wildcards, so no need\n                to use ls for supplying list to du\n   -s - summary for every directory w/o listings its contents\n   hidden directories:\n  du -hs .[^.]\n\n\ndiff:\n  checks differences two files or whole directories\n   could recursevly dig into dirs for diffs, and generate patch files with\n  differences, to be applied by 'patch' command\n   Params:\n  -c - context diff format (easier to read than standard POSIX diff)\n  -u - unified diff format (less text, even more easier)\n   Example:\n  diff packages.list packages.list2\n    packages.list:\n     pkg1\n     pkg2\n    packages.list2:\n     pkg2\n     pkg3\n   POSIX compliant output:\n    1d0\n    < pkg1\n    2a2\n    > pkg3\n    read output:\n   < - only first file has pkg1, it was on the left in the params list\n\n\n\n\n\n\nonly second file has it, and it was on the right\n\n\n\n\n\n\n-c output:\n   \n pkg1.lst 2019-02-02 02:22:25.312819822 +0200\n   --- pkg2.lst 2019-02-02 02:22:36.984692484 +0200\n   \n*\n*\n\n   \n 1,2 \n***\n   - pkg1\n     pkg2\n   --- 1,2 ----\n     pkg2\n   + pkg3\n\n\n-u output:\n   --- pkg1.lst 2019-02-02 02:22:25.312819822 +0200\n   +++ pkg2.lst 2019-02-02 02:22:36.984692484 +0200\n   @@ -1,2 +1,2 @@\n   -pkg1\n    pkg2\n   +pkg3\n\n\nSymbols:\n   (none) - line match\n   -      - line removed from first file\n   +      - line added into second file\n   !      - line changed (only -c mode)\n\n\npatch:\n  'diff' generated patch apply\n   generated patch file already has file names, so no need to say anything\n  just apply:\n   patch < patchfile.txt\n  Just kidding - use VCS\n\n\nbash history:\n  ! - invokes the Bash history mechanism\n    !echo - will display and execute latest echo from current shell history\n        if such exists, displays error if nothing found(event not found)\n   inverts exit code of commands if with space:\n    ! true; echo $? # 1 - means exit code 0 was changed to 1\n   also inverts pipe exit code\n    ls | bogus_command; echo $? # exit code: 127\n    ! ls | bogus_command; echo $? # exit code: 0\n\n\nunzip:\n  utility to unzip files. Unzips in the current folder by default\n   Params:\n  -d - specify folder where to extract\n  -l - lists archive contents, 'less' tool does the same\n\n\nzip: \n  creates zip archive, could also create encrypted archives(with password)\n  --encrypt - will promt for password during arch creation:\n   Example:\n  zip --encrypt archive.zip files\n   Less secure example:\n  zip --password (password) file.zip files\n\n\nNOTE: zip encyption is very weak, use additional or other encription like gpg\n\n\nEncryption \npublic private keys ssl , stuff\nhttps://www.devco.net/archives/2006/02/13/public_-_private_key_encryption_using_openssl.php\nSign with private key\n Example:\nhttps://raymii.org/s/tutorials/Sign_and_verify_text_files_to_public_keys_via_the_OpenSSL_Command_Line.html\n\n\nCreate CRS - certificate signing request (csr)\nhttps://www.sslshopper.com/what-is-a-csr-certificate-signing-request.html\nopenssl req -new -newkey rsa:2048 -nodes -out servername.csr -keyout servername.key \n\n\nGeneral info about Keys, Key Pairs, Key Storages and stuff:\nhttps://info.townsendsecurity.com/definitive-guide-to-encryption-key-management-fundamentals#How-Encryption-Key-Systems-Work\n\n\ngnupg: gpg:\n  Gnu PG could encrypt documents, basically all files\n   See docs for encription here:\n  https://www.gnupg.org/gph/en/manual/x110.html\n\n\nbasic example w/o using Public-key cryptography:\n   gpg --output \n --symmetryc \n - this will \n    ask for passphrase and its done\n  --gen-key  - generate a key\n  --list-keys  - list all keys\n\n\nrsync:\n  Remote Syncs files\\dirs locally or remote-local\\local-remote\n   Synopsis:\n  rsync [-params] /source/path [/another/source] /receiver/path\n   Source-Receiver could be:\n  - local file or dir\n  - remote file or dir [user@]hostname:/path/to/file\n  - remote server rsync, URI - rsync://[user@]hostname[:port]/path/to/file\n   Source or Receiver MUST be local, BOTH Remotes NOT WORK\n    Params:\n  -a - archive, recursive and saves files attributes\n  -v - verbose\n  --delete - remove files that are in Receiver but not in SOurce\n  --rsh=\n - remote shell, i.e. ssh - will transfer files via network\n    in secure way (secure ssh tunnel)\n   Example:\n  rsync -av --delete --rsh=ssh /etc /home /usr/local remotesys:/backup\n    Will archive(recurse and save file attrs) with verbose output , deleting\n    files absent in source dirs and present in remote:/backup, copy different \n    from source dirs files via ssh tunnel to remotesys network location under\n    path /backup\n   Example:\n  rsync -aAHvh bitbucket-home.tar.gz root@o-pe-bitbucket.twlab.int:/opt/atlassian/bitbucket/\n\n\nar:\n  archive?\n   seems like can unarchive deb packages(other may be too?)\n  Example:\n  https://dev.to/jake/using-libcurl3-and-libcurl4-on-ubuntu-1804-bionic-184g\n   Download libcurl3 package, extract into tar, untar it, copy following\n   symlink in source(inside untarred dir) into usr/lib:\n  $ mkdir ~/libcurl3 && cd ~/libcurl3\n  $ apt-get download -o=dir::cache=~/libcurl3 libcurl3\n  $ ar x libcurl3* data.tar.xz\n  $ tar xf data.tar.xz\n  $ cp -L ~/libcurl3/usr/lib/x86_64-linux-gnu/libcurl.so.4 /usr/lib/libcurl.so.3\n  $ cd && rm -rf ~/libcurl3\n\n\ntar:\n  tape archive - collects all files in archive, with savin all permissions\n   and user's ownership\n  tar \n \n \n \n\n  tar -cvf name.tar directory\n   -c - create\n   -v - verify\\verbose\n   -f - files\n   -t - view archive , or something like that\n   -r - append regular file to the end of an archive i.e:\n     tar -rvf uncompressed.tar mybkup/mytest.txt\n   -z - zip. tells tar that working with zips and not archives; \n        used for zipping, unzipping, viewing zip,\n          if 'czvf' - creates .gz archive, compress same as 'gzip'\n          if 'tzvf' - lists zip contents\n          if 'zxvf' - extracts zip contents\n     Example archive+compress:\n    tar -zcvf tmp.tar.gz tmp\n     Example unarchive+uncompress:\n    tar -zxvf tmp.tar.gz\n   -j - zup but with bzip2 instead of gzip\n   -p - preserve permissions\n   another params:\n    !! could be anywhere in the command, before or after other params\n    --exclude=filename - excludes filename/dir/type from adding to archive\n      i.e. tar -czvf arch.tar.gz --exclude=file.txt source_dir/\n      !! use relative path of tar archive itself! not system file path !!\n      !! use path like 'folder/folder/file' w/ or w/o quotas, optional\n      !! DO NOT use path like './folder/folder/file' - this will fail silently\n  params could be used w/o dash, like 'tar tvf archive.tar'\n  Unzip:\n    tar zxvf \n \n\n   -z - unzips contents\n   -x - eXtract it\n   -v - verbose\n   -f - files\n   \n - cold be empty for current location\n  Could pass view output to grep to search for particular file\n\n\ngzip:\n  compress files. it looks for a file it can compress by default even w/o\n   specifying its name\n  gzip \n\n  gzil archive.tar\n   will substitute 'archive.tar' by 'archive.tar.gz'\n  Could be called by 'tar' command using 'z' key, will works the same\n   Params:\n  -c - compressed output to STDOUT(console), original file is left untouched\n  -d - decompress file, same as 'gunzip'\n  -f - force compresses even if .gz with same name already exists\n  -h - help\n  -l - list files in archive, with compress ratio and orig sizes\n  -t - test compressed file for integrity\n  -v - verbose\n  -1 - set compress ratio 1 to 9, where ()\n    1 - fast and little uncompressed and (also --fast)\n    9 - slow and super compressed (also --best)\n    6 - default value\n\n\ngunzip:\n g unzip, unzips .gz files\n  -c - cat, same as in gzip - writes to STDOUT leaves original files unchanged\n\n\nzcat: \n shipped with gzip, is like 'cat' but with z\n  could be used instead of:\n gunzip -c file.gz | less\n\n\nzless: \n shipped with gzip, is like 'less' but with z\n  could be used instead of:\n zcat file.gz | less\n\n\nzgrep:\n grep for search in .gz compressed files\n\n\nlsblk:\n  List Block Devices\n  list all devices mounted and not\n   Example:\n  lsblk -po NAME,FSTYPE,SIZE,MOUNTPOINT /dev/sda /dev/sdb\n    show only 'sda' and 'sdb'\n   Params:\n  -f--fs - list fylesystems\n  -d--nodeps  - show only top device(w/o partitions)\n   Example:\n  lsblk -d /dev/sda\n  -o Col1,Col2  - Only columns Col1 and Col2\n  -O   - all columns\n  -p   - full path of Block device\n  -e--exclude - exclude block device number\n   Example:\n  lsblk -e7,11 -po NAME,FSTYPE,SIZE,MOUNTPOINT\n    excludes Loop(7) and 11th block devices (see /proc/devices)\n   Columns:\n  MAJ:MIN - block device number : number\n   /proc/devices - list of ALL block device numbers\n\n\nblock device types:\n /dev/fd\n - floppy drives\n /dev/hd\n - hard drives\n /dev/lp\n - printers\n /dev/sd\n - SCSI, including PATA/SATA in modern kernels, flash and usb, digital\n        cameras, players and stuff. \n /dev/sr* - cd/dvd\n\n\nblock device naming:\n old motherboards had 2 IDE channgels, with cable for 2 devices (master\\slave)\n devices are named by alphabet\n partitions are named by numbers\n first channel master - a - sda\n  first partition on sda - 1 - sda1\n  second partition on sda - 2 - sda2\n first channel slave - b - sdb\n second channel master - c - sdc\n second channel slave - d - sdd\n\n\ntune2fs:\n  adjust filesystem params\n   Params:\n  - l \n - list file system info\n   Example:\n  tune2fs -l $(df / | grep '/' | awk '{print $1}') | grep 'created'\n   will return Filesystem created date, means when system was installed\n  because looking at name of '/' root filesystem\n\n\ndf:\n  display all mounted devices with size\\free\\used\\paths etc\n    Stands for: Disk Free\n  -h - human readable sizes\n    Example:\n   df -h / - will show info about root partition - where / is mount point\n  -T  - adds colump with FS Type\n  -x  - exclude given FS Type\n    Example:\n    df -x tmpfs -x squashfs -h\n     not show tmps and /dev/loop* (snap) filesystems\n\n\nmount find usb disk hdd drive:\n\n\nformat usb disk:\nhttps://www.wikihow.com/Format-a-USB-Flash-Drive-in-Ubuntu\nhttps://www.cyberciti.biz/faq/linux-disk-format/\n\n\n\n\n\n\nlocate the disk and found its name:\n\n  tail -f /var/log/syslog  \n\n\n\n\nlook for something like this:\n\nMay 26 15:57:00 dospc kernel: [ 9385.461071]  sdc: sdc1 sdc2\n\n\n\n\n\n\n\n\nlsblk\n\n\n\n\n\n\n\n\nunmount everything that auto-mounted from that drive\n\n\n\n\nsudo umount /dev/sdb1\n\n\n\n\n(OPTIONALLY)  override everything , will take HOURS:\n\n\n\n\nsudo dd if=/dev/zero of=/dev/sdc bs=4k status=progress && sync\n\n\n3.1 Faster option, which will override MBR and partition table:\n\n\ndd if=/dev/urandom of=/dev/sdc bs=1M count=2\n\n\n\n\nfdisk /dev/sdc\n\n\n\n\noption: o\n\n\nwill create empty partition table\n\n\noption: n\n\n\nwill create new partition, then use Defaults\n\n\noption: w\n\n\nwill write changes, could take some time. wait for it\n\n\n\n\nlsblk\n\n\n\n\nwill return all block devices with updated sdc device, check size\n\n\n\n\nformat into FAT32\n\n\n\n\nsudo mkfs.vfat /dev/sdc1\n\n\n\n\neject disk when done \n\n\n\n\nsudo eject /dev/sdc\n\n\n\n\nhow to get block device name, partition, mount it and unmount after:\n  tail -f /var/log/messages     - open messages and follow. deprecated\n  tail -f /var/log/syslog - this is more standard log file.\n  #mount disk, recheck messages for something like:\n  \"sdb: sdb1\"\n  \"sd 3:0:0:0 [sdb] attached SCI removable disk\"\n  fdisk /dev/sdb\n   p                - see all partitions\n  mkdir ~/mounted_stuff\n  mount /dev/sdb1 ~/mounted_stuff\n  umount /dev/sdb\n\n\nmount:\n  mount drive (disk, usb, floppy etc)\n  mount -o rw,remount /\n   -o - dunno.. TODO\n   rw - seems to be read\\write\n   remount - pretty clear at first glance\n   / - what to remount\n\n\nExample:\n\n\n\nlsblk -f     - view all devices\n   mount /dev/sda5 /path/to/mount/point - mount device\n   umount /dev/sda5   - unmount (Notice N letter absent in commnad name)\n    Example:\n    mount ISO image\n   mount -t iso9660 -o loop image.iso /path/to/mountpoint\n\n\n-a - mount all lines added to /etc/fstab\n\n\n/etc/fstab - config for filesystems\n   read by fsck, mount, unmount\n  lines from this file read during 'mount -a'.\n   check man fstab for details. \n  simply add automount of HDD from old windows:\n  LABEL=Juli      /otherHDD/Juli  ntfs rw,suid,exec,auto,user,async\n  labels or UUID could be taken from 'blkid' command\n\n\nntfs-config:\n  Enable/disable write support for any NTFS devices\n\n\nfdisk:\n  manipulates disk partitions tables\n   Usage:\n  fdisk \n\n   Commands:\n  m - manual/help\n  p - show all partitions\n  l - list all known filesystems (use one in mkfs )\n   Example:\n  fdisk /dev/sda\n\n\n\n\nm   - see what could do\np   - see all partitions of the device \n\n\n\n\nmkfs:\n  create new filesystem\n   Params:\n  -t fylesystem type\n   Example:\n  mkfs -t ext3 /dev/sdb1\n\n\nlvm:\n Logical Volume Manager\n  How to install:\n https://help.ubuntu.com/community/UbuntuDesktopLVM\n  Ubuntu docs:\n https://wiki.ubuntu.com/Lvm\n  How to extend Logical volume:\n https://www.youtube.com/watch?v=hugEkh50Ynk\n  How to extend logical volume in text \n https://www.howtogeek.com/howto/40702/how-to-manage-and-use-lvm-logical-volume-management-in-ubuntu/\n\n\nfsck:\n  filesystem check and repair (lost+found dir will have repaired files)\n   Example:\n  fsck /dev/sdb1\n\n\ngenisoimage:\n  creates ISO images\n   Parameters:\n  -o \n - output to\n  -R - for long file names and access rights for files in POSIX style\n  -J - similar long names but for Windows\n   Example:\n  genisoimage -o cd-rom.iso -R -J ~/cd-rom-files\n   where cd-rom-files is a directory with files to be added into ISO image\n\n\nscp:\n  secure copy using ssh protocol\n   could copy to or copy from remote location.\n  copy to remote:\n  scp \n \n \n@\n:\n\n  copy from remote:\n  scp \n \n@\n:\n \n\n  params:\n  -v - verbose  \n\n\nsshd(openssh)\n  daemon config location:\n   /etc/ssh/sshd_config\n  sftp enable switch is located there, could be commented out and sshd restart\n  to disable it\n   /etc/ssh/ssh_config\n  some another config\n  ==\n   Options important:\n  PermitRootLogin:\n    \u201cyes\u201d, \u201cprohibit-password\u201d, \u201cwithout-password\u201d, \u201cforced-commands-only\u201d, or \u201cno\u201d\n  ==\n  RSA keys for ssh authentication generation process:\n  https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-on-centos7\n  Passwordless:\n  to add security ssh login with Password could be disabled, to login\n  only with keypair:\n  https://linuxize.com/post/how-to-setup-passwordless-ssh-login/#disabling-ssh-password-authentication\n  https://help.ubuntu.com/community/SSH/OpenSSH/Keys\n   Log files of sshd:\n  RHEL:\n   /var/log/secure\n    Example:\n   tail -f -n 500 /var/log/secure | grep 'sshd'\n    If getting errors like:\n   'ROOT LOGIN REFUSED FROM 10.253.0.28 port 34318'\n    the Root login is disabled\n   PermitRootLogin yes\n  Other:\n    /var/log/auth.log\n   tail -f -n 500 /var/log/auth.log | grep 'sshd'\n\n\nssh:\n  remote connect via ssh protocol\n\n\n-v - display debug info level1\n  -vv  - debug level2\n  -vvv - degut level3\n  Troubleshooting:\n   if -vvv  shows everything ok:\n     debug1: Server accepts key: pkalg ssh-rsa blen 535\n     debug2: input_userauth_pk_ok: SHA1 fp 28:bc:1b:64:94:3a:6b:ec:b4:13:45:ca:79:80:be:7c:75:cc:82:48\n     debug3: sign_and_send_pubkey: RSA 28:bc:1b:64:94:3a:6b:ec:b4:13:45:ca:79:80:be:7c:75:cc:82:48\n     debug1: read PEM private key done: type RSA\n     debug3: Wrote 1156 bytes for a total of 3101\n   Open Log files of sshd:\n    tail -f -n 500 /var/log/secure | grep 'sshd'\n   if errors like this appear:\n    ROOT LOGIN REFUSED FROM 10.253.0.28 port 34318\n   open  /etc/ssh/ssh_config and set\n  PermitRootLogin yes\n\n\nssh-keygen:\n  generates keypair private\\public\n    Example:\n   ssh-keygen -t rsa -b 4096 -C \"email\" -f my_key\n    Example PUBLIC from Private:\n   ssh-keygen -y -f private_key > private_key.pub\n  -t rsa - key type\n  -b 4096 - generates key pair into given path\n  -C \"text\" - comment for key\n  -f filename of the key, instead of default 'id_rsa'\n   Example:\n  ssh-keygen -t rsa -b 4096 -C \"email\" -f my_key\n      - will create new keypair, w/o -f it will generate\n    in the path ~/.ssh/id_rsa and its id_rsa.pub part\n  -R - remove from ~/.ssh/known_hosts\n    Example:\n   There are only 1 RSA key is allowed for the IP, in case RSA key is changed, then \n   old one need to be removed, otherwise it will not allow ssh acces:\n    ssh-keygen -f \"/home/dos/.ssh/known_hosts\" -R 172.17.0.2\n\n\n-l - list, not create anything.\n  -E - Specifies the hash algorithm used when displaying key fingerprints.\n\n       Valid options are: \u201cmd5\u201d and \u201csha256\u201d.\n    Example:\n Get fingerprint:\n  ssh-keygen -lf ~/.ssh/keyname[.pub]\n GitHub: Get fingerprint in hashed\\hex format:\n  ssh-keygen -E md5 -lf ~/.ssh/git_key[.pub]\n\n\nssh-copy-id:\n  copies public keys securely to remote machine for paswordless login\n  keys are concatenated into ~/.ssh/authorized_keys file\n  it will copy all public keys available in ~/.ssh/\n   Example:\n  ssh-copy-id dmitry2@192.168.42.74\n   if there issues with a default 22 port , another port could be changed:\n  ssh-copy-id \"\n@\n -p \n\"\n\n\nif ssh-copy-id is not available here is analoug:\n   cat ~/.ssh/id_rsa.pub | ssh remote_username@server_ip_address \"mkdir -p ~/.ssh && chmod 700 ~/.ssh && cat >> ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys\"\n\n\nssh-keygen:\n  Known hosts are stored in a file /home/\n/.ssh/knonwn_hosts.\n    Generate public key from private key:\n  ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub\n\n\nsftp:\n  secure file transfer protocol, allows to list dir contents\n  built-in into OpenSSH server, which workks as sshd service(daemon) together\n  with scp\n  sftp \n@\n\n sftp has its own set of directory related commands:\n pwd - print work dir, returns current remote_dir\n lpwd - current local_dir\n cd - change dir, changes remote_dir\n lcd - local change dir, changes local_dir from sftp via OpenSSH daemon\n get - copy file to current local_dir\n  get \n \n\n ls also works, ALIASES from bashrc do not work\n\n\nshutdown:\n shutdown or reboot or halt etc the computer\n  by default shutdowns in 1 minute, also could be scheduled:\n   Example:\n  shutdown \"21:45\" - will shutdown at 21:45\n  shutdown \"+25\" - will shutdown in 25 minutes from 'now'\n  shutdown - will shutdown as 'shutdown +1'\n  shutdown now - shutdown now\n\n\nat:\n need to be installed, daemon\n schedules to do something \nat\n some time:\n There are three ways to use it:\n  Example 1:\n echo \"shutdown\" | at now + 1 min - will execute what echo will return\n    at time of now + 1 minute - echo will return stuff in ''\n  Example 2:\n echo \"shutdown\" > cmd.txt\n at now + 1 min < cmd.txt - it will redirect STDIN from cmd.txt , will read\n    from there, and shutdown \n  Example 3: \n at now + 1 min\n at> shutdiwn     - will wait for input in STDIN\n ctrl+d           - to exit, and wait given time before execute shutdown\n\n\nTime could be like:\n [[CC]YY]MMDDhhmm[.ss]\n  Example: \n at -t 201403142134.12 < script.sh\n\n\ncrontab:\n  list cron schedules\n Exacmple:\n  crontab -u \n -l\n    will list all rules for a user\n    need sudo\n\n\ncron:\n task scheduler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncommand to be executed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-\n |     |     |     |     |\n |     |     |     |     +----- day of week (0 - 6) (Sunday=0)\n |     |     |     +------- month (1 - 12)\n |     |     +--------- day of month (1 - 31)\n |     +----------- hour (0 - 23)\n +------------- min (0 - 59)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, for example, this will run ls every day at 14:04:\n\n\n04 14 * * * ls\n\n\nTo set up a cronjob for a specific date:\n\n\n Create a new crontab by running crontab -e. This will bring up a window of \nyour favorite text editor.\n\n Add this line to the file that just opened. This particular example will \nrun at 14:34 on the 15th of March 2014 if that day is a Friday (so, OK,\nit might run more than once):\n\n 34 14 15 5  /path/to/command\n\n Save the file and exit the editor.\n\n\n\nawk:\n stuff to worh with text, programming language kind of\n  Example: \n awk 'BEGIN {printf \"title\"} {print \"line\"} END {printf \"end\"}' textfile.txt\n  this will print 'title'\n  then 'line' for every line in textfile\n  and print 'end' at the end\n\n\ncheck out tutorial here:\n https://www.tutorialspoint.com/awk/awk_basic_syntax.htm\n\n\nworkflow is Read, Execute, Repeat with tear up and tear down executed once:\n BEGIN\n  Read line\n  Execute user code\n  Repeat if not EOF\n END\n\n\nBegin block executes only once, Tear Up, syntax:\n  BEGIN {awk-commands}\n Optional\n\n\nBody block, which is actual stuff to do for every line matched by pattern:\n  /pattern/ {awk-commands}\n Pattern is optional\n\n\nEnd block executes only once, Tear Down, syntax:\n  END {awk-commands}\n Optional\n\n\nAlso gets input from a file:\n  -f \n\n    Example:\n  command.awk:\n   BEGIN {printf \"start\\n\"}\n   {print}\n   END {printf \"end\\n\"}\n\n\nawk -f command.awk textfile.txt   - this will read from command.awk and print\n   'start' at 1st line\n   then print every line\n   then print 'end' and the latest line\n\n\n===general\n\n\n===Env Vars / Environment variables:\ndeclare:\n  built-in command to assign or display variables and functions\n   Syntax:\n  declare [-aAfFgilnrtux] [-p] [name[=value] ...]\n  Params:\n  Using \n+' instead of\n-' turns off the given attribute.\n   Example:\n  declare - will show all variables and functions\n  declare | grep \"^a=\" - will return variable 'a' with its value \n\n\n-p - prints only variables w/o functions (displayhs onlyh NAME)\n  -a - to make NAMEs indexed arrays(only Bash, not Sh)\n   Example:\n\n\n-i - to make NAMEs integer\n   Example:\n   declare -i z=asdf    < echo $z - 0\n   declare -i z=12  < echo $z - 12\n  -l - to convert NAMEs to lower case on assignment\n   Example:\n   declare -l b=TEST\n   echo $b      >test  - will save it in loweracase\n  -u - same but for UPPRECASE\n  -g - create global variables in functions:\n  When used in a function, \ndeclare' makes NAMEs local, as with the\nlocal'\n    command.  The `-g' option suppresses this behavior.\n\n\n-x - export variables\n   Example:\n   declare -x FOO  - same as - export FOO\n  -r - readonly variables\n  -n - make NAME a reference to the variable named by its value\n   Example: pointer-to-pointer\n   b=foo        # variable\n   declare -n b2=b  # pointer to the variable\n   echo $b2\n\n\n\n\nfoo\n   declare -t b3=b2 # pointer to pointer\n   echo $b3\nfoo\n\n\n\n\nprintenv:\nenv:\n  very similar, prints environment variables to STDOUT\n  printenv VAR - prints value of specified variable to STDOUT\n\n\nUseful Env variables:\n  EDITOR - default text editor\n  SHELL - name of the shell\n  HOME - home path\n  LANG - chars and sort order for language\n  OLD_PWD - prev work dir\n  PWD - current working dir\n  PAGER - program for page view ( i.e. /usr/bin/less)\n  PS1 - current prompt string value\n    Example:   dos:~$: / root:~#:\n    Prompt string could be drastically changed\n    even including redraw of clocks in the cmd on every command entered\n        Example: [\\033[s\\033[0;0H\\033[0;41m\\033[K\\033[1;33m\\t\\033[0m\\033[u]<>$\n    Example:\n   export PS1='$(whoami)@$(hostname):$(pwd) '$\n  TERM - type of the terminal\n  TZ - time zone, usually *nix has Coordinated Universal Time , and it\n    is corrected with current time zone upon display\n  USER - username\n  HISTCONTROL - with =ignoredups - will ignore duplicates in bash history\n  HISTSIZE - changes default 500 value of history line-length\n\n\nset:\n  prints ALL variables - Shell vars, local vars, shell functions\n  Builtin so use 'help set'\n\n\nRead more of set and printenv\\env here:\nhttps://www.digitalocean.com/community/tutorials/how-to-read-and-set-environmental-and-shell-variables-on-a-linux-vps\n Read more of printenv\\env history here:\nhttps://unix.stackexchange.com/questions/123473/what-is-the-difference-between-env-and-printenv\n\n\nHow to set\\export env variables in bash:\n  export - share variable for child processes started from current shell\n\n   Example:\n  TEST=1 - will create var for current shell, will not be visible in processes\n    started from the shell\n  export TEST=1 - will create var for cur shell, and share it to processes\n    started from the shell\n  echo 'export TEST=1' >> ~/.bashrc - will create var at every non-login start \n    of the shell, if .profile reads .bashrc - login shell also get it\n     Usually for vars there are different file, which bashrc will read\n    see/update local .bashrc to do so.\n     Just exported into bashrc vars will not be visible until shell restart\n    or source load:\n     source ~/.bashrc - will add newly exported vars into current session\n\n\nRead export\\env differences here:\n http://hackjutsu.com/2016/08/04/Difference%20between%20set,%20export%20and%20env%20in%20bash/\n\n\nSet variable ONLY for current shell:\n  varname=\"my value\"\n  Set variable for current shell and ALL PROCESSES started from current shell:\n  export varname=\"my value\"\n  For below LogOut is required:\n  Set var PERMANENTLY for current user\n  ~/.bashrc - add 'export varname=\"my val\"' here\n  Set var permanently and SYSTEM WIDE (all users/all processes)\n  /etc/environment - add line 'VARNAME=\"my value\"', caps bcs of naming convents\n             'export' is not userd here parser does not know it\n  LogOut is required for permanent changes.\n\n\nUnset:\n  unset ENV_VAR\n  echo $ENV_VAR - returns nothing , bcs it is unset\n\n\nALSO has options:\n  set -o - will display all the options\n   one of the options is 'noclobber'\n   set -o noclobber - prevents '>' redirection from overriding files\n   set +o noclobber - removes noclobber back\n\n\nexecute:\nsource:\ndot (.):\n  read more:\n  https://superuser.com/questions/176783/what-is-the-difference-between-executing-a-bash-script-vs-sourcing-it/176788#176788\n\n\nexecute script if it has x permission\n  !! CREATES NEW SHELL !!\n   ./script - script in current directory(./ dots lash notaion)\n   script - execute script if it is in PATH var\n\n\nsource script even if it does not have x permission\n  documentation:\n  http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#dot\n  !! DOES NOT CREATE A NEW SHELL !!\n  Executes commands from a file in the CURRENT environment. \n  In case it is for login-shell, it will update whole command line, probably\n   not sure how it will work in case of 'su -'\n\n\nsource myscript - myscript is param for source so no matter myscript is in\n             path or not.\n             Still need to be valid shell script(#!/interpreter/pat)\n\n   . myscript - 'source' is alias for . in bash, syntax from POSIX \n\n\nSource saves stuff changed in script(i.e. Env var updates) to the current \n  shell session where as Execute creates new session where changes are saved\n  and kills it after everything is done - basic behaviour of everything\n\n\n===Login process\nRead More:\nhttp://mywiki.wooledge.org/DotFiles\n\n\nNo GUI login into text console. Local login shell:\n1. getty(8) starts and produces 'login:' prompt\n2. getty reads login and passes it to login(1)\n3. login reads password db and decides whether to ask for password\n4. after password provided PAM could be loaded - /etc/pam.d/login\n   this could load additional settings for environment or session etc.\n5. login 'execs' login shell(probably from /etc/passwd) with -\n   -/bin/bash - means \"this is login shell, not a regular shell\"\n6. since it's login shell it reads /etc/profile and /etc/profile.d\n   /etc/profile should have code to read profile.d\n   then ~/.bash_profile, \n   then if no - .bash_login, \n   then if no - .profile\n     .profile is standard Bourne/POSIX/Korn shell config file\n7. bash stops look for config files and displays prompt\n\n\nExample of login process:\nLet's take a moment to review. A system administrator has set up a Debian system (which is Linux-based and uses PAM) and has a locale setting of LANG=en_US in /etc/environment. A local user named pierre prefers to use the fr_CA locale instead, so he puts export LANG=fr_CA in his .bash_profile. He also puts source ~/.bashrc in that same file, and then puts set +o histexpand in his .bashrc. Now he logs in to the Debian system by sitting at the console. login(1) (via PAM) reads /etc/environment and puts LANG=en_US in the environment. Then login \"execs\" bash, which reads /etc/profile and .bash_profile. The export command in .bash_profile causes the environment variable LANG to be changed from en_US to fr_CA. Finally, the source command causes bash to read .bashrc, so that the set +o histexpand command is executed for this shell. After all this, pierre gets his prompt and is ready to type commands interactively. \n\n\n.bashrc is not read for login shells, it could to be read manually in .profile\nIn case this is .profile, before read it, need to ensure that Bash is used:\n  if  [ -n $BASH ] && [ -r ~/.bashrc ]; then\n    . ~/.bashrc\n  fi\n\n\nRemote Login Shell:\nalsmost same as previous, but except 'getty' and 'login' - 'sshd' handles\ngreetings(login prompt by 'getty') and password auth('login').\nPAM also linked with 'sshd', but will read /etc/pam.d/ssh (not .../login).\n Once sshd has run through the PAM steps (if applicable to your system), it \n\"execs\" bash as a login shell, which causes it to read /etc/profile and then \none of .bash_profile or .bash_login or .profile.\n\n\nNOTE:\nduring ssh login, some of local Env Vars could be sent to remote sshd\nlike LANG and LC_* variables want to be preserved by the remote login.\nUnfortunately, the configuration files on the server may override them.\n\n\n===User\\Groups Management\n\n\n=====User\\Groups Management General info\ngroup related info is stored in\n  /etc/group   - file, use 'vigr' for edit\nuser related info stored in \n  /etc/passwd  - file. in case of edit use 'vipw' like 'visudo' with sudoers\ndefault homedir files are stored in\n  /etc/skel/   - directory\n         those files will be copied to user home when it is created\n  ..../.bashrc - executed by bash for non-login shells - every time bash\n         is started interactively(from command line, which is also bash)\nsee more: https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work\n  ..../.profile - executed by command interpreter for login shells\n          is not read if '.bash_login' or '.bash_profile' exists\n        Usually also reads /etc/bash.bashrc\n  ~/.profile - for login-shells also reads ~/.bashrc\n  ..../.bash_logout - executed by bash when login shell ended\n\n\nunlike files in /etc/skel changes to this files will affect even existing users\nchanges from here will be applied with every re-login to every \n        user on thesystem:\n/etc/bash.bashrc -  will affect ~/.bashrc \n/etc/profile -  will affect ~/.profile\n\n\n/etc/login.defs - useradd/userdel/usermod config\n/etc/adduser.conf - adduser config(Ubuntu)\n/etc/shadow - secure account info - passwords\n/etc/gshadow - secure group account info - passwords\n\n\n/etc/passwd:\nroot:x:0:0:root:/root:/bin/bash\nroot - username\nx - placeholder for a password, modern systems use /etc/shadow file for pass\n0 - is the user ID(UID) for this user\n0 - is the group ID(GID) for this user\nroot - comment about this user\n/root - home directory for this user\n/bin/bash - default shell for this user(after he logs in presumably)\n        Possible values:\n  /bin/nologin - deny login at all\n  /bin/false - deny login but still can be logged in using 'su' command from \n        another account\n\n\n/etc/group:\nwheel:x:10:centos,user\nwheel - group name\nx - place holder for a password, modern systems use /etc/shadow filr for pass\n10 - group id(GID) for the group\ncentos,user - users in the group, and probably user wheel too\n   user along with who group was created id not listed in the group..\n   probably to make sure group do or does not contains it , could try check\n   groups of that user like 'groups wheel' - it will return 'no such user' if\n   there is no user with same name as a group\n\n\nNOTE: there are Primary group for a user and SUpplementary groups, \n  Primary is the first group of a user, others are suppplementary:\n    $ groups johnny\n    > johnny : test1 john newgroup1 newgroup2\n  to add extra supplementary groups use 'usermod -a -G group1,group2'. \n  To change Primary group use  'usermod -g john johnny'\n    > johnny : john newgroup1 newgroup2\n  NOTE THAT 'test1' group has been removed at all, and not moved to \n   supplementary groups\n  NOTE to change to test1 w/o deleting it user 'newgrp' command:\n    $ newgrp newgroup1\n    > newgroup1 john newgroup2\n  Files created by user are owned by PRIMARY group of the user\n\n\n=====user\\groups management general info end\n\n\n=====Manipulate Users And Groups\nuseradd:\n  old since any \nnix creation. Almost everything need to be done manually bcs of\n  compatibility - different \nnix platforms handles users differently, their ~\n  directories could be different\n  useradd -\n \n\n  -d <~_path> - use if home name\\path differs from default location, i.e.:\n\n\nuseradd -d /home/accounts/john testuser\n\n\n\nuseradd -c \"John from Accounts\" -m -s /bin/bash john\n   -c - adds a COMMENT in the /etc/passwd for this acc\n   -m - MAKEs the home directory for this user, like /home/john\n   -s - assigns the SHELL for the user\n   john - actual user name\n    user 'john' is the member of group 'john' and this is his PRIMARY group\n  also:\n   -u \n - user ID , first free picked (from range in config) id no specified\n   -g \n - assign to already existing group. i.e. -g accounts\n   -G \n - additional group. i.e. -G employees\n   -e \n - EXPIRATION date of the account\n   -k \n - sKELETON directory if differs from /etc/skel\n   -p \n - encrypted password for acc, or use 'passwd' command later\n\n\nNOTE: after user is created the password need to be added:\npasswd testuser\n\n\n=====Manual User Creation\nuser# sudo su -     - become root\nroot# vipw      - edit /etc/passwd\n add new line with user info like:\n username:x:UID:GID:comment:/home/username:/bin/bash\nroot# vigr      - edit /etc/group\n add new line with user group info like:\n usergroup:x:GID:\nroot# cd /home && mkdir username - create user's home folder\nroot# cp -rf /etc/skel/.* /home/username/. - copy everything from skel\nroot# chown -R username:usergroup username/ - change recurse ownership of dir\nroot# passwd username - create password for the user\n=====manual user creation end\n\n\nid:\n  returns info of current user - id, main group id, other groups, etc.\n\n\ngroups:\n  list groups of a user\n  groups [username]\n    w/o params gives current user groups\n    with username - gives groups of that user\n\n\ngetent:\n  getent group <$(whoami)>\n  lists all groups on system and users of those groups(similar to /etc/group)\n\n\nusermod:\n  modify existing users\n  usermod [\n] [groups] \n\n   -l - change LOGIN of the user. i.e.:\n      usermod -l johnny john - change username john to johnny\n   -d - DIRECTORY path for New home directory of the user\n   -m - MOVE-home dir to new DIRECTORY path\n      usermod -m -d /home/johnny johnny - creates new dir for the user and moves\n    everything from old dir to new dir\n   -g - change primary GROUP\n   -G - add to other GROUPs delimited , and no spaces adds, more groups for user\n    will overwrite other supplementary groups except primary\n   -a - APPEND groups, used with -G, to append existing supplementary groups:\n   Example:\n      usermod -a -G newgroup1,newgroup2 johnny - will add two groups to the\n    end of the list of a groups user johnny have\n   -L - LOCK user\n    after lock there will be exclamation mark in /etc/shadow file before\n    user passwod:\n    johnny:!$6$DQMYnvhr$xKMYZSorH2wePlAunWDBKYWYSK8bmnyKMbr9IAuMoykPl7....\n   -U - UNlock user, and remove exclamation mark '!' before user passwd hash\n  More to read at:\n   https://www.tecmint.com/usermod-command-examples/\n\n\nuserdel:\n  deletes user in old way\n  userdel \n\n  -r - deletes home directory too\n\n\nadduser:\n  adding user\n  shell around perl written useradd command, which is hard to use\n   adduser \n\n  CentOS7, does not have adduser, it links it to useradd:\n   lrwxrwxrwx. 1 root root 7 \u0433\u0440\u0443 14  2016 /usr/sbin/adduser -> useradd\n\n\n/etc/adduser.conf - config for this command, has default ids, default shell\n            default home dir(/home) and so on\n    detailed how adduser works on Ubuntu:\n    https://askubuntu.com/questions/659953/what-is-ubuntus-automatic-uid-generation-behavior\n\n\ngroupadd:\n  adds group\n  info about groups is stored in /etc/group file, see General above for details \n   once group is created use 'usermod' to add users to it\n  group [options] \n\n  -g - manually set GROUP id\n\n\nchgrp: \n  change group, changes group of file or directory\n   Synopsys:\n  chgrp [param] \n /path/to/file\n   Params:\n  -R - recursive\n  -c - like verbose but only when change is made\n  -v - verbose\n\n\ngroupmod:\n  modify existing groups\n  -g - change GROUP id\n    groupmod -g 300 manager - change group 'manager' to have new GID of 300\n  -n - change NAME of a group\n    groupmod -n managers manager - change group 'manager' to 'managers'\n\n\npasswd: \n  change password for current user\n   or for given user\n  passwd [username]\n  located in /usr/bin/passwd - ubuntu\n             /bin/passwd - centos/red hat\n  has 'setuid' thing..\n\n\ngpasswd:\n  add\\remove users from a group, set admins for the group, set password for\n  a group\n  gpasswd \n \n\n  -a \n  - ADD user to a group\n  -M \n - add MULTIPLE users, commadelimited w/o spaces\n     gpasswd -M john,jane manager\n  -d \n - DELETE user from a group\n     sudo gpasswd -d johnny newgroup1 - remove 'johnny' from 'newgroup1'\n  -A \n - add ADMIN user for a group, dunno what it is about.. TODO\n\n\nnewgrp:\n  newgrp \n\n  Changes primary group of the user w/o deleting any groups\n   could freely move between assigned groups \n  And asks for password(probably group password) if trying to set unassigned\n  group as primary one\n\n\ngroupdel:\n  delete group\n  groupdel [params] \n\n\nsetuid and setgid:\n setuid:\n  Attribute of a file that allows unprivileged user to have level of permission\n  of original owner of the file. \n  For example passwd executable is owned by Root user. But unprivileged user \n  could execute it and the passwd binary will made the changes into /etc/passwd \n  and /etc/shadow file(contains some related to passwords stuff too), and those \n  both files also are owned by root user.\n  In the same time passwd used by unprivileged user accepts no params - means\n  user can not change other users passwords\n setgid:\n  Attribute of file\\directory inherited byu sub dirs\\files, so subdirs inherit \n  parent's attributes. \n  Gives access  equal to group owning the file to unprivileged user executing \n  the file.\n  Unprivileged user executes file Under the privileges granted to the user group  owner of that file.\n\n\nwhoami:\n returns username of currently logged user\n\n\n=====manipulate users and groups\n\n\n=====Super User\n/etc/sudoers - file where all the privileges set. has user accounts with \n        privileges, as well as some groups, \n        for Ubuntu: \n            admin - almost root\n            sudo - as root\n        for CentOS7: \n            wheel - as root\nNOTE: this file must be edited through 'visudo' command, this will ensure safe\n  changes including lock file on edit and so on\n\n\nroot access:\n  should be disabled for remote login in sshd for security reasons\n\n\nsu:\n  su - [\n] - creates new session for a user, i.e.:\n     su - user - log in under 'user' user\n  log in as super user(root), need to provide root password i.e.: \n  su -\n   - dash is used to call login shell and reset most of env vars, basically safe\n     against env related exploits and overriden standard commands. see here:\n  https://unix.stackexchange.com/questions/7013/why-do-we-use-su-and-not-just-su\n\n\n!!!!\n  in case root pasword is lost here is how to recover it from recovery mode\n  https://askubuntu.com/questions/24006/how-do-i-reset-a-lost-administrative-password\n  UBUNTU by default installs with random root password so noone knows it,\n   need to use SUDO \n\n\nAnother way to recover a root password is to add a user to a privileged group\n   like 'sudo' or 'wheel' - this way user, using program 'sudo' can become root\n   w/o entering the forgotten(random for Ubuntu) password:\n  user# groups - make sure user is in sudo group\n  user# sudo su -  - become root w/o entering password root's password\n  root# passwd  - enter new password\n\n\nsudo\n  TODO\n=====super user end\n\n\n=====user\\groups management end\n\n\n===Processes and Services Management\n\n\ntop: htop:\n  gives a list of processes and resources used by the OS\n  Process could has a priority:\n   20 - is the LOWEST priority\n   -20 - is the HIGHEST priority\n  PID number 1 is always 'init' command or 'systemd' in my case on ubuntu 16 and\n  centos 7\n\n\nglances:\nhttps://opensource.com/article/19/11/monitoring-linux-glances?sc_cid=70160000001273HAAQ\nif you are a fan of top / htop / atop / iotop....  you might like this..\nlooks like glances can also setup a \"web interface\" to view.. as well as monitor things like docker etc\n\n\ngeneral processes:\n  all processes are spawned from process with ID 1, 'init' in manual or systemd\n  in my case, for some reason\n  so there is a PID which is Process ID\n  and PPID which is Parent Process ID, so every process has parent, except PID 1  it seems\n\n\nstrace:\n  shows trace of used files byt the given command - magic eh?\n   Example:\n  strace -s 2000 hostnamectl |& grep ^open | tail -5\n   will return files directly opened by 'hostnamectl' tool\n   still it could iteract with other services which could read from files, and this is not \n   seen here, alas.\n\n\nwatch:\n  cool debug tool again:\n  execute a program periodically, showing output fullscreen\n   Example:\n  watch \"ps -eaf|grep [h]ostname\"\n  #execute 'hostnamectl' and new entry will be added \n\n\n\n\nroot     11003     1  0 02:51 ?        00:00:00 /lib/systemd/systemd-hostnamed\n\n\n\n\nps:\n  by default returns processes run by my current user and current terminal \n  session\n  ps [-[-]]\n \n       UNIX standards - -\n\n       BSD standards - \n\n       GNU standards - --\n\n    params could be mixed, but conflicts could appear\n   a - list all processes that has terminal attached: ps a\n   x - list all processes owned my you(same EUID as ps)\n   ax - list all processes(less columns)\n   u - user oriented format\n   p \n (-p, --pid)- select process by id (or just PID, == --pid \n)\n   -C \n - select by command name(COMMAND column)\n   t \n - select by tty\n   U, -U, -u, --user - selects by user name or EUID(RUID for -U), different \n        selections by user , will output different stuff\n   -j - jobs format\n   j - job control format\n   -H - hierarhy(tree\\forest format)\n   l - long format\n   -l long format , -y good with this\n   -f - full format listing\n   f - ASCII art process hierarchy (forest)\n   o, -o, --format - change columns format\n    i.e. ps -o pid,ruser=RealUser -o comm=Command - wil return 3 columns\n        named PID, RealUser and Command, with values of standard colmns\n\n\nps aux - returns all processes run by all users from all terminals\n        if user does not exist 'x' in case of UNIX format it could treat it\n        in BSD format\n  ps axjf - formatted method of processes with parents in tree view\n\n\npgrep:\n  process grep , could find process id (PID) by process name, like\n  pgrep bash - will return PID of the bash process running somewhere locally\n\n\nkill:\n  for terminating the processes\n  kill \n \n\n  each kills param has its number equialent\n  -TERM-15-  sends a Signal to a process (Term[inate] Signal), in other words\n   it asks the application to call its Dispose method, to gracefully stop. i.e.\n   kill 1292 or kill -15 1292 or kill -TERM 1292\n  -KILL-9 ask OS's Kernel to shut down the process even if the process(app) \n   does not respond for \n  -HUP - restarts process if possible, does not change PID\n  -l - lists all the signals available with their names and numbers(minus SIG\n   prefix)\n  NOTE: only owner of the process(or root) could kill the process\n  %JOB_SPEC - kills job with appropriate number\n   Examle:\n  kill %1 - will kill the job with JOB_SPEC [1]\n\n\nkill all processes:\n  kill -9 $(ps aux | grep '/usr/lib/firefox' | grep -v grep | awk '{print $2}')\n   $() will be expanded. if list is returned will iterate over every item\n    in the list\n   grep -v - invert grep results (excluding own grep process from ps aux)\n   awk will print second column from the list - PID\n    as per grep matched several lines(List de-facto) every line will be \n    rinted. And sent out of $() into kill -9 as argument\n\n\nkillall:\n kills all instances of the process\n  Example:\n killall xlogo - will kill all instances of 'xlogo'\n\n\nnice: renice:\n  changes priority of the process\n  nice is for new processes\n\n\nrenice is for already running processes\n\n\nrenice \n \n i.e.\n  renice 10 1292 - will change priority of process 1292 to 10, and will display\n   previous value of the priority \n\n\nnice \n \n \n i.e.:\n  nice -n 20 /bin/bash - will start New(-n) process from /bin/bash binary, with\n   priority of 20(the lowest one)\n\n\n=====Service controllers\n\n\nsysvinit, systemd, upstart\n/usr/lib/systemd - systemd service manager working dir (new Rhel\\Debian-like)\n  https://unix.stackexchange.com/questions/5877/what-are-the-pros-cons-of-upstart-and-systemd  - GOOOOOOOD TO READ. Systemd compared to Upstart\n/usr/share/upstart - upstart service manageer working dir(old Debian\\Ubuntu)\n/etc/init.d - sysvitin service manager working dir (old Centos\\Debian\\Ubuntu)\nNote: Ubuntu could has all three installed currently, unlike centos that has\ninitd but it wrapped in systemd\n\n\nservice: daemon:\nservice vs systemctl vs upstart\nservice is an \"high-level\" command used for starting and stopping services in different unixes and linuxes. Depending on the \"lower-level\" service manager, service redirects on different binaries.\nFor example, on CentOS 7 it redirects to systemctl, while on CentOS 6 it directly calls the relative /etc/init.d script. On the other hand, in older Ubuntu releases it redirects to upstart\nservice is adequate for basic service management, while directly calling systemctl give greater control options.\nhttps://serverfault.com/questions/867322/what-is-the-difference-between-service-and-systemctl\n\n\nRHEL-like - systemd (moved from SysVinit)\n    https://fedoraproject.org/wiki/SysVinit_to_Systemd_Cheatsheet\n  Debian-like - upstart service (also adopted systemd)\n  for Debian-like and RHEL-like systems its two different ways\n   for Ubuntu since 15.xx it seems it is even more different..\n   okay its systemd again(which uses systemctl as cli)\n  Debian-like:\n  status \n \n  start \n\n  stop \n\n  restart \n\n   where service could be like 'ssh' or 'cron'\n  to disable\\enable services in upstart services the .override file need to be\n  created in the /etc/init directory. For instance to disable cron:\n   ensure cron exists:\n   /etc/init/cron.conf - should exist\n   echo \"manual\" > /etc/init/cron.override - will create text file with word\n    'manual' as its only contents\n   now cron will not be loaded on boot\n   to enable cron back - simply delete 'cron.override' file from /etc/init dir\n\n\nRHEL-like:\n  systemctl start \n\n  systemctl status \n\n  -- restart\\stop \n   where systemd could be like 'sshd' or 'crond' where d means daemon\n  systemctl disable \n - disable system daemon from starting on boot\n   basically it will delete soft link from /etc/systemd/... directory which it \n   seems is monitored on Init and everything there is executed\n  systemctl enable \n - enable datemon to start on boot\n   basically it creates soft link from real binary location of daemon file into\n   /etc/systemd/.... directory where all the links for boot are stored\n\n\nUbuntu since 15.xx\n  service \n status\n  -- stop\\start\\restart\n   where service could be like 'ssh' or 'cron'\n  service --status-all - will return statuses of all the services\n!!  TODO: \n   check how to enable/disable services on boot for this new stuff and check how\n  it exactly called now\n  it seems it also supports systemctl and disable\\enable with same commands,\n   but creating .override file in the Debian way\n\n\nSystemd:\nconfigure units(services)\nhttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/sect-managing_services_with_systemd-unit_files\n Example:\n https://www.jetbrains.com/help/teamcity/setting-up-and-running-additional-build-agents.html#Automatic-Agent-Start-under-Linux\n\n\nsystemctl:\n  cli for systemd service controller\n  it is called by 'service' command for backwards and cross compatibility reasons\n\n\njournalctl:\n  displays logs\n   Params\n  -u - show logs for Unit\n   Example:\n  sudo journalctl -u sshd.service\n\n\nhostnamectl:\n  cli of systemd service controller related to hostname changes and management\n  unified way to deal with hostname, list /etc/*-release, uname and stuff\n   read more here, like really detailed - good stuff:\n  https://unix.stackexchange.com/questions/459610/whats-the-point-of-the-hostnamectl-command\n   Example:\n  hostnamectl set-hostname 'gfs-server-03'\n\n\nchange hostname:\n  read more:  https://www.cyberciti.biz/faq/linux-change-hostname/\n   with systemd adopted:\n  hostnamectl set-hostname 'gfs-server-03'\n   before\n  /etc/hostname\n   + edit everything with old hostname in /etc/hosts, and add hostname to localhost addr\n\n\nsupervisor:\n  lightweight services controller, available for both RHEL and Debian like\n   distros. Could be used in containers instead of systemd or systemctl etc\n  Manages processes, could restart and monitor them, require config file to\n  operate\n   See how to start several processes in Docker using this tool:\nhttps://kuldeeparya.wordpress.com/tag/how-to-run-ssh-and-apache2-in-docker-container/\n\n\nconfig file:\n    [supervisord]\n    nodaemon=true\n\n\n[program:sshd]\ncommand=/usr/sbin/sshd -D\n\n[program:apache2]\ncommand=/bin/bash -c \u201csource /etc/apache2/envvars && exec /usr/sbin/apache2 -DFOREGROUND\u201d\n\n\n\n===processes and services management\n\n\n===Package management\n=====Debian\\Ubuntu\n\n\n.deb - packages format\n\n\ndpkg:\n  dpackage - debian packages manager, fully console, installs only package\n  -i - install a package\n     only package , no dependencies - generate error with dependencies required\n  -l - list of all installed packages(use grep for specific packages)\n  -L \n - list of all the files that were created during package \n         installation\n  -S \n - similar to 'yum provides' but only for Installed packages\n        it returns list of all packages that contain the string:\n        -S less will return also 'serverless' word.. \n    Another option is to use web interface on Ubuntu:\n    https://packages.ubuntu.com/\n\n\naptitude:\n  frontend of dpkg, GUI in commandline\n   categories of packages, local, available etc, then all the packages by \n  categories. also displays info about each package\n  Enter key to open\\close category.\n  g or u - install selected package\n\n\napt-get: apt:\n  stands for Advanced Package Tool\n   apt is merge of apt-get and apt-cache, for easier use, has not all functions\n   but also has some additional functions\n  main apt tool used to install or download packages\n  reads dependencies, and could install all of them\n  apt-get update - reads all the repos and updates the local packages cache\n  apt-get install - installs package(s), list delimited by space\n       or install \n - installs particular version if\n            it is compatible with distro and stuff \n\n       install -f - will install .deb package with its dependencies\n            Fix Broken Dependencies:\n            https://unix.stackexchange.com/questions/159094/how-to-install-a-deb-file-by-dpkg-i-or-by-apt\n  apt-get upgrade - made after Update, upgrades all the packages installed to\n            latest updates\n          -y - answer Yes for all questons automatically\n  .. dist-upgrade - updates to next available supported distro, 14.04 to 14.10 \n            if 14.10 is not supported already then to 15.04 etc\n  apt-get autoclean - cleans cache, which means freeing space on hdd\n\n\napt search \n - search for package\n  apt list --\n - lists available packages from repos\n       --installed - lists only installed packages\n  apt show \n - shows info about package(apt-cache's command)\n\n\n--dry-run - simulation, will display possible actions but not perform any\n\n\napt-get check - what dependencies may be broken\n  apt-get build-dep - exact build dependencies for particular app (it seems it \n        is not necessery to download them all for work though...)\n\n\napt-get download \n - downloads package(probably to current dir\n                   or /etc/apt?) - but only package w/o dependencies\n  apt-get changelog - package changelog, like version history\n\n\n/etc/apt/ - apt configs\n   sources.list - config of the repositories, which are re-read during 'update'\n          command execution\n  /var/cache/apt - cache folder of apt\n  ./archives - contain all the archives, could be removed by 'autoclean' command\n\n\napt-cache:\n  support tool of apt used to work with apt cache(updated by apt-get cache \n  command)\n  apt-cache pkgnames - list of all pkgs APT knows, not all could be downloaded\n            installed or installable e.g. virtual pkgs\n  ... search \n - lists all packages that contain package name in its \n             name or description\n  ... show \n - info about the package, like description in apptitude\n  ... stats - info about local cache(packages related stuff) - could be shrinked\n          by running 'apt-get autoclean' to delete useless stuff\n  apt-cache showpkg \n - shows infou about package\n            and where from package came from\n    Example:\n\n\n\n\napt-cache showpkg htop\nPackage: htop\nVersions: \n2.0.1-1ubuntu1 (/var/lib/apt/lists/ua.archive.ubuntu.com_ubuntu_dists_xenial-updates_universe_binary-amd64_Packages)\n     it is htop package with version 2.0.1ubuntu1\n     taken from ua.archive.ubuntu.com\n      where repository is for xenial\n      and name of repository is Universe\n   apt-cache policy - returns all used repositories, kind of \n\n\n\n\napt-file:\n  another extension for apt-get, is in different package, \n   need to be installed first\n\n\napt-file update - updates its own caches, need to be run first\n  apt-file find \n  - alias for search, see below:\n  apt-file search \n - search packages for file matched by pattern.\n        returns list of packages and paths where file was found;\n        includes removed packages too\n  apt-file list - similar to dpkg -L but package not need to be installed or\n        fetched\n\n\n=======Uninstallation\n  apt-get remove \n - removes package binaries, leaves configs in the \n                     system for future use by other version or another\n                 reason\n  .. remove --purge \n - removes package and all the stuff package \n  .. purge \n ^same^     created during install(SYSTEMWIDE configs, link\n                etc)\n        it will not remove:\n         -dependence packages , to delete orphanes use\n            apt-get autoremove\n              or\n            .. --purge autoremove (same - will remove configs also)\n         -NOT SYSTEMWIDE config files - user-specific files\n            files in user's home dir, or .config subdirectory of \n            home, those could be hidde (starts with .)\n         -doesn't reverse changes in already existing user-specific\n          config files\n         -doesn't remove 'gconf' and 'dconf' files or reverse any\n          configuration d\\gconf changes\n    existing SYSTEMWIDE configs also are not affected by neither purge or\n    remove commands, those ones created by user or other packages. but \n    uninstalling package could sometimes affect such files and undone \n    something\n  apt-get autoremove - removes all orphaned packages\n  .. purge --auto-remove \n is similar to autoremove\n=======uninstallation end\n\n\n=======Repository setup\n Repository configs are in \n  /etc/apt/sources.list and /etc/apt/sources.list.d/\n\n  Contents of sources.list file:\n   deb http://archive.ubuntu.com/ubuntu/ xenial main restricted universe\n  where http... is address of repo\n        xenial is name of distro - $(lsb_release -sc)\n        main ... is name of repository\n   After 'add-apt-repository multiverse' this repo will be concatenated into\n   line above:\n    deb http... xenial ... multiverse\n\n\nadd-apt-repository:\napt-add-repository:\n  Newer versions of Ubuntu support 'add-apt-repository'\n   standard repositories could be added w/o specifing address, just repo name: \n    Example:\n   sudo apt-add-repository universe\n    this will add 'universe' repo to standard ubuntu addresses\n  Older versions support this:\n   sudo add-apt-repository \"deb http://archive.ubuntu.com/ubuntu $(lsb_release -sc) universe\"\n\n\nNOTE: make sure to 'sudo apt-get update' before use newly added repository\nRead More:\nhttps://askubuntu.com/questions/148638/how-do-i-enable-the-universe-repository\n\n\nDue to regular use of secure stuff also make sure following packages are\n  installed(use apt-cahce show \n or 'apt list --installed'):\n   apt-transport-https - This package enables the usage of \n    'deb https://foo distro main' lines\n    in the /etc/apt/sources.list so that all package managers using the\n    libapt-pkg library can access metadata and packages available in sources\n    accessible over https (Hypertext Transfer Protocol Secure).\n      This transport supports server as well as client authentication\n     with certificates.\n   ca-certificates - list of PEM files of certificates approved by Certificate\n    Authorities, some common trusted certificates. For instance it has\n    certificates for Debian stuff and Mozilla stuff.. \n    TLDR:\n    It is a list of default trusted SSL sertificates stored in PEM format.\n   curl - tool for transfering data from or to a server via HTTP/S, FTP/S, LDAP\n    and lots of other formats, except SSH, but it has SCP..\n    Used to download GPG key from docker storage. Which is added then to\n    local apt storage\n   software-properties-common - adds additional command for APT repositories \n    management. Such as 'add-apt-repository'\n\n\nAdd gpg key:\n    curl -fsSl https://download.docker.com/linux/ubuntu/gpg | sudo apt-key\n    add -\n      Curl will download gpg key from docker , then pass it to apt-key (-) \n    and it will be added to local key storage\n    make sure it is added, by searching for fingerpring:\n    sudo apt-key fingerprint 0EBFCD88 - this will return OK if everything is\n    fine\n  Add a STABLE repository, it seems it is required:\n    it is stored in:\n    /etc/apt/sources.list - the file with the sources, where \n    add-apt-repository will add the repo by default\n    /etc/apt/sources.list.d - directory where new file with a repo should be\n    added, like in CentOS' yum\n    add-apt-repository example:\n    sudo add-apt-repository \\\n       \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n       $(lsb_release -cs) \\ - returns distro name\n       stable\" - list of repos, test, edge could be added for docker\n  PPA: launchpad.net:\n        Ubuntu in default repos has only stable versions of packages that was \n        on moment of release. Later on only security updates.\n        launchad.net has lots of repos(Personal Package Archives - PPA)\n        add PPA repo to for latest versions:\n      Ansible:\n     $ sudo apt update\n     $ sudo apt install software-properties-common\n     $ sudo apt-add-repository --yes --update ppa:ansible/ansible\n     $ sudo apt install ansible\n    Another example:\n     $ sudo add-apt-repository ppa:jonathonf/python-3.7\n     $ sudo apt-get update\n\n\nupdate-ca-certificates:\n  updates certificates from \nsomewhere\n\n   Pamars:\n  --fresh - somethinh fresh?\n\n\nList certificates:\n  awk -v cmd='openssl x509 -noout -subject' '\n    /BEGIN/{close(cmd)};{print | cmd}' < /etc/ssl/certs/ca-certificates.crt\n   Search for existing root cert:\n  \\ | grep -i '\n'\n\n\n=======List all installed packages:\n  dpkg -l - list of packages and their details\n  apt list --installed - list of full names\n  aptitude - has a category for installed pkgs divided by categories\n Active Repositories\n  apt-cache policy\n\n\n=====debian\\ubuntu end\n\n\n=====RedHat RHEL\\Centos\n\n\n.rpm files\nrpm:\n  red hat package manager\n  It seems it has some differend modes, where similar keys could exist with \n  different functions, i.e. -i - installation mode, -i in -q(query) mode is info\n   the modes are:\n   - install/update/freshen (first param -i)\n   - uninstall (first param -e)\n   - query (first param -q)\n   - verify (first param -V probably)\n   - set owners/groups\n   - show querytags\n   - show configurations\n  Also it has general options, seems like it works with any mode\n  if files or dependencies package need are absent installation of rpm will fail\n  -q - query, probably 'what about something, like package requirements'\n    single param will display package full name if it is installed or will\n    say that it is not installed, i.e.:\n!   rpm -q openssh-server - will return fill name if installed, like this\n                openssh-server-7.4p1-13.el7_4.x86_64\n   -p - package file\n   -R - requires, lists package dependencies\n  i.e.:\n  rpm -qpR \n - will list package requirements(depen\n    dencies), if rpm_package name is given(xterm-123.3.rpm) - will check for\n    package locally if general package name is given(xterm) will look into\n    remote repos\n   -l - list files of the package, similar to 'dpkg -L' i.e.:\n    rpm -ql openssh-server - will list all the files and paths of sshd\n   -a --last - will list (a)ll the packages installed filtered by latest\n   rpm -qa | grep nmap - will list all Installed packages, and grep nmap from \n        this list\n   -d - documentation mentionings of a package \n   -f - some related to documentation param\n   rpm -qdf \n - return all documentation files where this package is\n        mentioned\n  -i - installation mode (fails if package installed of any version)\n   -v - verbose (general parameter)\n   -h - prints some hashes, looks better with -v\n   -U - update package, BUT will install package if there is none installed\n    if package is installed will not fail but update it(controversy to -i)\n  rpm -Uvh \n - will update or install package, \n    fails if there are missing dependencies\n  -e or --erase - erase package, uninstall mode\n   -v - short verbose mode, RETURNS NOTHING if no Verbose mode is passed\n   -vv - very verbose, dunno why not -v\n  -V - verify, there is some verification key for every package and rpm could\n    check whether the package indeed has that key\n   -a - all, same as for -a in Query mode of rpm tool \n   Keys could be imported from remote repos, or rpm could list all the keys\n   that has been already imported\n   rpm --import - will import key somehow from somewhere..\n   rpm -qa gpg-pubkey* - will list all the public verification keys on system\n\n\nyum:\n  Stands for \"Yellowdog Updater, Modified\". yellowdog is already unsupported \n  version of Linux for PowerPC, which is also dead now\n  yum \n \n\n  Package management system\n  yum update \n  yum upgrade - same as update - updates all the pkg repos and upgrades pkgs\n\n\nyum list [\n] - displays if installed, version, and @updates mean that it\n           accepts updates. Package could be market to ignore updates\n           which probably means that yum update will not affect it\n  ..  list installed - returns list of installed packages\n  yum search \n - searches for package, search in name and description\n  yum install \n - install package, and its dependencies\n    -y - answer Yes for prompts automatically\n  ..  install https://...../package.rmp \n        will install package from remote location\n        or this installs repolist.. from which that packages could be \n        installed..\n        --enablerepo=\n  - enables repo\n    Example:\n   yum install -y https://centos7.iuscommunity.org/ius-release.rpm && \\\n    yum install -y php71u-fpm\n  yum localinstall \n - installs .rpm package downloaded to local machine\n    will ALSO install needed dependencies, unlike 'rpm' tool\n  yum info \n - info about a package: descr, url, size, arch, etc\n  yum check-update - checks which packages could be updated\n     -C - use only local cache, doesn't update cache\n  yum grouplist - YUM can group packages, this command displays all the groups\n          those groups will install bunch of packages\n  .. groupinstall '\n' - will install all the packages from the group\n  .. gropuremove '\n' - removes all the packages related to the group\n  .. groupupdate '\n' - updates same stuff\n\n\nyum-config-manager --enable remi  - enable repo\n\n\nyum repolist - lists all available(enabled) repositories\n  ..  .. all - lists all the repos disregard of their staus\n  .. --enablerepo=\n install \n - enable a repo and\n        install the package from that repo\n    Example:\n   yum repolist enabled\n    show enabled repos\n   yum repolist all\n    show all repos (including disabled)\n  yum provides \n - Just use a specific name or a \n    file-glob-syntax wild\u2010cards to list the packages available or installed\n    that provide that feature or file.\n         Note:\n        this could not work for regular user, use 'sudo'\n  yum clean all - clears cache similar to apt-get autoclean\n    Example of usage:\n     use it after some bunch of packages has been installed to clean space\n  yum history - history of yum - install\\remove\\upd package and stuff, sudo\n\n\n=======Manage repositories\nrepos stored in /etc/yum.repos.d, which is a directory\nto add new repository:\n create new file in the /etc/yum.repos.d dir named '\n.repo'\n\n\nFormat of repo file:\n[dockerrepo]  - name of the repository section\nname=Docker Repository - name of repo to display\nbaseurl=https://...    - address to where from do pulls of packages\nenabled=1          - repo is enabled inside my system\ngpgcheck=1         - enables gpg check\ngpgkey=https://....    - location of the key\n\n\n=======Uninstallation\n  yum remove \n - removes the package, doesn't remove its dependencies\n!   yum autoremove \n - also removes package, probably its deps too..\n  To remove dependencies there are several ways:\n  Option1: [probably should do this as standard]\n  yum autoremove - removes orphaned dependencies, similar to 'apt-get utoremove'\n  Option2: [probably could try this]\n  update /etc/yum.conf file\n   set\n    clean_requirements_on_remove=1\n   it is boolean value, which works on removel\\update\\obsoletion, goes through \n   each package's dependencies and deletes ones that are no longer required.\n   default value False, could be 1,0,True,False,yes,no\n  Option3: [probably better try not to use it]\n  yum history undo \n\n  basically in undoes the operation, that was performed during installation\n  and thus removing the package and all the dependencies, and probably other\n  stuff. \n  the only thing here is that i'm not sure whether it checks that package from \n  dependencies is dependent only by this package that caused them to be \n  installed, or not, so is not - it could end up with broken dependencies\n  so how to do:\n  yum history - check the ID column next to the command line with installation\n  command of the package that need to be uninstalled\n  then call 'yum history undo \n'\n=======uninstallation end\n\n\n=======List all installed packages:\n  rpm -qa - query all packages, will return list of all installed packages\n  yum list installed - will return list of installed packages\n\n\nyum-utils:\n  yumdownloader:\n  yumdownloader \n\n    downloads a .rpm file, into current dir pretty similar to apt-get download\n    command\n=====redhat rhel\\centos end\n===package management end\n\n\n===File Permissions / Ownership\n\n\n! Linux treats everything - device\\file\\directory as if it is a FILE\n\n\nDiscertionary Access Control (DAC)(used after Mandatory Access Control MAC from LSM):\ndrwxrwxrwx:\nd - type of file(file\\device\\dir\\link)\n r - read\n   r only gives ability to list contents, but lots of ????? and file names\n    can't even read files with 'cat' and stuff, need x permission for this\n w - write\n x - execute\n   x only gives ability to 'cat' files, but cant ls into dir\n    if file in such dir has only 'r' can even 'cat' it, if file has only x: cant\n     cat it, need 'r' on file in dir with only 'x' to cat\n   together with read - can ls norm and execute stuff like 'cat' to read file\nfirst 3 - user permissions \nsecond 3 - user's group permissions\nthird 3 - all other users\n\n\nchown:\n  changes owner and group of a file\\directory\\link\n  chown \n [user][:group] \n\n         -reference=R_FILE FILE\n  !!!! links could change owner of targeting file only!!!!!!!!!!\n  -h \\ -H - for changing owner of LINK not referenced location, see MANUAL\n  -R - recursively change owners of nested directories\n  -v - verbose, could be redirected to a file for future use, just in case\n   in case of fucked up links\n  could change only owner(\n), only group(<:group>), or both(\n)  could take ownership schema like owner:group from a file:\n   chown --from=:user :otheruser file.txt\n\n\nchmod:\n  change modification\n  chmod [\n] \n \n\n  text modifications:\n  u - owner\n  g - group\n  a - all\n  r/w/x - read/write/execute\n  i.e.\n  chmod a+rwx \n - give all the permissions of read/write/execute\n\n\nbinary mode:\n  4 - read\n  2 - write\n  1 - execute\n\n\nsum of numbers above will indicate permissions\n  rwx = 4+2+1 = 7, and so on \n\n\nParameters:\n  -v - verbose for every file processed\n  -c - changes (less verbose), only when change is made\n  -f - supress most error messages, --silent, --quiet\n  --preserve-root - fail to operate recursively on '/'\n  -R - recursive\n\n\nlsattr:\n  lists extra attributes of files on linux file system\n  lsattr file1 - display attributes of the file1\n\n\nchattr:\n  changes extra attributes, lots of them, here some:\n  i - immutable (can not be changed)\n  a - appendable (can be opened only in append mode)\n  u - undeletable\n  syntax: \n  + - adds attribute\n  - - removes attribute\n  = - causes  selected attrs to be only attrs that files have..(not sure)\n  i.e. \n  chattr +u file1 - adds undeletable attr to file1\n\n\n===file permissions / ownership end\n\n\n===IPTables / Linux kernel Firewall(not d)\nread more:\nhttps://www.unixmen.com/iptables-vs-firewalld/\nhttps://linuxacademy.com/cp/socialize/index/type/community_post/id/15473\n\n\n!!!!!!!!!!!!!!!!\n!!  WHen using SSH configure SSH rules first BEFORE changing \n!!    the policies of the chains in the Filter table. \n!!  !!    TO DO NOT LOCK MYSELF OUT OF THE MACHINE  !!  !!   !!   !!\n!!  CentOS 7 applies the rules right after entering the command\n!!  even if Service IS stopped.\n!!!!!!!!!!!!!!!\n\n\n///Iptables is a tool to work with Netilter Framework, kind of firewall\n/There are different services on different Linux distributions such as:\nUbuntu - ufw(Uncomplicated Firewall) service for Ubuntu, which uses IPtables\nCentos7 - firewalld - firewall daemon which uses iptables\ngeneric? - iptables-services - firewall service that uses iptables, generic \n       it seems\n\n\nMain three groups or something:\nTables:\n used to group different functionalities and separated in five:\n - filter - default table if no specified. Packet filtering.\n   Built-in chains:\n    - INPUT\n    - OUTPUT\n    - FORWARD\n - nat - used for network address translation(NAT)\n - raw - first table to check, configuring exemptions from conn. tracking\n - mangle - some specialized packet alteration...\n - security - Mandatory Access Control(MAC) networking rules\n -t--table - parameter to specify a table\n\n\nChains:\n Tables contain set of chains.\n Chains group rules on different points of process.\n Chains could be Built-in or User defined.\n  INPUT - Filter. input packets\n  OUTPUT - Filter output packets\n  FORWARD - Filter. going through packets\n -N--new-chain - adds user defined chain\n -X (--delete-chain) - delete chain\n\n\nRules:\n Defined as a set of matches and a target. \n Are listed in chains and followed by order until a match is found, then packet\n is handled by the target specified by the rule.\n -A (--append) - add new rule\n -D (--delete) - delete rule along with the chain in which is contained\n -L (--list) - list rules\n -S (--list-rules) - same list but in command format, as if the commands\n             that were used to add rule is saved and displayed here\n\n\nMatches:\n   if match is true the packet will be processed by iptables.\n   Following matches exist(not full list):\n   -s (--source) - source of the packe(IP, hostname or network IP)\n   -d (--destination) - destination of the packet(IP, hostname or network IP)\n   -p (--protocol) - protocol(tcp/udp/all , all - default if none specified)\n   -i (--in-interface) - interface that receives(left side of ifconfig output)\n   -o (--out-interface) - output interface\n   ! - Not. Match everything that is not in the match.\n\n\nProtocols has also its own matches, like \n   --dport - destination port(22 for ssh) is match for TCP protocol\n  see full list of protocol related matches:\n   iptables -p \n -h - where protocol is tcp, icmp etc i.e.:\n    iptables -A INPUT -p tcp -i eth0 --dport 22:25 -j ACCEPT \n      - adds rule to default table (-t filter) , appends Chain INPUT, matches:\n    protocol TCP and interface eth0, protocol specific match port with\n    range of ports 22 to 25. Rule's target if match is met - ACCEPT. \n    If not next rule will be processed until RETURN or end of rules, then\n    Default TARGET(Default policy) will be applied.\n\n\nTargets:\n  Determines the action to be taken if packet is Matched.\n  -j (--jump) - specify Target\n  There are 4 built-in Targets:\n   ACCEPT - no checks , just accept packet\n   DROP - refuse packet, do not send a response(simply ignore it)\n   QUEUE - sends the packet to user space\n   RETURN - returns to previous chain, or handle by Chain policy(Default?)\n\n\nDefault Policies:\n  When packet is not matched by any Rule on the Chain - it is handled by the\n  target specified in the policy of that chain.\n  Two main approaches:\n   Accept everything by default, and add rules to refuse access\n   Refuse everything by default, and add rules for accept\n  -P (--policy) \n \n - set default policy for a chain with target\n\n\niptables:\n  a generic table structure for the definition of rulsets\n  basically ruleset of Linux kernel firewall\n  it is a tool to get daemon(service) install other app:\n   Centos - iptables-services\n   Ubuntu\\Debian - ufw(probably goes with iptables pkg)\n   RHEL - iptables\n\n\n-L - list all rules currently installed on the system\n  -A \n - ADD\n\n\nService uses loads default config file when starts, applying some \ndefault rules, /etc/sysconfig/iptables for CentOS 7\n  to manage a service:\n   Debian/RHEL: sudo /etc/init.d/iptables start\\stop\\status\n   Ubuntu: sudo service ufw start\\stop\\status\n   Centos: servicectl start\\stop\\status iptables\n\n\nIP address ranges:\nCIDR blocks\nhttps://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation\nhttps://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#IPv4_CIDR_blocks\n\n\nfirewalld:\nfirewall-cmd:\n  https://www.digitalocean.com/community/tutorials/how-to-set-up-a-firewall-using-firewalld-on-centos-7\n firewall-cmd --get-default-zone\n  get default zone\n firewall-cmd --get-active-zones\n  get all active zones\n sudo firewall-cmd --list-all\n  list all rules for zones\n\n\nOpen ports:\n  1. Open port for a service.\n   add service to /etc/services\n   add service to FirewallD\n   https://linuxhint.com/open-port-80-centos7/\n  2. Open prot directly:\n\n\nOPEN PORT:\n  sudo firewall-cmd --zone=public --add-port=5000/tcp --permanent\n CHECK OPENED:\n  sudo firewall-cmd --zone=public --list-ports\n RELOAD AFTERWARDS:\n  sudo firewall-cmd --reload\n\n\nExample:\n  firewall-cmd --zone=public --add-port=5693/tcp      - turns on the rule\n  firewall-cmd --zone=public --add-port=5693/tcp --permanent   - saves the rule so it is applied on the boot\n\n\n===ip tables / firewall\n\n\n===Bash\n=====Bash command line shortcuts\nctrl+a - moves cursor on the beginning of the line\nctrl+e - moved cursor on the End of the line\nctrl+u - removes everything from cursor position to the beginning of line\nctrl+k - removes everything from cursor position to the edn of the line\nctrl+w - removes previous word(space separated)\nctrl+l - cleares screen except of current command LINE\nctrl+p - last executed command shown in command line, same as UP ARROW key \nctrl+c - terminate current process + exit\\del current line by terminating it\nctrl+z - pause program, adding it to backgroud job. 'bg'\\'fg' to continue run\nhistory:\nctrl+r - bash history by typed command\nctrl+r - again looks one matched line upwards\nctrl+j - edit matched line\nctrl+o - execute current command from history\n\n\n=====Bash Shell\nalias:\n new aliasws workws only for current session\n alias \n=\"\n\" - adds command as alias\n alias \n - shows command\n alias - shows all aliases for current session\n\n\nsleep:\n  just regular sleep, stops current thread for amount of given seconds\n\n\nExperiments:\n  sleep 10 & - will stop the thread for 10 seconds and move it to background\n\n\nbackground: (not command)\n  to start process in a background use & at the end\n   Example:\n  xlogo & - will start xlogo but cmd will return control to user\n   xlogo will run as a Job, with 'Running' state\n\n\njobs:\n  built-in command\n  shows jobs in background, running, stopped\n   More read:\n  https://unix.stackexchange.com/questions/116959/there-are-stopped-jobs-on-bash-exit\n   Params:\n  -l - display PID\n\n\nNOte: kill could also accept JOB_SPECs to kill jobs, instead of PIDs\n\n\nfg:\n  moves job to the foreground\n  fg JOB_SPEC\n   Example:\n  fg 1 - moves job [1] to the foreground\n  Ctrl+z will move it back, but 'paused'- use bg to continue in background\n\n\nbg:\n  moves job to the background\n   If program is 'Stopped' like after executiong ctrl+z shortcut, bg <%num>\n  will continue its execution as a job, in the background\n   Params:\n  %job_spec - address numbered job\n\n\nscript:\n  writes a session script - all executed commands and stuff\n   with time parameter will create file with timings of each command and could\n  play it afterwards using these two files\n\n\nbash:\n  man bash\n\n\n-c - accepts a whole bunch of commands like:\n  /bin/bash -c \"while true; do echo HELLO; sleep 1; done\" - which will execute\n    echo HELLO endlessly witn interval of 1 second.\n    This even could be passed into a container:\n   docker run -d ubuntu:xenial /bin/bash -c \"....\"\n\n\ncould customize command prompt from default \n@\n:\n$ \n  to something like \n \n \n!\n   \\t - 24 hour time format\n   \\w - full current path\n   \\w - current directory only\n   \\u - username\n   \\j - amount of running jobs\n   \\h - hostname\n      Hostname is stored in /etc/hostname\n\n\nhostname:\n  network computer name. command returns current hostname\n  value is stored in /etc/hostname\n   Synopsis:\n  hostname [-b] {hostname|-F file} - set hostname (from file)\n  hostname [params]            - formatted output\n  hostname             - display hostname\n   Params:\n  -I - show all ip addresses for host machine\n  -b - set hostname\n   Example:\n  hostname -b new_hostname\n\n\nTO CHANGE HOSTNAME:\n   change hostname in /etc/hostname\n   change every occurance of old hostname to new one  in /etc/hosts\n    If nothing is available in /etc/hostname first hostname with 127.0.0.1 \n   address will be taken as hostanme\n\n\nhistory:\n  lists history, dumps .bash_history file into STDOUT, lines numbered\n  -c - Clear history, but only in session, .bash_history remains the same\n    only in CentOS it seems, ubuntu does nothing\n   Example:\n  history | grep /usr/bin - look for only what matched\n\n\nwhich:\n  shows location of a command and its alias if exists\n   Does not show built-ins\n  can search by alias too\n  which \n\n   i.e.\n  which ll - will return alias line and path for binary\n\n\nwhereis:\n  searches for binary file, sources, man files of the command\n  Does not work with aliases!\n   i.e.\n  whereis pwd - will return path for binary and path for man page\n   -b - search only for binaries\n  Example:\n  Remove IntelliJIdea:\n  $whereis idea\n  remove all entries returned, and places whihch they are linking\n   https://intellij-support.jetbrains.com/hc/en-us/articles/206544519\n   https://askubuntu.com/questions/300684/uninstall-intellij-ultimate-edition-version-12/300692#300692\n\n\nStreams and Redirects. redirection:\n Linux\\Unix philosophy is that 'everything is a file' but in fact this are not\n  files but block defices stored in /dev or /proc directories, and not files but\n  some stuff stored in RAM for instance\nSTD - means standard\n  Read More:\n  https://stackoverflow.com/questions/3385201/confused-about-stdin-stdout-and-stderr\nSTDIN:\n  I/O Stream\n  file handle that process read to get info from user\n  NOTE:\n/dev/stdin is a symlink to /proc/self/fd/0 -- the first file descriptor that the\n  currently running program has open. So, what is pointed to by /dev/stdin will\n  change from program to program, because /proc/self/ always points to the \n  'currently running program'. (Whichever program is doing the open call.) \n  /dev/stdin and friends were put there to make setuid shell scripts safer, and\n  let you pass the filename /dev/stdin to programs that only work with files, \n  but you want to control more interactively. (Someday this will be a useful \n  trick for you to know. :) \nSTDOUT:\n  I/O Stream\n  process writes normal info to this file handle\nSTDERR:\n  I/O Stream\n  process writes error info to this file handle\n\n\n\n\n\n\nused to redirect STDOUT in cli to someplace else (i.e. file instaed of \n    console)\n  2> - redirects STDERR\n\n\n\n\nredirects STDOUT\\ERR but file will not be overwritten, and appended \n    instead\n  < - redurects STDIN, it will take data not from command line but from\n    something else, i.e. a file\n  cat /etc/passwd > /tmp/out     # redirect cat's standard out to /tmp/out\n  cat /nonexistant 2> /tmp/err   # redirect cat's standard error to /tmp/error\n  cat < /etc/passwd              # redirect cat's standard input to /etc/passwd\n    or like this:\n  cat < /etc/passwd > /tmp/out 2> /tmp/err\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE: redirect Standard Output(and error too) to null will hide it from everyone\n  like this:\n   ls secret_place >> /dev/null - will drop output into a void!!11\n  Errors also could be hidden like this, in case of exposing of some secure or\n  sensitive info\n\n\nRedirect SDOUT and STDERR into two files in the same time:\n  cat goodFile notExistingFile >mystdout 2> mystderr\n    mystdout will contain standard output\n    mystderr will contain standard error output\n\n\nRedirect STDOUT and STDERR into single file:\n  cat goodFile notExistingFile >mystdout 2>&1\n    mystdout will contain both STDOUT and STDERR\n    Or\n  cat goodFile notExistingFile &> mystdout\n\n\nnoclobber:\n  is an option set-up by 'set' command\n   prevents redirection '>' from overriding files. it will return an error\n\n\npipe: |:\n  redirects standard output of first program into standard input of another\n\n\nexpand:\n  changes Tabs to Spaces in files from argument or stream from STDIN\n   Example:\n    cat -A test   >>> ^Ia ^Ia    a^Ia$\n    expand test | cat -A  >>>        a       a    a  a$\n  will change all tabs in file to spaces.\n\n\nunexpand:\n  change Spaces to Tabs - vice versa of 'expand'  \n\n\ncut:\n  Removes section from each line(column in fact) from a File\n   Or parses each line in file by delimiter(Default is TAB) and could retreive\n   only selected columns, or bytes - this is for -f option\n  cut \n \n\n  See more: https://www.computerhope.com/unix/ucut.htm\n  -d '\n' - change delimiter to \n10.1\n10/01/2007 - '-f 3' will cut out '10/01/2007'\n  How LIST works:\n   !List of Integers\n   !Starts from 1\n   list of Integers, or range of integers, or multiple ranges, Separated by \n   commas. Selected columns is printed in the same order they were read,\n   written to output exactly once(dunno what it means)\n  N - the Nth byte, character or field\n  N- - range from Nth byte,char,field to end of the line\n  N-M - range from N to M as above\n  -M - from beginning of the line till M\n  -f Option:\n  cut -f 1-2,4-5 data.txt - will cut 1st,2nd, 4th, 5th columns from data.txt\n   --output-delimiter - changes delimiter in the output i.e.:\n    cut -f 1,3 -d ':' --output-delimiter=' ' /etc/passwd - will substitute\n    colon delimiter ':' to space ' ' in the OUTPUT of command for easier\n    read\n    ..... --output-delimiter=$'\\t' - will substitute colon ':' for tab '\\t'\n    character, '$' here is for escapt\n  -c Option:\n   cut -c 3-12 data.txt - will cut from 3rd to 12th characters in the data.txt\n        will not use delimiters at all\n  -b Option:\n   cut -b 3-12 data.txt - will cut rom 3rd to 12th BYTE, in case of ASCII \n        encoding it will be the same with -c as there 1 char == 1 byte\n\n\nAdvanced example:\n   grep '/bin/bash' /etc/passwd | cut -d ':' -f 1,6 - greps passwd file for \n    users that use /bin/bash as their default shell, and prints out \n    username and user home directory\n    So cut accepts STDIN redirection\n\n\npaste:\n vice versa of 'cut' - pastes columns in files\n\n\njoin:\n sql join but for text files\n  in case of need to use this, use man\\google, i'm to lazy to document\n Sorry.\n\n\nbash escapes:\nspecial symbols:\n$'\\t' - print tab, which is not printable character, same as /r/n or somehting\n\n\n=====Bash Scripting\ntest:\n [ 1 -eq 1 ]    - ints compared with -eq, -lt, -gt, -ge, etc\n [ 'a' = 'a' ]  - strings compared with =, <, >, >=, etc\n  tests condition, returns 0 if test is passed(true)\n\n\n-f \n - file exists, aviode ~\n  -d \n - directory exists\n  -h \n or -L - symlinc exists\n  -s \n - exists and has size greater than 0\n  -u \n - exists and set-user-ID bit is set\n  -k \n - exists and its sticky bit set \n  -n \n - length is not zero\n  -r \n - file exists and with read permission\n  -w \n - file exists and with write permission \n  -x \n - file exists and with execute permission\n  -z \n - length of string is 0\n\n\nTest could be performed from script , better to use braces [ . ]\n  Also could be preformed right inside CLI, it will return nothing, so need to\n  create manual then\\else-like structures:\n   test -f '/path/to/file name' && echo \"exitcode: &?. True\" || echo \"exitcode:\n &?. False\" - test for file existance, then if exit code 0 will execute second \n    echo with True, else (OR) will execute echo with False.\n      &? -  will print exit code of previously executed command, which in this \n    case is 'test' command\n\n\nTest even can be performed w/o test itself:\n    Example:\n  curl -s google.com | egrep -ci '301 moved' > /dev/null && echo \"file has moved\n\" || echo \"false\" - curl will pass STDOUT to egrep which will try to match \n    string '301 moved', if it succeed exit code will be 0.\n    And first Echo (after &&) will be executed\n    Else, egrep will return 1, and thus second echo (after ||) will be\n    executed\n\n\nfor:\n  regular for loop could be done in command line too\n\n\nfor i in \ncommand\n; do echo $i; done\n    backtick -  \n, does the job, expression between it will be executed in \n    sub shell and its result will be returned, when we have a collection\n    we could iterate it with FOR \n  for line in\ncat file1`; do echo $line; done\n\n\n===Other programs:\n\n\nelinks:\n  command line browser, with interface similar to MC or aptitude\n\n\nmc:\n  two-windowed GUI to work with file system\n\n\n===Nginx\n  fast web server, could be used as load balancer between servers(docker \n   containers) - reverse proxy server\n\n\npackage name: nginx\n  /etc/nginx - config(as usual in etc dir)\n    ...conf.d - directory for some main configs probably\n    ...default.d - directory for default configs\n    ...sites-available - directory for available sites, convention directory\n    does not exist by default\n    ...sites-enabled - same as previous\n    Read more about sites-* here:\n    https://serverfault.com/questions/527630/what-is-the-different-usages-for-sites-available-vs-the-conf-d-directory-for-ngi\n\n\nBasically all the configs are included in main config:\n/etc/nginx/nginx.conf\nit includes all the configs from directories mentioned above\nsame settings from different configs are overwritten by later added:\nfirst.conf(a=1, b=2), second.conf(b=4, c=3) will be treated like this:\na=1, b=4, c =3\n\n\n\n!!! MAKE SURE to follow syntax !!!\n    'upstream' directive can not be added before http{} section \n    or inside http{} section\n\n\nSo all includes need to be made correctly\nalso not all directives could be added into configs that are included\nbecause it could lead to conflicts and Nginx wont start\n\nIF nginx is not starting check:\njournalctl -xe - for service errors\n\nIF nginx throws errors or behave in not expected way check error log\nits location in nginx.conf file\ndefault location: /var/log/nginx/error.log\n\nNginx could refuse connection:\n(13: Permission denied)\n   this related to SELinux, which also has error log:\nsudo cat /var/log/audit/audit.log | grep nginx | grep denied\n SELinux has bool values that Nginx and Apache and probably others use\n   getsebool -a | grep httpd - will return all bools related to httpd \nand Nginx(it uses them too)\n To allow connections set:\nsetsebool httpd_can_network_connect on - or use 1\n-P - will set it permanently\n\n\nConfig setup, for proxy server:\nOption 1:\ncreate config with following stuff:\n\n\n\npool for balancing? should be after http{} section\n\n\nupstream containerapp {\n        server localhost:8081; #- not sure whether localhost is fine\n        server localhost:8082;\n}\n\n\nserver configuration\n\n\nserver {\n        listen *:80; #everything coming to port 80\n\n\n    server_name localhost;\n    index index.html index.htm index.php #match any such file and use it\n\n    access_log /var/log/nginx/localweb.log; #save logs here\n    error_log /var/log/nginx/localerr.log;\n\n#location is root , for some reason\n    location / {\n    #pass traffic coming through port 80 into the pool\n            proxy_pass http://containerapp; \n    }\n\n\n\n}\n\n\n Option 2:\nAdd upstream (with local network IPs) into nginx.config after http{}\nsection\nUpdate server{} like above, but w/o *_log fields\nServer_name also use local network IP\nALso i commented default 'root' not sure if this necessary\n\n\n\n===Networking\n\n\nnetworking relies on configuration files:\n/etc/sysconfig/network-scripts/ifcfg-\n - this file describes \n    how network interface works, every interface has its own file.\n    Settings of the interface are stored in here\n     Example:\n    DEVICE=\"eth0\"\n    BOOTPROTO=\"dhcp\"\n    HWADDR=\"00:0C:29:3B:44:D3\"\n    IPV6INIT=\"yes\"\n    NM_CONTROLLED=\"yes\"\n    ONBOOT=\"yes\"\n    TYPE=\"Ethernet\"\n    UUID=\"951b47f7-e9c5-4e64-85ce-44c79732d914\"\n/etc/sysconfig/network - general network configuration, probably\n     Example:\n    NETWORKING=yes\n    HOSTNAME=centos7\n    GATEWAY=10.0.0.1\n/etc/resolv.conf - something like 'hosts' in Windows, it seems, store default DNS here\n     Example:\n    nameserver 8.8.8.8   - default google's DNS\nsee this for details on resolving Networking issue on VMWare:\nhttps://superuser.com/questions/901672/centos-7-ping-8-8-8-8-connect-network-is-unreachable/901677\n\n\ntcptraceroute:\n  traceroute to port \n   Example:\n  sudo tcptraceroute teamcity.tideworks.com 443\n\n\nip: \n  link - creates network interface with name and type:\n    also could be used if 'ifconfig' is absent\n    Example:\n     ip link add br10 type bridge - will create network interface with \n    name br10 and of type bridge\n   add - add interface\n   set - sets interface, somehow\n    ip link set br10 up - will enable network interface br10\n\n\naddr - manages ip addresses. \n    in case 'ifconfig' is not available this comes handy\n   add - adds gateway ip and subnet mask for device with a name\n    Example:\n     addr add 10.10.100.1/24 dev br10 - adds gateway with addres ...100.1 \n    and with subnet mask 255.255.255.0 to the device called br10\n    which is a network interface\n\n\nAdd persistant network interface:\n    /etc/network/interfaces\n    auto \n\n    iface \n inet static\n        address 10.10.100.1 #main address\n        netmask 255.255.255.0   #class C, like 10.10.100.1/24\n        bridge_ports dummy0 #some mock stuff\n        bridge_stp off\n        bridge fd 0         #frame 0\n====Ports checks\nread more:\nhttps://www.cyberciti.biz/faq/how-do-i-find-out-what-ports-are-listeningopen-on-my-linuxfreebsd-server/\nhttps://www.maketecheasier.com/check-open-ports-linux/\n\n\nnetstat:\n  find open ports\n  Find port on machine:  https://www.tecmint.com/find-open-ports-in-linux/\n   Params:\n  -l / --listen - displays ports that are in LISTEN state (lots of)\n  -vatn - opened ports and established TCP connections\n   - v - ?\n   - a - All - show all sockets\n   - t - TCP connections\n   - u - UDP connections\n   - n - short DNS hostname. will display dns names/ports w/o it\n   - p - show related PID and Program Name\n  Example:\n   netstat -vat - display used porta and established connections\n   sudo netstat -tulw - will display all ports for local machine\n  Check ports 9211, 8685 opened on local machine for tcp\\udp for IPv4 and IPv6:\n   netstat -lntu | egrep \"9211|8685\"\n\n\nss:\n  netstat could be outdated on some systems, use it instead\n   Example:\n  sudo ss -tulwn - display all opened ports on local machine\n\n\nlsof:\n  also checks ports, could display processes that are using\n   port, and files used by the process\n\n\nParams:\n  -i - displays processes and ports used, and connection\n    with friendly names\n   - 4 - ipv4\n   - tcp - display tcp (cant use 4 and tcp in the same time, it seems)\n\n\ntelnet:\n  check ports open\n  telnet \n \n\n   Example:\n  telnet lwpeartifabld.tideworks.com 5000\n\n\nnmap:\n  Network Mapped, detects open ports on my system\n   -s - Scan, there are lots of it\n    -sT - scan TCP\n    -sn - disable port scan, just 'ping'\n    -stuff, see 'man nmap'\n   -O - OS detection\n   -T - TCP\n   -U - UDP\n    Example:\n   sudo nmap -sT -O 192.168.42.74  - scan remote machine for opened ports\n   sudo nmap -sT -O localhost - scan local machine\n    Example:\n   nmap artifactory-dev.tideworks.com -p 5000\n     check port 5000 opened on 'artifactory-dev.tideworks.com' machine\n\n\nNOTE: \n   Read MAN - it has examples and lots of stuff:\n    scanme.nmap.org/24 - will scan 256 IP in C network class of\n    scanme network - CRAZY!!111\n\n\nExample:\n   nmap -sT -O localhost    - [s]cans [T]CP on localhost\n\n\nnslookup:\n  looks for domain names by given IP\n  uses /etc/resolv.conf  - where all the DNS servers are\n   resolv.conf:\n  domain twlab.int\n  search customer.int usa.int tideworks.com\n  nameserver 172.19.65.2\n  nameserver 172.19.65.1\n  nameserver 10.253.1.1\n  nameserver 10.253.1.3\n\n\n==Security\nLSM: linux security module:\nhttps://www.kernel.org/doc/html/latest/admin-guide/LSM/index.html\n\n\nPrimary user of LSM is Mandatory Access Control(MAC) extensions\n\n\nLSM loads a bit earlier than UserSpace, and controls user\niteraction with system through policies.\nLSMs exist to apply policies to actions taken by user space, so \nas long as the LSM infrastructure is running by the time user \nspace starts, everything is fine.\nLockdown, though, must act earlier: it needs to be able to \nblock the action of certain types of command-line parameters and \nmust be functional even before a security policy can be loaded. \nSo the patch set starts by creating a new type of \"early security \nmodule\" that is initialized toward the beginning of the boot process. \nAt this point, the module can't do much \u2014 even basic amenities like \nkmalloc() are not available \u2014 but it's enough to register its \nhooks and take control.\n\n\nLoaded modules - MAC Extensions:\n cat /sys/kernel/security/lsm\n    \n\n   capability - loaded always\n   then 'minor' modules\n    yama\n   last 'major' module, ONE:\n    SELinux - OEL(RHEL?) OEL extension , might be custom\n    AppArmor - Ubuntu(Debian?) MAC Extension default\n     https://www.kernel.org/doc/html/latest/admin-guide/LSM/apparmor.html\n    Smack\n    Tomoyo\n  Process attributes for MAJOR module location:\n   /proc/.../attr\n    Subfolder with Module Name could be there\n    This is LEGACY for modules that provide subdirectories\n\n\nAppArmor: apparmor:\ncentered around Tasks and Profiles for tasks\nProfiles are attached to Tasks\nTasks w/o profile controlled only be regular permissions wrx (DAC)\n\n\nIf AppArmor is not the default security module it can be enabled by passing \nsecurity=apparmor on the kernel\u2019s command line.\nIf AppArmor is the default security module it can be disabled by passing \napparmor=0, security=XXXX (where XXXX is valid security module), on the \nkernel\u2019s command line.\nFor AppArmor to enforce any restrictions beyond standard Linux DAC \npermissions policy must be loaded into the kernel from user space \n(see the Documentation and tools links).\n\n\nEnable\\disable:\n  set CONFIG_SECURITY_APPARMOR=y\n\n\nSet default:\n  CONFIG_DEFAULT_SECURITY=\"apparmor\"\n  CONFIG_SECURITY_APPARMOR_BOOTPARAM_VALUE=1\n\n\nWiki - http://wiki.apparmor.net\nUser space tools - https://gitlab.com/apparmor\n\n\nLockdown:\n  https://lwn.net/Articles/791863/\n   Together with UEFI locks who can access kernel\n   only Private Key owner with pair in UEFI\n   or certificate?\n\n\nLockdown is loaded much earlier than LSM and User Space, to prevent\ntricky cmd params from doing anything\n  Lockdown adds hooks at early-boot, throughh which operates \n\n\nThere are two lockdown modes. Confidentiality mode prevents user-land processes from extracting confidential information from the kernel. The other mode, Integrity, allows the kernel to switch off features that would allow user-land processes to modify the running kernel. Both of these modes even prevent processes launched by the root user or anyone with sudo privileges from modifying the kernel.\n\n\nSELinux: selinux:\n\n\nStatuses:\n1. Enforced : Actions contrary to the policy are blocked and a corresponding event is logged in the audit log.\n2. Permissive : Permissive mode loads the SELinux software, but doesn\u2019t enforce the rules, only logging is performed.\n3. Disabled : The SELinux is disabled entirely.\n\n\nRead about status change and check:\nhttps://www.thegeekdiary.com/how-to-check-whether-selinux-is-enabled-or-disabled/\n\n\ngetenforce:\n  check status of selinux\n   Example:\n  # getenforce\n  Permissive\n\n\nsetenforce:\n  change enforce\n   Example:\n  # setenforce 0\n  # getenforce\n  Permissive\n\n\nsetstatus:\n  more detailed current status output\n\n\nchange to permissive and configuration info:\nhttps://www.thegeekdiary.com/how-to-disable-or-set-selinux-to-permissive-mode/\n\n\ncat /etc/selinux/config\nSELINUX=disabled  # to disable\n\n\ndot in permisisons:\ndrwxr-xr-x.    < this dot(.) means file has Access Control List(ACL) with SELinux.\nthis will not appear if SELinux is disabled\n\n\nsetfacl:\n will or unset ACL for files\\directories etc.  (the dot at the end of permisisons)\nhttps://stackoverflow.com/questions/30594871/what-does-the-dot-at-the-end-of-the-permissions-in-the-output-of-ls-lah-mean\n\n\n==Virtualization general\nVirtual box has several different network interfaces\n\n\nNAT and NATNet networks would not be accessible from outside(and HOST)\nso to SSH into virtual machine, gotta use Port Forwarding \n  and connecto to localhost:\n\ntable:\n+-----------+-------------+-------------+----------------+----------------+\n|           | VM <-> Host | VM1 <-> VM2 | VM -> Internet | VM <- Internet |\n+-----------+-------------+-------------+----------------+----------------+\n| HostOnly  |     Yes     |     Yes     |      No        |       No       |\n| Internal  |     No      |     Yes     |      No        |       No       |\n| Bridged   |     Yes     |     Yes     |      Yes       |       Yes      |\n| NAT       |     No      |     No      |      Yes       |  Port forward  |\n| NATNet    |     No      |     Yes     |      Yes       |  Port forward  |\n+-----------+-------------+-------------+----------------+----------------+\nBridged will allow your guest to appear as just another PC on your host's network. The host, all the other network PCs, the internet and the guest can all communicate. The guest would default to getting an IP address from your host network's router. The host needs to be connected to an active network to allow guests to use Bridged. And Bridged is not always compatible with Wi-Fi.\n\n\nInternal makes a private network just for your guests. There is no host communication or internet. There is no DCHP server on an internal network by default, though you can put one on in Virtualbox. Otherwise configure static IP address for the guests within the guest OS's. Or you can make a router guest using a router OS like pfSense. You can make more than one internal network by naming the new networks differently.\n\n\nHost-Only is a special form of internal network that also includes the host, via a virtual Host-Only network adapter made on your host, defaulting to IP address 192.168.56.1, and there is a default DHCP server handing out IP addresses to the guests starting at 192.168.56.101. You can change the IP address range or make multiple host-only networks in Virtualbox's Network settings in the main GUI. There is no internet in Host-Only.\n\n\nVirtualbox has two kinds of NAT: regular NAT and a NAT network service. Regular NAT (\"NAT\" in the dropdown) connects each guest to the host's network connection and internet via an independent channel. The host and any other guests cannot connect to the NAT-connected guest, although ports can be opened, just like NAT in a router. (The NAT guest can, however, see any services and shared folders on the host's network.)\n\n\nThe NAT network service (\"NAT network\" in the dropdown) is like a home router, NAT to the outside world and multiple \"LAN\" guest connections. All the guests connected to this \"NAT network\" can see and communicate with each other. Internet is accessible. Ports can be opened just like a real router. You need to make a new NAT network in the main Virtualbox window, File Menu, Preferences, Network. Then attach the guests using the name of the NAT network you made in Preferences.\n\n\n===GUI\nGUI on Centos7\nhttps://unix.stackexchange.com/questions/181503/how-to-install-desktop-environments-on-centos-7\n\n\nGNOME install on Centos7\nyum group list\n  check groups available\n \"Server with GUI\" - this for RHEL\n \"GNOME Desktop\" this is GNOME for Centos\n \"KDE Plasma Workspaces\"  - this is KDE\n \"Graphical Administration Tools\" - extra admin tools for GUI\nyum groupinstall \"GNOME Desktop\" \"Graphical Administration Tools\"\n  this will install two groups for Gnome and admin tools\n  Read more:\n  https://www.itzgeek.com/how-tos/linux/centos-how-tos/install-gnome-gui-on-centos-7-rhel-7.html\n\n\ngnome:\ngnome-control-center:\n  calls GUI of main settings window\n  gnome version could be viewed somewhere here\n  also version:\n  /usr/share/gnome/gnome-version.xml\n\n\ngnome-tweaks:\n  calls GUI of advanced? settings window\n\n\nutilities:\nclamav:\nclamscan:\n      ClamAV \u2013 A command line utility: It is a free, open source, and cross-platform antivirus toolkit for detecting many types of malicious software and viruses.\n    ClamTK \u2013 A Graphical utility: ClamTk is a graphical front-end for the Clam Antivirus. It is designed to be an easy-to-use, lightweight, on-demand antivirus scanner for Linux systems.\n   Install:\n  sudo apt-get install clamav clamav-daemon\n   Execute scan:\n   Example:\n  clamscan --invected --log=clamscan.log --recursive --copy=infected_files --detect-pua=yes /",
            "title": "Linux cheatsheet"
        },
        {
            "location": "/linux_cheatsheet/#mount-find-usb-disk-hdd-drive",
            "text": "format usb disk:\nhttps://www.wikihow.com/Format-a-USB-Flash-Drive-in-Ubuntu\nhttps://www.cyberciti.biz/faq/linux-disk-format/    locate the disk and found its name: \n  tail -f /var/log/syslog     look for something like this: \nMay 26 15:57:00 dospc kernel: [ 9385.461071]  sdc: sdc1 sdc2",
            "title": "mount find usb disk hdd drive:"
        },
        {
            "location": "/linux_cheatsheet/#lsblk",
            "text": "unmount everything that auto-mounted from that drive   sudo umount /dev/sdb1   (OPTIONALLY)  override everything , will take HOURS:   sudo dd if=/dev/zero of=/dev/sdc bs=4k status=progress && sync  3.1 Faster option, which will override MBR and partition table:  dd if=/dev/urandom of=/dev/sdc bs=1M count=2   fdisk /dev/sdc   option: o  will create empty partition table  option: n  will create new partition, then use Defaults  option: w  will write changes, could take some time. wait for it   lsblk   will return all block devices with updated sdc device, check size   format into FAT32   sudo mkfs.vfat /dev/sdc1   eject disk when done    sudo eject /dev/sdc   how to get block device name, partition, mount it and unmount after:\n  tail -f /var/log/messages     - open messages and follow. deprecated\n  tail -f /var/log/syslog - this is more standard log file.\n  #mount disk, recheck messages for something like:\n  \"sdb: sdb1\"\n  \"sd 3:0:0:0 [sdb] attached SCI removable disk\"\n  fdisk /dev/sdb\n   p                - see all partitions\n  mkdir ~/mounted_stuff\n  mount /dev/sdb1 ~/mounted_stuff\n  umount /dev/sdb  mount:\n  mount drive (disk, usb, floppy etc)\n  mount -o rw,remount /\n   -o - dunno.. TODO\n   rw - seems to be read\\write\n   remount - pretty clear at first glance\n   / - what to remount  Example:  lsblk -f     - view all devices\n   mount /dev/sda5 /path/to/mount/point - mount device\n   umount /dev/sda5   - unmount (Notice N letter absent in commnad name)\n    Example:\n    mount ISO image\n   mount -t iso9660 -o loop image.iso /path/to/mountpoint  -a - mount all lines added to /etc/fstab  /etc/fstab - config for filesystems\n   read by fsck, mount, unmount\n  lines from this file read during 'mount -a'.\n   check man fstab for details. \n  simply add automount of HDD from old windows:\n  LABEL=Juli      /otherHDD/Juli  ntfs rw,suid,exec,auto,user,async\n  labels or UUID could be taken from 'blkid' command  ntfs-config:\n  Enable/disable write support for any NTFS devices  fdisk:\n  manipulates disk partitions tables\n   Usage:\n  fdisk  \n   Commands:\n  m - manual/help\n  p - show all partitions\n  l - list all known filesystems (use one in mkfs )\n   Example:\n  fdisk /dev/sda   m   - see what could do\np   - see all partitions of the device    mkfs:\n  create new filesystem\n   Params:\n  -t fylesystem type\n   Example:\n  mkfs -t ext3 /dev/sdb1  lvm:\n Logical Volume Manager\n  How to install:\n https://help.ubuntu.com/community/UbuntuDesktopLVM\n  Ubuntu docs:\n https://wiki.ubuntu.com/Lvm\n  How to extend Logical volume:\n https://www.youtube.com/watch?v=hugEkh50Ynk\n  How to extend logical volume in text \n https://www.howtogeek.com/howto/40702/how-to-manage-and-use-lvm-logical-volume-management-in-ubuntu/  fsck:\n  filesystem check and repair (lost+found dir will have repaired files)\n   Example:\n  fsck /dev/sdb1  genisoimage:\n  creates ISO images\n   Parameters:\n  -o   - output to\n  -R - for long file names and access rights for files in POSIX style\n  -J - similar long names but for Windows\n   Example:\n  genisoimage -o cd-rom.iso -R -J ~/cd-rom-files\n   where cd-rom-files is a directory with files to be added into ISO image  scp:\n  secure copy using ssh protocol\n   could copy to or copy from remote location.\n  copy to remote:\n  scp      @ : \n  copy from remote:\n  scp    @ :   \n  params:\n  -v - verbose    sshd(openssh)\n  daemon config location:\n   /etc/ssh/sshd_config\n  sftp enable switch is located there, could be commented out and sshd restart\n  to disable it\n   /etc/ssh/ssh_config\n  some another config\n  ==\n   Options important:\n  PermitRootLogin:\n    \u201cyes\u201d, \u201cprohibit-password\u201d, \u201cwithout-password\u201d, \u201cforced-commands-only\u201d, or \u201cno\u201d\n  ==\n  RSA keys for ssh authentication generation process:\n  https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-on-centos7\n  Passwordless:\n  to add security ssh login with Password could be disabled, to login\n  only with keypair:\n  https://linuxize.com/post/how-to-setup-passwordless-ssh-login/#disabling-ssh-password-authentication\n  https://help.ubuntu.com/community/SSH/OpenSSH/Keys\n   Log files of sshd:\n  RHEL:\n   /var/log/secure\n    Example:\n   tail -f -n 500 /var/log/secure | grep 'sshd'\n    If getting errors like:\n   'ROOT LOGIN REFUSED FROM 10.253.0.28 port 34318'\n    the Root login is disabled\n   PermitRootLogin yes\n  Other:\n    /var/log/auth.log\n   tail -f -n 500 /var/log/auth.log | grep 'sshd'  ssh:\n  remote connect via ssh protocol  -v - display debug info level1\n  -vv  - debug level2\n  -vvv - degut level3\n  Troubleshooting:\n   if -vvv  shows everything ok:\n     debug1: Server accepts key: pkalg ssh-rsa blen 535\n     debug2: input_userauth_pk_ok: SHA1 fp 28:bc:1b:64:94:3a:6b:ec:b4:13:45:ca:79:80:be:7c:75:cc:82:48\n     debug3: sign_and_send_pubkey: RSA 28:bc:1b:64:94:3a:6b:ec:b4:13:45:ca:79:80:be:7c:75:cc:82:48\n     debug1: read PEM private key done: type RSA\n     debug3: Wrote 1156 bytes for a total of 3101\n   Open Log files of sshd:\n    tail -f -n 500 /var/log/secure | grep 'sshd'\n   if errors like this appear:\n    ROOT LOGIN REFUSED FROM 10.253.0.28 port 34318\n   open  /etc/ssh/ssh_config and set\n  PermitRootLogin yes  ssh-keygen:\n  generates keypair private\\public\n    Example:\n   ssh-keygen -t rsa -b 4096 -C \"email\" -f my_key\n    Example PUBLIC from Private:\n   ssh-keygen -y -f private_key > private_key.pub\n  -t rsa - key type\n  -b 4096 - generates key pair into given path\n  -C \"text\" - comment for key\n  -f filename of the key, instead of default 'id_rsa'\n   Example:\n  ssh-keygen -t rsa -b 4096 -C \"email\" -f my_key\n      - will create new keypair, w/o -f it will generate\n    in the path ~/.ssh/id_rsa and its id_rsa.pub part\n  -R - remove from ~/.ssh/known_hosts\n    Example:\n   There are only 1 RSA key is allowed for the IP, in case RSA key is changed, then \n   old one need to be removed, otherwise it will not allow ssh acces:\n    ssh-keygen -f \"/home/dos/.ssh/known_hosts\" -R 172.17.0.2  -l - list, not create anything.\n  -E - Specifies the hash algorithm used when displaying key fingerprints. \n       Valid options are: \u201cmd5\u201d and \u201csha256\u201d.\n    Example:\n Get fingerprint:\n  ssh-keygen -lf ~/.ssh/keyname[.pub]\n GitHub: Get fingerprint in hashed\\hex format:\n  ssh-keygen -E md5 -lf ~/.ssh/git_key[.pub]  ssh-copy-id:\n  copies public keys securely to remote machine for paswordless login\n  keys are concatenated into ~/.ssh/authorized_keys file\n  it will copy all public keys available in ~/.ssh/\n   Example:\n  ssh-copy-id dmitry2@192.168.42.74\n   if there issues with a default 22 port , another port could be changed:\n  ssh-copy-id \" @  -p  \"  if ssh-copy-id is not available here is analoug:\n   cat ~/.ssh/id_rsa.pub | ssh remote_username@server_ip_address \"mkdir -p ~/.ssh && chmod 700 ~/.ssh && cat >> ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys\"  ssh-keygen:\n  Known hosts are stored in a file /home/ /.ssh/knonwn_hosts.\n    Generate public key from private key:\n  ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub  sftp:\n  secure file transfer protocol, allows to list dir contents\n  built-in into OpenSSH server, which workks as sshd service(daemon) together\n  with scp\n  sftp  @ \n sftp has its own set of directory related commands:\n pwd - print work dir, returns current remote_dir\n lpwd - current local_dir\n cd - change dir, changes remote_dir\n lcd - local change dir, changes local_dir from sftp via OpenSSH daemon\n get - copy file to current local_dir\n  get    \n ls also works, ALIASES from bashrc do not work  shutdown:\n shutdown or reboot or halt etc the computer\n  by default shutdowns in 1 minute, also could be scheduled:\n   Example:\n  shutdown \"21:45\" - will shutdown at 21:45\n  shutdown \"+25\" - will shutdown in 25 minutes from 'now'\n  shutdown - will shutdown as 'shutdown +1'\n  shutdown now - shutdown now  at:\n need to be installed, daemon\n schedules to do something  at  some time:\n There are three ways to use it:\n  Example 1:\n echo \"shutdown\" | at now + 1 min - will execute what echo will return\n    at time of now + 1 minute - echo will return stuff in ''\n  Example 2:\n echo \"shutdown\" > cmd.txt\n at now + 1 min < cmd.txt - it will redirect STDIN from cmd.txt , will read\n    from there, and shutdown \n  Example 3: \n at now + 1 min\n at> shutdiwn     - will wait for input in STDIN\n ctrl+d           - to exit, and wait given time before execute shutdown  Time could be like:\n [[CC]YY]MMDDhhmm[.ss]\n  Example: \n at -t 201403142134.12 < script.sh  crontab:\n  list cron schedules\n Exacmple:\n  crontab -u   -l\n    will list all rules for a user\n    need sudo  cron:\n task scheduler           command to be executed                -\n |     |     |     |     |\n |     |     |     |     +----- day of week (0 - 6) (Sunday=0)\n |     |     |     +------- month (1 - 12)\n |     |     +--------- day of month (1 - 31)\n |     +----------- hour (0 - 23)\n +------------- min (0 - 59)         So, for example, this will run ls every day at 14:04:  04 14 * * * ls  To set up a cronjob for a specific date:   Create a new crontab by running crontab -e. This will bring up a window of \nyour favorite text editor.\n\n Add this line to the file that just opened. This particular example will \nrun at 14:34 on the 15th of March 2014 if that day is a Friday (so, OK,\nit might run more than once):\n\n 34 14 15 5  /path/to/command\n\n Save the file and exit the editor.  awk:\n stuff to worh with text, programming language kind of\n  Example: \n awk 'BEGIN {printf \"title\"} {print \"line\"} END {printf \"end\"}' textfile.txt\n  this will print 'title'\n  then 'line' for every line in textfile\n  and print 'end' at the end  check out tutorial here:\n https://www.tutorialspoint.com/awk/awk_basic_syntax.htm  workflow is Read, Execute, Repeat with tear up and tear down executed once:\n BEGIN\n  Read line\n  Execute user code\n  Repeat if not EOF\n END  Begin block executes only once, Tear Up, syntax:\n  BEGIN {awk-commands}\n Optional  Body block, which is actual stuff to do for every line matched by pattern:\n  /pattern/ {awk-commands}\n Pattern is optional  End block executes only once, Tear Down, syntax:\n  END {awk-commands}\n Optional  Also gets input from a file:\n  -f  \n    Example:\n  command.awk:\n   BEGIN {printf \"start\\n\"}\n   {print}\n   END {printf \"end\\n\"}  awk -f command.awk textfile.txt   - this will read from command.awk and print\n   'start' at 1st line\n   then print every line\n   then print 'end' and the latest line  ===general  ===Env Vars / Environment variables:\ndeclare:\n  built-in command to assign or display variables and functions\n   Syntax:\n  declare [-aAfFgilnrtux] [-p] [name[=value] ...]\n  Params:\n  Using  +' instead of -' turns off the given attribute.\n   Example:\n  declare - will show all variables and functions\n  declare | grep \"^a=\" - will return variable 'a' with its value   -p - prints only variables w/o functions (displayhs onlyh NAME)\n  -a - to make NAMEs indexed arrays(only Bash, not Sh)\n   Example:  -i - to make NAMEs integer\n   Example:\n   declare -i z=asdf    < echo $z - 0\n   declare -i z=12  < echo $z - 12\n  -l - to convert NAMEs to lower case on assignment\n   Example:\n   declare -l b=TEST\n   echo $b      >test  - will save it in loweracase\n  -u - same but for UPPRECASE\n  -g - create global variables in functions:\n  When used in a function,  declare' makes NAMEs local, as with the local'\n    command.  The `-g' option suppresses this behavior.  -x - export variables\n   Example:\n   declare -x FOO  - same as - export FOO\n  -r - readonly variables\n  -n - make NAME a reference to the variable named by its value\n   Example: pointer-to-pointer\n   b=foo        # variable\n   declare -n b2=b  # pointer to the variable\n   echo $b2   foo\n   declare -t b3=b2 # pointer to pointer\n   echo $b3\nfoo   printenv:\nenv:\n  very similar, prints environment variables to STDOUT\n  printenv VAR - prints value of specified variable to STDOUT  Useful Env variables:\n  EDITOR - default text editor\n  SHELL - name of the shell\n  HOME - home path\n  LANG - chars and sort order for language\n  OLD_PWD - prev work dir\n  PWD - current working dir\n  PAGER - program for page view ( i.e. /usr/bin/less)\n  PS1 - current prompt string value\n    Example:   dos:~$: / root:~#:\n    Prompt string could be drastically changed\n    even including redraw of clocks in the cmd on every command entered\n        Example: [\\033[s\\033[0;0H\\033[0;41m\\033[K\\033[1;33m\\t\\033[0m\\033[u]<>$\n    Example:\n   export PS1='$(whoami)@$(hostname):$(pwd) '$\n  TERM - type of the terminal\n  TZ - time zone, usually *nix has Coordinated Universal Time , and it\n    is corrected with current time zone upon display\n  USER - username\n  HISTCONTROL - with =ignoredups - will ignore duplicates in bash history\n  HISTSIZE - changes default 500 value of history line-length  set:\n  prints ALL variables - Shell vars, local vars, shell functions\n  Builtin so use 'help set'  Read more of set and printenv\\env here:\nhttps://www.digitalocean.com/community/tutorials/how-to-read-and-set-environmental-and-shell-variables-on-a-linux-vps\n Read more of printenv\\env history here:\nhttps://unix.stackexchange.com/questions/123473/what-is-the-difference-between-env-and-printenv  How to set\\export env variables in bash:\n  export - share variable for child processes started from current shell \n   Example:\n  TEST=1 - will create var for current shell, will not be visible in processes\n    started from the shell\n  export TEST=1 - will create var for cur shell, and share it to processes\n    started from the shell\n  echo 'export TEST=1' >> ~/.bashrc - will create var at every non-login start \n    of the shell, if .profile reads .bashrc - login shell also get it\n     Usually for vars there are different file, which bashrc will read\n    see/update local .bashrc to do so.\n     Just exported into bashrc vars will not be visible until shell restart\n    or source load:\n     source ~/.bashrc - will add newly exported vars into current session  Read export\\env differences here:\n http://hackjutsu.com/2016/08/04/Difference%20between%20set,%20export%20and%20env%20in%20bash/  Set variable ONLY for current shell:\n  varname=\"my value\"\n  Set variable for current shell and ALL PROCESSES started from current shell:\n  export varname=\"my value\"\n  For below LogOut is required:\n  Set var PERMANENTLY for current user\n  ~/.bashrc - add 'export varname=\"my val\"' here\n  Set var permanently and SYSTEM WIDE (all users/all processes)\n  /etc/environment - add line 'VARNAME=\"my value\"', caps bcs of naming convents\n             'export' is not userd here parser does not know it\n  LogOut is required for permanent changes.  Unset:\n  unset ENV_VAR\n  echo $ENV_VAR - returns nothing , bcs it is unset  ALSO has options:\n  set -o - will display all the options\n   one of the options is 'noclobber'\n   set -o noclobber - prevents '>' redirection from overriding files\n   set +o noclobber - removes noclobber back  execute:\nsource:\ndot (.):\n  read more:\n  https://superuser.com/questions/176783/what-is-the-difference-between-executing-a-bash-script-vs-sourcing-it/176788#176788  execute script if it has x permission\n  !! CREATES NEW SHELL !!\n   ./script - script in current directory(./ dots lash notaion)\n   script - execute script if it is in PATH var  source script even if it does not have x permission\n  documentation:\n  http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#dot\n  !! DOES NOT CREATE A NEW SHELL !!\n  Executes commands from a file in the CURRENT environment. \n  In case it is for login-shell, it will update whole command line, probably\n   not sure how it will work in case of 'su -'  source myscript - myscript is param for source so no matter myscript is in\n             path or not.\n             Still need to be valid shell script(#!/interpreter/pat) \n   . myscript - 'source' is alias for . in bash, syntax from POSIX   Source saves stuff changed in script(i.e. Env var updates) to the current \n  shell session where as Execute creates new session where changes are saved\n  and kills it after everything is done - basic behaviour of everything  ===Login process\nRead More:\nhttp://mywiki.wooledge.org/DotFiles  No GUI login into text console. Local login shell:\n1. getty(8) starts and produces 'login:' prompt\n2. getty reads login and passes it to login(1)\n3. login reads password db and decides whether to ask for password\n4. after password provided PAM could be loaded - /etc/pam.d/login\n   this could load additional settings for environment or session etc.\n5. login 'execs' login shell(probably from /etc/passwd) with -\n   -/bin/bash - means \"this is login shell, not a regular shell\"\n6. since it's login shell it reads /etc/profile and /etc/profile.d\n   /etc/profile should have code to read profile.d\n   then ~/.bash_profile, \n   then if no - .bash_login, \n   then if no - .profile\n     .profile is standard Bourne/POSIX/Korn shell config file\n7. bash stops look for config files and displays prompt  Example of login process:\nLet's take a moment to review. A system administrator has set up a Debian system (which is Linux-based and uses PAM) and has a locale setting of LANG=en_US in /etc/environment. A local user named pierre prefers to use the fr_CA locale instead, so he puts export LANG=fr_CA in his .bash_profile. He also puts source ~/.bashrc in that same file, and then puts set +o histexpand in his .bashrc. Now he logs in to the Debian system by sitting at the console. login(1) (via PAM) reads /etc/environment and puts LANG=en_US in the environment. Then login \"execs\" bash, which reads /etc/profile and .bash_profile. The export command in .bash_profile causes the environment variable LANG to be changed from en_US to fr_CA. Finally, the source command causes bash to read .bashrc, so that the set +o histexpand command is executed for this shell. After all this, pierre gets his prompt and is ready to type commands interactively.   .bashrc is not read for login shells, it could to be read manually in .profile\nIn case this is .profile, before read it, need to ensure that Bash is used:\n  if  [ -n $BASH ] && [ -r ~/.bashrc ]; then\n    . ~/.bashrc\n  fi  Remote Login Shell:\nalsmost same as previous, but except 'getty' and 'login' - 'sshd' handles\ngreetings(login prompt by 'getty') and password auth('login').\nPAM also linked with 'sshd', but will read /etc/pam.d/ssh (not .../login).\n Once sshd has run through the PAM steps (if applicable to your system), it \n\"execs\" bash as a login shell, which causes it to read /etc/profile and then \none of .bash_profile or .bash_login or .profile.  NOTE:\nduring ssh login, some of local Env Vars could be sent to remote sshd\nlike LANG and LC_* variables want to be preserved by the remote login.\nUnfortunately, the configuration files on the server may override them.  ===User\\Groups Management  =====User\\Groups Management General info\ngroup related info is stored in\n  /etc/group   - file, use 'vigr' for edit\nuser related info stored in \n  /etc/passwd  - file. in case of edit use 'vipw' like 'visudo' with sudoers\ndefault homedir files are stored in\n  /etc/skel/   - directory\n         those files will be copied to user home when it is created\n  ..../.bashrc - executed by bash for non-login shells - every time bash\n         is started interactively(from command line, which is also bash)\nsee more: https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work\n  ..../.profile - executed by command interpreter for login shells\n          is not read if '.bash_login' or '.bash_profile' exists\n        Usually also reads /etc/bash.bashrc\n  ~/.profile - for login-shells also reads ~/.bashrc\n  ..../.bash_logout - executed by bash when login shell ended  unlike files in /etc/skel changes to this files will affect even existing users\nchanges from here will be applied with every re-login to every \n        user on thesystem:\n/etc/bash.bashrc -  will affect ~/.bashrc \n/etc/profile -  will affect ~/.profile  /etc/login.defs - useradd/userdel/usermod config\n/etc/adduser.conf - adduser config(Ubuntu)\n/etc/shadow - secure account info - passwords\n/etc/gshadow - secure group account info - passwords  /etc/passwd:\nroot:x:0:0:root:/root:/bin/bash\nroot - username\nx - placeholder for a password, modern systems use /etc/shadow file for pass\n0 - is the user ID(UID) for this user\n0 - is the group ID(GID) for this user\nroot - comment about this user\n/root - home directory for this user\n/bin/bash - default shell for this user(after he logs in presumably)\n        Possible values:\n  /bin/nologin - deny login at all\n  /bin/false - deny login but still can be logged in using 'su' command from \n        another account  /etc/group:\nwheel:x:10:centos,user\nwheel - group name\nx - place holder for a password, modern systems use /etc/shadow filr for pass\n10 - group id(GID) for the group\ncentos,user - users in the group, and probably user wheel too\n   user along with who group was created id not listed in the group..\n   probably to make sure group do or does not contains it , could try check\n   groups of that user like 'groups wheel' - it will return 'no such user' if\n   there is no user with same name as a group  NOTE: there are Primary group for a user and SUpplementary groups, \n  Primary is the first group of a user, others are suppplementary:\n    $ groups johnny\n    > johnny : test1 john newgroup1 newgroup2\n  to add extra supplementary groups use 'usermod -a -G group1,group2'. \n  To change Primary group use  'usermod -g john johnny'\n    > johnny : john newgroup1 newgroup2\n  NOTE THAT 'test1' group has been removed at all, and not moved to \n   supplementary groups\n  NOTE to change to test1 w/o deleting it user 'newgrp' command:\n    $ newgrp newgroup1\n    > newgroup1 john newgroup2\n  Files created by user are owned by PRIMARY group of the user  =====user\\groups management general info end  =====Manipulate Users And Groups\nuseradd:\n  old since any  nix creation. Almost everything need to be done manually bcs of\n  compatibility - different  nix platforms handles users differently, their ~\n  directories could be different\n  useradd -   \n  -d <~_path> - use if home name\\path differs from default location, i.e.:  useradd -d /home/accounts/john testuser  useradd -c \"John from Accounts\" -m -s /bin/bash john\n   -c - adds a COMMENT in the /etc/passwd for this acc\n   -m - MAKEs the home directory for this user, like /home/john\n   -s - assigns the SHELL for the user\n   john - actual user name\n    user 'john' is the member of group 'john' and this is his PRIMARY group\n  also:\n   -u   - user ID , first free picked (from range in config) id no specified\n   -g   - assign to already existing group. i.e. -g accounts\n   -G   - additional group. i.e. -G employees\n   -e   - EXPIRATION date of the account\n   -k   - sKELETON directory if differs from /etc/skel\n   -p   - encrypted password for acc, or use 'passwd' command later  NOTE: after user is created the password need to be added:\npasswd testuser  =====Manual User Creation\nuser# sudo su -     - become root\nroot# vipw      - edit /etc/passwd\n add new line with user info like:\n username:x:UID:GID:comment:/home/username:/bin/bash\nroot# vigr      - edit /etc/group\n add new line with user group info like:\n usergroup:x:GID:\nroot# cd /home && mkdir username - create user's home folder\nroot# cp -rf /etc/skel/.* /home/username/. - copy everything from skel\nroot# chown -R username:usergroup username/ - change recurse ownership of dir\nroot# passwd username - create password for the user\n=====manual user creation end  id:\n  returns info of current user - id, main group id, other groups, etc.  groups:\n  list groups of a user\n  groups [username]\n    w/o params gives current user groups\n    with username - gives groups of that user  getent:\n  getent group <$(whoami)>\n  lists all groups on system and users of those groups(similar to /etc/group)  usermod:\n  modify existing users\n  usermod [ ] [groups]  \n   -l - change LOGIN of the user. i.e.:\n      usermod -l johnny john - change username john to johnny\n   -d - DIRECTORY path for New home directory of the user\n   -m - MOVE-home dir to new DIRECTORY path\n      usermod -m -d /home/johnny johnny - creates new dir for the user and moves\n    everything from old dir to new dir\n   -g - change primary GROUP\n   -G - add to other GROUPs delimited , and no spaces adds, more groups for user\n    will overwrite other supplementary groups except primary\n   -a - APPEND groups, used with -G, to append existing supplementary groups:\n   Example:\n      usermod -a -G newgroup1,newgroup2 johnny - will add two groups to the\n    end of the list of a groups user johnny have\n   -L - LOCK user\n    after lock there will be exclamation mark in /etc/shadow file before\n    user passwod:\n    johnny:!$6$DQMYnvhr$xKMYZSorH2wePlAunWDBKYWYSK8bmnyKMbr9IAuMoykPl7....\n   -U - UNlock user, and remove exclamation mark '!' before user passwd hash\n  More to read at:\n   https://www.tecmint.com/usermod-command-examples/  userdel:\n  deletes user in old way\n  userdel  \n  -r - deletes home directory too  adduser:\n  adding user\n  shell around perl written useradd command, which is hard to use\n   adduser  \n  CentOS7, does not have adduser, it links it to useradd:\n   lrwxrwxrwx. 1 root root 7 \u0433\u0440\u0443 14  2016 /usr/sbin/adduser -> useradd  /etc/adduser.conf - config for this command, has default ids, default shell\n            default home dir(/home) and so on\n    detailed how adduser works on Ubuntu:\n    https://askubuntu.com/questions/659953/what-is-ubuntus-automatic-uid-generation-behavior  groupadd:\n  adds group\n  info about groups is stored in /etc/group file, see General above for details \n   once group is created use 'usermod' to add users to it\n  group [options]  \n  -g - manually set GROUP id  chgrp: \n  change group, changes group of file or directory\n   Synopsys:\n  chgrp [param]   /path/to/file\n   Params:\n  -R - recursive\n  -c - like verbose but only when change is made\n  -v - verbose  groupmod:\n  modify existing groups\n  -g - change GROUP id\n    groupmod -g 300 manager - change group 'manager' to have new GID of 300\n  -n - change NAME of a group\n    groupmod -n managers manager - change group 'manager' to 'managers'  passwd: \n  change password for current user\n   or for given user\n  passwd [username]\n  located in /usr/bin/passwd - ubuntu\n             /bin/passwd - centos/red hat\n  has 'setuid' thing..  gpasswd:\n  add\\remove users from a group, set admins for the group, set password for\n  a group\n  gpasswd    \n  -a    - ADD user to a group\n  -M   - add MULTIPLE users, commadelimited w/o spaces\n     gpasswd -M john,jane manager\n  -d   - DELETE user from a group\n     sudo gpasswd -d johnny newgroup1 - remove 'johnny' from 'newgroup1'\n  -A   - add ADMIN user for a group, dunno what it is about.. TODO  newgrp:\n  newgrp  \n  Changes primary group of the user w/o deleting any groups\n   could freely move between assigned groups \n  And asks for password(probably group password) if trying to set unassigned\n  group as primary one  groupdel:\n  delete group\n  groupdel [params]   setuid and setgid:\n setuid:\n  Attribute of a file that allows unprivileged user to have level of permission\n  of original owner of the file. \n  For example passwd executable is owned by Root user. But unprivileged user \n  could execute it and the passwd binary will made the changes into /etc/passwd \n  and /etc/shadow file(contains some related to passwords stuff too), and those \n  both files also are owned by root user.\n  In the same time passwd used by unprivileged user accepts no params - means\n  user can not change other users passwords\n setgid:\n  Attribute of file\\directory inherited byu sub dirs\\files, so subdirs inherit \n  parent's attributes. \n  Gives access  equal to group owning the file to unprivileged user executing \n  the file.\n  Unprivileged user executes file Under the privileges granted to the user group  owner of that file.  whoami:\n returns username of currently logged user  =====manipulate users and groups  =====Super User\n/etc/sudoers - file where all the privileges set. has user accounts with \n        privileges, as well as some groups, \n        for Ubuntu: \n            admin - almost root\n            sudo - as root\n        for CentOS7: \n            wheel - as root\nNOTE: this file must be edited through 'visudo' command, this will ensure safe\n  changes including lock file on edit and so on  root access:\n  should be disabled for remote login in sshd for security reasons  su:\n  su - [ ] - creates new session for a user, i.e.:\n     su - user - log in under 'user' user\n  log in as super user(root), need to provide root password i.e.: \n  su -\n   - dash is used to call login shell and reset most of env vars, basically safe\n     against env related exploits and overriden standard commands. see here:\n  https://unix.stackexchange.com/questions/7013/why-do-we-use-su-and-not-just-su  !!!!\n  in case root pasword is lost here is how to recover it from recovery mode\n  https://askubuntu.com/questions/24006/how-do-i-reset-a-lost-administrative-password\n  UBUNTU by default installs with random root password so noone knows it,\n   need to use SUDO   Another way to recover a root password is to add a user to a privileged group\n   like 'sudo' or 'wheel' - this way user, using program 'sudo' can become root\n   w/o entering the forgotten(random for Ubuntu) password:\n  user# groups - make sure user is in sudo group\n  user# sudo su -  - become root w/o entering password root's password\n  root# passwd  - enter new password  sudo\n  TODO\n=====super user end  =====user\\groups management end  ===Processes and Services Management  top: htop:\n  gives a list of processes and resources used by the OS\n  Process could has a priority:\n   20 - is the LOWEST priority\n   -20 - is the HIGHEST priority\n  PID number 1 is always 'init' command or 'systemd' in my case on ubuntu 16 and\n  centos 7  glances:\nhttps://opensource.com/article/19/11/monitoring-linux-glances?sc_cid=70160000001273HAAQ\nif you are a fan of top / htop / atop / iotop....  you might like this..\nlooks like glances can also setup a \"web interface\" to view.. as well as monitor things like docker etc  general processes:\n  all processes are spawned from process with ID 1, 'init' in manual or systemd\n  in my case, for some reason\n  so there is a PID which is Process ID\n  and PPID which is Parent Process ID, so every process has parent, except PID 1  it seems  strace:\n  shows trace of used files byt the given command - magic eh?\n   Example:\n  strace -s 2000 hostnamectl |& grep ^open | tail -5\n   will return files directly opened by 'hostnamectl' tool\n   still it could iteract with other services which could read from files, and this is not \n   seen here, alas.  watch:\n  cool debug tool again:\n  execute a program periodically, showing output fullscreen\n   Example:\n  watch \"ps -eaf|grep [h]ostname\"\n  #execute 'hostnamectl' and new entry will be added    root     11003     1  0 02:51 ?        00:00:00 /lib/systemd/systemd-hostnamed   ps:\n  by default returns processes run by my current user and current terminal \n  session\n  ps [-[-]]  \n       UNIX standards - - \n       BSD standards -  \n       GNU standards - -- \n    params could be mixed, but conflicts could appear\n   a - list all processes that has terminal attached: ps a\n   x - list all processes owned my you(same EUID as ps)\n   ax - list all processes(less columns)\n   u - user oriented format\n   p   (-p, --pid)- select process by id (or just PID, == --pid  )\n   -C   - select by command name(COMMAND column)\n   t   - select by tty\n   U, -U, -u, --user - selects by user name or EUID(RUID for -U), different \n        selections by user , will output different stuff\n   -j - jobs format\n   j - job control format\n   -H - hierarhy(tree\\forest format)\n   l - long format\n   -l long format , -y good with this\n   -f - full format listing\n   f - ASCII art process hierarchy (forest)\n   o, -o, --format - change columns format\n    i.e. ps -o pid,ruser=RealUser -o comm=Command - wil return 3 columns\n        named PID, RealUser and Command, with values of standard colmns  ps aux - returns all processes run by all users from all terminals\n        if user does not exist 'x' in case of UNIX format it could treat it\n        in BSD format\n  ps axjf - formatted method of processes with parents in tree view  pgrep:\n  process grep , could find process id (PID) by process name, like\n  pgrep bash - will return PID of the bash process running somewhere locally  kill:\n  for terminating the processes\n  kill    \n  each kills param has its number equialent\n  -TERM-15-  sends a Signal to a process (Term[inate] Signal), in other words\n   it asks the application to call its Dispose method, to gracefully stop. i.e.\n   kill 1292 or kill -15 1292 or kill -TERM 1292\n  -KILL-9 ask OS's Kernel to shut down the process even if the process(app) \n   does not respond for \n  -HUP - restarts process if possible, does not change PID\n  -l - lists all the signals available with their names and numbers(minus SIG\n   prefix)\n  NOTE: only owner of the process(or root) could kill the process\n  %JOB_SPEC - kills job with appropriate number\n   Examle:\n  kill %1 - will kill the job with JOB_SPEC [1]  kill all processes:\n  kill -9 $(ps aux | grep '/usr/lib/firefox' | grep -v grep | awk '{print $2}')\n   $() will be expanded. if list is returned will iterate over every item\n    in the list\n   grep -v - invert grep results (excluding own grep process from ps aux)\n   awk will print second column from the list - PID\n    as per grep matched several lines(List de-facto) every line will be \n    rinted. And sent out of $() into kill -9 as argument  killall:\n kills all instances of the process\n  Example:\n killall xlogo - will kill all instances of 'xlogo'  nice: renice:\n  changes priority of the process\n  nice is for new processes  renice is for already running processes  renice     i.e.\n  renice 10 1292 - will change priority of process 1292 to 10, and will display\n   previous value of the priority   nice       i.e.:\n  nice -n 20 /bin/bash - will start New(-n) process from /bin/bash binary, with\n   priority of 20(the lowest one)  =====Service controllers  sysvinit, systemd, upstart\n/usr/lib/systemd - systemd service manager working dir (new Rhel\\Debian-like)\n  https://unix.stackexchange.com/questions/5877/what-are-the-pros-cons-of-upstart-and-systemd  - GOOOOOOOD TO READ. Systemd compared to Upstart\n/usr/share/upstart - upstart service manageer working dir(old Debian\\Ubuntu)\n/etc/init.d - sysvitin service manager working dir (old Centos\\Debian\\Ubuntu)\nNote: Ubuntu could has all three installed currently, unlike centos that has\ninitd but it wrapped in systemd  service: daemon:\nservice vs systemctl vs upstart\nservice is an \"high-level\" command used for starting and stopping services in different unixes and linuxes. Depending on the \"lower-level\" service manager, service redirects on different binaries.\nFor example, on CentOS 7 it redirects to systemctl, while on CentOS 6 it directly calls the relative /etc/init.d script. On the other hand, in older Ubuntu releases it redirects to upstart\nservice is adequate for basic service management, while directly calling systemctl give greater control options.\nhttps://serverfault.com/questions/867322/what-is-the-difference-between-service-and-systemctl  RHEL-like - systemd (moved from SysVinit)\n    https://fedoraproject.org/wiki/SysVinit_to_Systemd_Cheatsheet\n  Debian-like - upstart service (also adopted systemd)\n  for Debian-like and RHEL-like systems its two different ways\n   for Ubuntu since 15.xx it seems it is even more different..\n   okay its systemd again(which uses systemctl as cli)\n  Debian-like:\n  status   \n  start  \n  stop  \n  restart  \n   where service could be like 'ssh' or 'cron'\n  to disable\\enable services in upstart services the .override file need to be\n  created in the /etc/init directory. For instance to disable cron:\n   ensure cron exists:\n   /etc/init/cron.conf - should exist\n   echo \"manual\" > /etc/init/cron.override - will create text file with word\n    'manual' as its only contents\n   now cron will not be loaded on boot\n   to enable cron back - simply delete 'cron.override' file from /etc/init dir  RHEL-like:\n  systemctl start  \n  systemctl status  \n  -- restart\\stop \n   where systemd could be like 'sshd' or 'crond' where d means daemon\n  systemctl disable   - disable system daemon from starting on boot\n   basically it will delete soft link from /etc/systemd/... directory which it \n   seems is monitored on Init and everything there is executed\n  systemctl enable   - enable datemon to start on boot\n   basically it creates soft link from real binary location of daemon file into\n   /etc/systemd/.... directory where all the links for boot are stored  Ubuntu since 15.xx\n  service   status\n  -- stop\\start\\restart\n   where service could be like 'ssh' or 'cron'\n  service --status-all - will return statuses of all the services\n!!  TODO: \n   check how to enable/disable services on boot for this new stuff and check how\n  it exactly called now\n  it seems it also supports systemctl and disable\\enable with same commands,\n   but creating .override file in the Debian way  Systemd:\nconfigure units(services)\nhttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/sect-managing_services_with_systemd-unit_files\n Example:\n https://www.jetbrains.com/help/teamcity/setting-up-and-running-additional-build-agents.html#Automatic-Agent-Start-under-Linux  systemctl:\n  cli for systemd service controller\n  it is called by 'service' command for backwards and cross compatibility reasons  journalctl:\n  displays logs\n   Params\n  -u - show logs for Unit\n   Example:\n  sudo journalctl -u sshd.service  hostnamectl:\n  cli of systemd service controller related to hostname changes and management\n  unified way to deal with hostname, list /etc/*-release, uname and stuff\n   read more here, like really detailed - good stuff:\n  https://unix.stackexchange.com/questions/459610/whats-the-point-of-the-hostnamectl-command\n   Example:\n  hostnamectl set-hostname 'gfs-server-03'  change hostname:\n  read more:  https://www.cyberciti.biz/faq/linux-change-hostname/\n   with systemd adopted:\n  hostnamectl set-hostname 'gfs-server-03'\n   before\n  /etc/hostname\n   + edit everything with old hostname in /etc/hosts, and add hostname to localhost addr  supervisor:\n  lightweight services controller, available for both RHEL and Debian like\n   distros. Could be used in containers instead of systemd or systemctl etc\n  Manages processes, could restart and monitor them, require config file to\n  operate\n   See how to start several processes in Docker using this tool:\nhttps://kuldeeparya.wordpress.com/tag/how-to-run-ssh-and-apache2-in-docker-container/  config file:\n    [supervisord]\n    nodaemon=true  [program:sshd]\ncommand=/usr/sbin/sshd -D\n\n[program:apache2]\ncommand=/bin/bash -c \u201csource /etc/apache2/envvars && exec /usr/sbin/apache2 -DFOREGROUND\u201d  ===processes and services management  ===Package management\n=====Debian\\Ubuntu  .deb - packages format  dpkg:\n  dpackage - debian packages manager, fully console, installs only package\n  -i - install a package\n     only package , no dependencies - generate error with dependencies required\n  -l - list of all installed packages(use grep for specific packages)\n  -L   - list of all the files that were created during package \n         installation\n  -S   - similar to 'yum provides' but only for Installed packages\n        it returns list of all packages that contain the string:\n        -S less will return also 'serverless' word.. \n    Another option is to use web interface on Ubuntu:\n    https://packages.ubuntu.com/  aptitude:\n  frontend of dpkg, GUI in commandline\n   categories of packages, local, available etc, then all the packages by \n  categories. also displays info about each package\n  Enter key to open\\close category.\n  g or u - install selected package  apt-get: apt:\n  stands for Advanced Package Tool\n   apt is merge of apt-get and apt-cache, for easier use, has not all functions\n   but also has some additional functions\n  main apt tool used to install or download packages\n  reads dependencies, and could install all of them\n  apt-get update - reads all the repos and updates the local packages cache\n  apt-get install - installs package(s), list delimited by space\n       or install   - installs particular version if\n            it is compatible with distro and stuff  \n       install -f - will install .deb package with its dependencies\n            Fix Broken Dependencies:\n            https://unix.stackexchange.com/questions/159094/how-to-install-a-deb-file-by-dpkg-i-or-by-apt\n  apt-get upgrade - made after Update, upgrades all the packages installed to\n            latest updates\n          -y - answer Yes for all questons automatically\n  .. dist-upgrade - updates to next available supported distro, 14.04 to 14.10 \n            if 14.10 is not supported already then to 15.04 etc\n  apt-get autoclean - cleans cache, which means freeing space on hdd  apt search   - search for package\n  apt list --  - lists available packages from repos\n       --installed - lists only installed packages\n  apt show   - shows info about package(apt-cache's command)  --dry-run - simulation, will display possible actions but not perform any  apt-get check - what dependencies may be broken\n  apt-get build-dep - exact build dependencies for particular app (it seems it \n        is not necessery to download them all for work though...)  apt-get download   - downloads package(probably to current dir\n                   or /etc/apt?) - but only package w/o dependencies\n  apt-get changelog - package changelog, like version history  /etc/apt/ - apt configs\n   sources.list - config of the repositories, which are re-read during 'update'\n          command execution\n  /var/cache/apt - cache folder of apt\n  ./archives - contain all the archives, could be removed by 'autoclean' command  apt-cache:\n  support tool of apt used to work with apt cache(updated by apt-get cache \n  command)\n  apt-cache pkgnames - list of all pkgs APT knows, not all could be downloaded\n            installed or installable e.g. virtual pkgs\n  ... search   - lists all packages that contain package name in its \n             name or description\n  ... show   - info about the package, like description in apptitude\n  ... stats - info about local cache(packages related stuff) - could be shrinked\n          by running 'apt-get autoclean' to delete useless stuff\n  apt-cache showpkg   - shows infou about package\n            and where from package came from\n    Example:   apt-cache showpkg htop\nPackage: htop\nVersions: \n2.0.1-1ubuntu1 (/var/lib/apt/lists/ua.archive.ubuntu.com_ubuntu_dists_xenial-updates_universe_binary-amd64_Packages)\n     it is htop package with version 2.0.1ubuntu1\n     taken from ua.archive.ubuntu.com\n      where repository is for xenial\n      and name of repository is Universe\n   apt-cache policy - returns all used repositories, kind of    apt-file:\n  another extension for apt-get, is in different package, \n   need to be installed first  apt-file update - updates its own caches, need to be run first\n  apt-file find    - alias for search, see below:\n  apt-file search   - search packages for file matched by pattern.\n        returns list of packages and paths where file was found;\n        includes removed packages too\n  apt-file list - similar to dpkg -L but package not need to be installed or\n        fetched  =======Uninstallation\n  apt-get remove   - removes package binaries, leaves configs in the \n                     system for future use by other version or another\n                 reason\n  .. remove --purge   - removes package and all the stuff package \n  .. purge   ^same^     created during install(SYSTEMWIDE configs, link\n                etc)\n        it will not remove:\n         -dependence packages , to delete orphanes use\n            apt-get autoremove\n              or\n            .. --purge autoremove (same - will remove configs also)\n         -NOT SYSTEMWIDE config files - user-specific files\n            files in user's home dir, or .config subdirectory of \n            home, those could be hidde (starts with .)\n         -doesn't reverse changes in already existing user-specific\n          config files\n         -doesn't remove 'gconf' and 'dconf' files or reverse any\n          configuration d\\gconf changes\n    existing SYSTEMWIDE configs also are not affected by neither purge or\n    remove commands, those ones created by user or other packages. but \n    uninstalling package could sometimes affect such files and undone \n    something\n  apt-get autoremove - removes all orphaned packages\n  .. purge --auto-remove   is similar to autoremove\n=======uninstallation end  =======Repository setup\n Repository configs are in \n  /etc/apt/sources.list and /etc/apt/sources.list.d/ \n  Contents of sources.list file:\n   deb http://archive.ubuntu.com/ubuntu/ xenial main restricted universe\n  where http... is address of repo\n        xenial is name of distro - $(lsb_release -sc)\n        main ... is name of repository\n   After 'add-apt-repository multiverse' this repo will be concatenated into\n   line above:\n    deb http... xenial ... multiverse  add-apt-repository:\napt-add-repository:\n  Newer versions of Ubuntu support 'add-apt-repository'\n   standard repositories could be added w/o specifing address, just repo name: \n    Example:\n   sudo apt-add-repository universe\n    this will add 'universe' repo to standard ubuntu addresses\n  Older versions support this:\n   sudo add-apt-repository \"deb http://archive.ubuntu.com/ubuntu $(lsb_release -sc) universe\"  NOTE: make sure to 'sudo apt-get update' before use newly added repository\nRead More:\nhttps://askubuntu.com/questions/148638/how-do-i-enable-the-universe-repository  Due to regular use of secure stuff also make sure following packages are\n  installed(use apt-cahce show   or 'apt list --installed'):\n   apt-transport-https - This package enables the usage of \n    'deb https://foo distro main' lines\n    in the /etc/apt/sources.list so that all package managers using the\n    libapt-pkg library can access metadata and packages available in sources\n    accessible over https (Hypertext Transfer Protocol Secure).\n      This transport supports server as well as client authentication\n     with certificates.\n   ca-certificates - list of PEM files of certificates approved by Certificate\n    Authorities, some common trusted certificates. For instance it has\n    certificates for Debian stuff and Mozilla stuff.. \n    TLDR:\n    It is a list of default trusted SSL sertificates stored in PEM format.\n   curl - tool for transfering data from or to a server via HTTP/S, FTP/S, LDAP\n    and lots of other formats, except SSH, but it has SCP..\n    Used to download GPG key from docker storage. Which is added then to\n    local apt storage\n   software-properties-common - adds additional command for APT repositories \n    management. Such as 'add-apt-repository'  Add gpg key:\n    curl -fsSl https://download.docker.com/linux/ubuntu/gpg | sudo apt-key\n    add -\n      Curl will download gpg key from docker , then pass it to apt-key (-) \n    and it will be added to local key storage\n    make sure it is added, by searching for fingerpring:\n    sudo apt-key fingerprint 0EBFCD88 - this will return OK if everything is\n    fine\n  Add a STABLE repository, it seems it is required:\n    it is stored in:\n    /etc/apt/sources.list - the file with the sources, where \n    add-apt-repository will add the repo by default\n    /etc/apt/sources.list.d - directory where new file with a repo should be\n    added, like in CentOS' yum\n    add-apt-repository example:\n    sudo add-apt-repository \\\n       \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n       $(lsb_release -cs) \\ - returns distro name\n       stable\" - list of repos, test, edge could be added for docker\n  PPA: launchpad.net:\n        Ubuntu in default repos has only stable versions of packages that was \n        on moment of release. Later on only security updates.\n        launchad.net has lots of repos(Personal Package Archives - PPA)\n        add PPA repo to for latest versions:\n      Ansible:\n     $ sudo apt update\n     $ sudo apt install software-properties-common\n     $ sudo apt-add-repository --yes --update ppa:ansible/ansible\n     $ sudo apt install ansible\n    Another example:\n     $ sudo add-apt-repository ppa:jonathonf/python-3.7\n     $ sudo apt-get update  update-ca-certificates:\n  updates certificates from  somewhere \n   Pamars:\n  --fresh - somethinh fresh?  List certificates:\n  awk -v cmd='openssl x509 -noout -subject' '\n    /BEGIN/{close(cmd)};{print | cmd}' < /etc/ssl/certs/ca-certificates.crt\n   Search for existing root cert:\n  \\ | grep -i ' '  =======List all installed packages:\n  dpkg -l - list of packages and their details\n  apt list --installed - list of full names\n  aptitude - has a category for installed pkgs divided by categories\n Active Repositories\n  apt-cache policy  =====debian\\ubuntu end  =====RedHat RHEL\\Centos  .rpm files\nrpm:\n  red hat package manager\n  It seems it has some differend modes, where similar keys could exist with \n  different functions, i.e. -i - installation mode, -i in -q(query) mode is info\n   the modes are:\n   - install/update/freshen (first param -i)\n   - uninstall (first param -e)\n   - query (first param -q)\n   - verify (first param -V probably)\n   - set owners/groups\n   - show querytags\n   - show configurations\n  Also it has general options, seems like it works with any mode\n  if files or dependencies package need are absent installation of rpm will fail\n  -q - query, probably 'what about something, like package requirements'\n    single param will display package full name if it is installed or will\n    say that it is not installed, i.e.:\n!   rpm -q openssh-server - will return fill name if installed, like this\n                openssh-server-7.4p1-13.el7_4.x86_64\n   -p - package file\n   -R - requires, lists package dependencies\n  i.e.:\n  rpm -qpR   - will list package requirements(depen\n    dencies), if rpm_package name is given(xterm-123.3.rpm) - will check for\n    package locally if general package name is given(xterm) will look into\n    remote repos\n   -l - list files of the package, similar to 'dpkg -L' i.e.:\n    rpm -ql openssh-server - will list all the files and paths of sshd\n   -a --last - will list (a)ll the packages installed filtered by latest\n   rpm -qa | grep nmap - will list all Installed packages, and grep nmap from \n        this list\n   -d - documentation mentionings of a package \n   -f - some related to documentation param\n   rpm -qdf   - return all documentation files where this package is\n        mentioned\n  -i - installation mode (fails if package installed of any version)\n   -v - verbose (general parameter)\n   -h - prints some hashes, looks better with -v\n   -U - update package, BUT will install package if there is none installed\n    if package is installed will not fail but update it(controversy to -i)\n  rpm -Uvh   - will update or install package, \n    fails if there are missing dependencies\n  -e or --erase - erase package, uninstall mode\n   -v - short verbose mode, RETURNS NOTHING if no Verbose mode is passed\n   -vv - very verbose, dunno why not -v\n  -V - verify, there is some verification key for every package and rpm could\n    check whether the package indeed has that key\n   -a - all, same as for -a in Query mode of rpm tool \n   Keys could be imported from remote repos, or rpm could list all the keys\n   that has been already imported\n   rpm --import - will import key somehow from somewhere..\n   rpm -qa gpg-pubkey* - will list all the public verification keys on system  yum:\n  Stands for \"Yellowdog Updater, Modified\". yellowdog is already unsupported \n  version of Linux for PowerPC, which is also dead now\n  yum    \n  Package management system\n  yum update \n  yum upgrade - same as update - updates all the pkg repos and upgrades pkgs  yum list [ ] - displays if installed, version, and @updates mean that it\n           accepts updates. Package could be market to ignore updates\n           which probably means that yum update will not affect it\n  ..  list installed - returns list of installed packages\n  yum search   - searches for package, search in name and description\n  yum install   - install package, and its dependencies\n    -y - answer Yes for prompts automatically\n  ..  install https://...../package.rmp \n        will install package from remote location\n        or this installs repolist.. from which that packages could be \n        installed..\n        --enablerepo=   - enables repo\n    Example:\n   yum install -y https://centos7.iuscommunity.org/ius-release.rpm && \\\n    yum install -y php71u-fpm\n  yum localinstall   - installs .rpm package downloaded to local machine\n    will ALSO install needed dependencies, unlike 'rpm' tool\n  yum info   - info about a package: descr, url, size, arch, etc\n  yum check-update - checks which packages could be updated\n     -C - use only local cache, doesn't update cache\n  yum grouplist - YUM can group packages, this command displays all the groups\n          those groups will install bunch of packages\n  .. groupinstall ' ' - will install all the packages from the group\n  .. gropuremove ' ' - removes all the packages related to the group\n  .. groupupdate ' ' - updates same stuff  yum-config-manager --enable remi  - enable repo  yum repolist - lists all available(enabled) repositories\n  ..  .. all - lists all the repos disregard of their staus\n  .. --enablerepo=  install   - enable a repo and\n        install the package from that repo\n    Example:\n   yum repolist enabled\n    show enabled repos\n   yum repolist all\n    show all repos (including disabled)\n  yum provides   - Just use a specific name or a \n    file-glob-syntax wild\u2010cards to list the packages available or installed\n    that provide that feature or file.\n         Note:\n        this could not work for regular user, use 'sudo'\n  yum clean all - clears cache similar to apt-get autoclean\n    Example of usage:\n     use it after some bunch of packages has been installed to clean space\n  yum history - history of yum - install\\remove\\upd package and stuff, sudo  =======Manage repositories\nrepos stored in /etc/yum.repos.d, which is a directory\nto add new repository:\n create new file in the /etc/yum.repos.d dir named ' .repo'  Format of repo file:\n[dockerrepo]  - name of the repository section\nname=Docker Repository - name of repo to display\nbaseurl=https://...    - address to where from do pulls of packages\nenabled=1          - repo is enabled inside my system\ngpgcheck=1         - enables gpg check\ngpgkey=https://....    - location of the key  =======Uninstallation\n  yum remove   - removes the package, doesn't remove its dependencies\n!   yum autoremove   - also removes package, probably its deps too..\n  To remove dependencies there are several ways:\n  Option1: [probably should do this as standard]\n  yum autoremove - removes orphaned dependencies, similar to 'apt-get utoremove'\n  Option2: [probably could try this]\n  update /etc/yum.conf file\n   set\n    clean_requirements_on_remove=1\n   it is boolean value, which works on removel\\update\\obsoletion, goes through \n   each package's dependencies and deletes ones that are no longer required.\n   default value False, could be 1,0,True,False,yes,no\n  Option3: [probably better try not to use it]\n  yum history undo  \n  basically in undoes the operation, that was performed during installation\n  and thus removing the package and all the dependencies, and probably other\n  stuff. \n  the only thing here is that i'm not sure whether it checks that package from \n  dependencies is dependent only by this package that caused them to be \n  installed, or not, so is not - it could end up with broken dependencies\n  so how to do:\n  yum history - check the ID column next to the command line with installation\n  command of the package that need to be uninstalled\n  then call 'yum history undo  '\n=======uninstallation end  =======List all installed packages:\n  rpm -qa - query all packages, will return list of all installed packages\n  yum list installed - will return list of installed packages  yum-utils:\n  yumdownloader:\n  yumdownloader  \n    downloads a .rpm file, into current dir pretty similar to apt-get download\n    command\n=====redhat rhel\\centos end\n===package management end  ===File Permissions / Ownership  ! Linux treats everything - device\\file\\directory as if it is a FILE  Discertionary Access Control (DAC)(used after Mandatory Access Control MAC from LSM):\ndrwxrwxrwx:\nd - type of file(file\\device\\dir\\link)\n r - read\n   r only gives ability to list contents, but lots of ????? and file names\n    can't even read files with 'cat' and stuff, need x permission for this\n w - write\n x - execute\n   x only gives ability to 'cat' files, but cant ls into dir\n    if file in such dir has only 'r' can even 'cat' it, if file has only x: cant\n     cat it, need 'r' on file in dir with only 'x' to cat\n   together with read - can ls norm and execute stuff like 'cat' to read file\nfirst 3 - user permissions \nsecond 3 - user's group permissions\nthird 3 - all other users  chown:\n  changes owner and group of a file\\directory\\link\n  chown   [user][:group]  \n         -reference=R_FILE FILE\n  !!!! links could change owner of targeting file only!!!!!!!!!!\n  -h \\ -H - for changing owner of LINK not referenced location, see MANUAL\n  -R - recursively change owners of nested directories\n  -v - verbose, could be redirected to a file for future use, just in case\n   in case of fucked up links\n  could change only owner( ), only group(<:group>), or both( )  could take ownership schema like owner:group from a file:\n   chown --from=:user :otheruser file.txt  chmod:\n  change modification\n  chmod [ ]    \n  text modifications:\n  u - owner\n  g - group\n  a - all\n  r/w/x - read/write/execute\n  i.e.\n  chmod a+rwx   - give all the permissions of read/write/execute  binary mode:\n  4 - read\n  2 - write\n  1 - execute  sum of numbers above will indicate permissions\n  rwx = 4+2+1 = 7, and so on   Parameters:\n  -v - verbose for every file processed\n  -c - changes (less verbose), only when change is made\n  -f - supress most error messages, --silent, --quiet\n  --preserve-root - fail to operate recursively on '/'\n  -R - recursive  lsattr:\n  lists extra attributes of files on linux file system\n  lsattr file1 - display attributes of the file1  chattr:\n  changes extra attributes, lots of them, here some:\n  i - immutable (can not be changed)\n  a - appendable (can be opened only in append mode)\n  u - undeletable\n  syntax: \n  + - adds attribute\n  - - removes attribute\n  = - causes  selected attrs to be only attrs that files have..(not sure)\n  i.e. \n  chattr +u file1 - adds undeletable attr to file1  ===file permissions / ownership end  ===IPTables / Linux kernel Firewall(not d)\nread more:\nhttps://www.unixmen.com/iptables-vs-firewalld/\nhttps://linuxacademy.com/cp/socialize/index/type/community_post/id/15473  !!!!!!!!!!!!!!!!\n!!  WHen using SSH configure SSH rules first BEFORE changing \n!!    the policies of the chains in the Filter table. \n!!  !!    TO DO NOT LOCK MYSELF OUT OF THE MACHINE  !!  !!   !!   !!\n!!  CentOS 7 applies the rules right after entering the command\n!!  even if Service IS stopped.\n!!!!!!!!!!!!!!!  ///Iptables is a tool to work with Netilter Framework, kind of firewall\n/There are different services on different Linux distributions such as:\nUbuntu - ufw(Uncomplicated Firewall) service for Ubuntu, which uses IPtables\nCentos7 - firewalld - firewall daemon which uses iptables\ngeneric? - iptables-services - firewall service that uses iptables, generic \n       it seems  Main three groups or something:\nTables:\n used to group different functionalities and separated in five:\n - filter - default table if no specified. Packet filtering.\n   Built-in chains:\n    - INPUT\n    - OUTPUT\n    - FORWARD\n - nat - used for network address translation(NAT)\n - raw - first table to check, configuring exemptions from conn. tracking\n - mangle - some specialized packet alteration...\n - security - Mandatory Access Control(MAC) networking rules\n -t--table - parameter to specify a table  Chains:\n Tables contain set of chains.\n Chains group rules on different points of process.\n Chains could be Built-in or User defined.\n  INPUT - Filter. input packets\n  OUTPUT - Filter output packets\n  FORWARD - Filter. going through packets\n -N--new-chain - adds user defined chain\n -X (--delete-chain) - delete chain  Rules:\n Defined as a set of matches and a target. \n Are listed in chains and followed by order until a match is found, then packet\n is handled by the target specified by the rule.\n -A (--append) - add new rule\n -D (--delete) - delete rule along with the chain in which is contained\n -L (--list) - list rules\n -S (--list-rules) - same list but in command format, as if the commands\n             that were used to add rule is saved and displayed here  Matches:\n   if match is true the packet will be processed by iptables.\n   Following matches exist(not full list):\n   -s (--source) - source of the packe(IP, hostname or network IP)\n   -d (--destination) - destination of the packet(IP, hostname or network IP)\n   -p (--protocol) - protocol(tcp/udp/all , all - default if none specified)\n   -i (--in-interface) - interface that receives(left side of ifconfig output)\n   -o (--out-interface) - output interface\n   ! - Not. Match everything that is not in the match.  Protocols has also its own matches, like \n   --dport - destination port(22 for ssh) is match for TCP protocol\n  see full list of protocol related matches:\n   iptables -p   -h - where protocol is tcp, icmp etc i.e.:\n    iptables -A INPUT -p tcp -i eth0 --dport 22:25 -j ACCEPT \n      - adds rule to default table (-t filter) , appends Chain INPUT, matches:\n    protocol TCP and interface eth0, protocol specific match port with\n    range of ports 22 to 25. Rule's target if match is met - ACCEPT. \n    If not next rule will be processed until RETURN or end of rules, then\n    Default TARGET(Default policy) will be applied.  Targets:\n  Determines the action to be taken if packet is Matched.\n  -j (--jump) - specify Target\n  There are 4 built-in Targets:\n   ACCEPT - no checks , just accept packet\n   DROP - refuse packet, do not send a response(simply ignore it)\n   QUEUE - sends the packet to user space\n   RETURN - returns to previous chain, or handle by Chain policy(Default?)  Default Policies:\n  When packet is not matched by any Rule on the Chain - it is handled by the\n  target specified in the policy of that chain.\n  Two main approaches:\n   Accept everything by default, and add rules to refuse access\n   Refuse everything by default, and add rules for accept\n  -P (--policy)     - set default policy for a chain with target  iptables:\n  a generic table structure for the definition of rulsets\n  basically ruleset of Linux kernel firewall\n  it is a tool to get daemon(service) install other app:\n   Centos - iptables-services\n   Ubuntu\\Debian - ufw(probably goes with iptables pkg)\n   RHEL - iptables  -L - list all rules currently installed on the system\n  -A   - ADD  Service uses loads default config file when starts, applying some \ndefault rules, /etc/sysconfig/iptables for CentOS 7\n  to manage a service:\n   Debian/RHEL: sudo /etc/init.d/iptables start\\stop\\status\n   Ubuntu: sudo service ufw start\\stop\\status\n   Centos: servicectl start\\stop\\status iptables  IP address ranges:\nCIDR blocks\nhttps://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation\nhttps://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#IPv4_CIDR_blocks  firewalld:\nfirewall-cmd:\n  https://www.digitalocean.com/community/tutorials/how-to-set-up-a-firewall-using-firewalld-on-centos-7\n firewall-cmd --get-default-zone\n  get default zone\n firewall-cmd --get-active-zones\n  get all active zones\n sudo firewall-cmd --list-all\n  list all rules for zones  Open ports:\n  1. Open port for a service.\n   add service to /etc/services\n   add service to FirewallD\n   https://linuxhint.com/open-port-80-centos7/\n  2. Open prot directly:  OPEN PORT:\n  sudo firewall-cmd --zone=public --add-port=5000/tcp --permanent\n CHECK OPENED:\n  sudo firewall-cmd --zone=public --list-ports\n RELOAD AFTERWARDS:\n  sudo firewall-cmd --reload  Example:\n  firewall-cmd --zone=public --add-port=5693/tcp      - turns on the rule\n  firewall-cmd --zone=public --add-port=5693/tcp --permanent   - saves the rule so it is applied on the boot  ===ip tables / firewall  ===Bash\n=====Bash command line shortcuts\nctrl+a - moves cursor on the beginning of the line\nctrl+e - moved cursor on the End of the line\nctrl+u - removes everything from cursor position to the beginning of line\nctrl+k - removes everything from cursor position to the edn of the line\nctrl+w - removes previous word(space separated)\nctrl+l - cleares screen except of current command LINE\nctrl+p - last executed command shown in command line, same as UP ARROW key \nctrl+c - terminate current process + exit\\del current line by terminating it\nctrl+z - pause program, adding it to backgroud job. 'bg'\\'fg' to continue run\nhistory:\nctrl+r - bash history by typed command\nctrl+r - again looks one matched line upwards\nctrl+j - edit matched line\nctrl+o - execute current command from history  =====Bash Shell\nalias:\n new aliasws workws only for current session\n alias  =\" \" - adds command as alias\n alias   - shows command\n alias - shows all aliases for current session  sleep:\n  just regular sleep, stops current thread for amount of given seconds  Experiments:\n  sleep 10 & - will stop the thread for 10 seconds and move it to background  background: (not command)\n  to start process in a background use & at the end\n   Example:\n  xlogo & - will start xlogo but cmd will return control to user\n   xlogo will run as a Job, with 'Running' state  jobs:\n  built-in command\n  shows jobs in background, running, stopped\n   More read:\n  https://unix.stackexchange.com/questions/116959/there-are-stopped-jobs-on-bash-exit\n   Params:\n  -l - display PID  NOte: kill could also accept JOB_SPECs to kill jobs, instead of PIDs  fg:\n  moves job to the foreground\n  fg JOB_SPEC\n   Example:\n  fg 1 - moves job [1] to the foreground\n  Ctrl+z will move it back, but 'paused'- use bg to continue in background  bg:\n  moves job to the background\n   If program is 'Stopped' like after executiong ctrl+z shortcut, bg <%num>\n  will continue its execution as a job, in the background\n   Params:\n  %job_spec - address numbered job  script:\n  writes a session script - all executed commands and stuff\n   with time parameter will create file with timings of each command and could\n  play it afterwards using these two files  bash:\n  man bash  -c - accepts a whole bunch of commands like:\n  /bin/bash -c \"while true; do echo HELLO; sleep 1; done\" - which will execute\n    echo HELLO endlessly witn interval of 1 second.\n    This even could be passed into a container:\n   docker run -d ubuntu:xenial /bin/bash -c \"....\"  could customize command prompt from default  @ : $ \n  to something like      !\n   \\t - 24 hour time format\n   \\w - full current path\n   \\w - current directory only\n   \\u - username\n   \\j - amount of running jobs\n   \\h - hostname\n      Hostname is stored in /etc/hostname  hostname:\n  network computer name. command returns current hostname\n  value is stored in /etc/hostname\n   Synopsis:\n  hostname [-b] {hostname|-F file} - set hostname (from file)\n  hostname [params]            - formatted output\n  hostname             - display hostname\n   Params:\n  -I - show all ip addresses for host machine\n  -b - set hostname\n   Example:\n  hostname -b new_hostname  TO CHANGE HOSTNAME:\n   change hostname in /etc/hostname\n   change every occurance of old hostname to new one  in /etc/hosts\n    If nothing is available in /etc/hostname first hostname with 127.0.0.1 \n   address will be taken as hostanme  history:\n  lists history, dumps .bash_history file into STDOUT, lines numbered\n  -c - Clear history, but only in session, .bash_history remains the same\n    only in CentOS it seems, ubuntu does nothing\n   Example:\n  history | grep /usr/bin - look for only what matched  which:\n  shows location of a command and its alias if exists\n   Does not show built-ins\n  can search by alias too\n  which  \n   i.e.\n  which ll - will return alias line and path for binary  whereis:\n  searches for binary file, sources, man files of the command\n  Does not work with aliases!\n   i.e.\n  whereis pwd - will return path for binary and path for man page\n   -b - search only for binaries\n  Example:\n  Remove IntelliJIdea:\n  $whereis idea\n  remove all entries returned, and places whihch they are linking\n   https://intellij-support.jetbrains.com/hc/en-us/articles/206544519\n   https://askubuntu.com/questions/300684/uninstall-intellij-ultimate-edition-version-12/300692#300692  Streams and Redirects. redirection:\n Linux\\Unix philosophy is that 'everything is a file' but in fact this are not\n  files but block defices stored in /dev or /proc directories, and not files but\n  some stuff stored in RAM for instance\nSTD - means standard\n  Read More:\n  https://stackoverflow.com/questions/3385201/confused-about-stdin-stdout-and-stderr\nSTDIN:\n  I/O Stream\n  file handle that process read to get info from user\n  NOTE:\n/dev/stdin is a symlink to /proc/self/fd/0 -- the first file descriptor that the\n  currently running program has open. So, what is pointed to by /dev/stdin will\n  change from program to program, because /proc/self/ always points to the \n  'currently running program'. (Whichever program is doing the open call.) \n  /dev/stdin and friends were put there to make setuid shell scripts safer, and\n  let you pass the filename /dev/stdin to programs that only work with files, \n  but you want to control more interactively. (Someday this will be a useful \n  trick for you to know. :) \nSTDOUT:\n  I/O Stream\n  process writes normal info to this file handle\nSTDERR:\n  I/O Stream\n  process writes error info to this file handle    used to redirect STDOUT in cli to someplace else (i.e. file instaed of \n    console)\n  2> - redirects STDERR   redirects STDOUT\\ERR but file will not be overwritten, and appended \n    instead\n  < - redurects STDIN, it will take data not from command line but from\n    something else, i.e. a file\n  cat /etc/passwd > /tmp/out     # redirect cat's standard out to /tmp/out\n  cat /nonexistant 2> /tmp/err   # redirect cat's standard error to /tmp/error\n  cat < /etc/passwd              # redirect cat's standard input to /etc/passwd\n    or like this:\n  cat < /etc/passwd > /tmp/out 2> /tmp/err       NOTE: redirect Standard Output(and error too) to null will hide it from everyone\n  like this:\n   ls secret_place >> /dev/null - will drop output into a void!!11\n  Errors also could be hidden like this, in case of exposing of some secure or\n  sensitive info  Redirect SDOUT and STDERR into two files in the same time:\n  cat goodFile notExistingFile >mystdout 2> mystderr\n    mystdout will contain standard output\n    mystderr will contain standard error output  Redirect STDOUT and STDERR into single file:\n  cat goodFile notExistingFile >mystdout 2>&1\n    mystdout will contain both STDOUT and STDERR\n    Or\n  cat goodFile notExistingFile &> mystdout  noclobber:\n  is an option set-up by 'set' command\n   prevents redirection '>' from overriding files. it will return an error  pipe: |:\n  redirects standard output of first program into standard input of another  expand:\n  changes Tabs to Spaces in files from argument or stream from STDIN\n   Example:\n    cat -A test   >>> ^Ia ^Ia    a^Ia$\n    expand test | cat -A  >>>        a       a    a  a$\n  will change all tabs in file to spaces.  unexpand:\n  change Spaces to Tabs - vice versa of 'expand'    cut:\n  Removes section from each line(column in fact) from a File\n   Or parses each line in file by delimiter(Default is TAB) and could retreive\n   only selected columns, or bytes - this is for -f option\n  cut    \n  See more: https://www.computerhope.com/unix/ucut.htm\n  -d ' ' - change delimiter to  10.1 10/01/2007 - '-f 3' will cut out '10/01/2007'\n  How LIST works:\n   !List of Integers\n   !Starts from 1\n   list of Integers, or range of integers, or multiple ranges, Separated by \n   commas. Selected columns is printed in the same order they were read,\n   written to output exactly once(dunno what it means)\n  N - the Nth byte, character or field\n  N- - range from Nth byte,char,field to end of the line\n  N-M - range from N to M as above\n  -M - from beginning of the line till M\n  -f Option:\n  cut -f 1-2,4-5 data.txt - will cut 1st,2nd, 4th, 5th columns from data.txt\n   --output-delimiter - changes delimiter in the output i.e.:\n    cut -f 1,3 -d ':' --output-delimiter=' ' /etc/passwd - will substitute\n    colon delimiter ':' to space ' ' in the OUTPUT of command for easier\n    read\n    ..... --output-delimiter=$'\\t' - will substitute colon ':' for tab '\\t'\n    character, '$' here is for escapt\n  -c Option:\n   cut -c 3-12 data.txt - will cut from 3rd to 12th characters in the data.txt\n        will not use delimiters at all\n  -b Option:\n   cut -b 3-12 data.txt - will cut rom 3rd to 12th BYTE, in case of ASCII \n        encoding it will be the same with -c as there 1 char == 1 byte  Advanced example:\n   grep '/bin/bash' /etc/passwd | cut -d ':' -f 1,6 - greps passwd file for \n    users that use /bin/bash as their default shell, and prints out \n    username and user home directory\n    So cut accepts STDIN redirection  paste:\n vice versa of 'cut' - pastes columns in files  join:\n sql join but for text files\n  in case of need to use this, use man\\google, i'm to lazy to document\n Sorry.  bash escapes:\nspecial symbols:\n$'\\t' - print tab, which is not printable character, same as /r/n or somehting  =====Bash Scripting\ntest:\n [ 1 -eq 1 ]    - ints compared with -eq, -lt, -gt, -ge, etc\n [ 'a' = 'a' ]  - strings compared with =, <, >, >=, etc\n  tests condition, returns 0 if test is passed(true)  -f   - file exists, aviode ~\n  -d   - directory exists\n  -h   or -L - symlinc exists\n  -s   - exists and has size greater than 0\n  -u   - exists and set-user-ID bit is set\n  -k   - exists and its sticky bit set \n  -n   - length is not zero\n  -r   - file exists and with read permission\n  -w   - file exists and with write permission \n  -x   - file exists and with execute permission\n  -z   - length of string is 0  Test could be performed from script , better to use braces [ . ]\n  Also could be preformed right inside CLI, it will return nothing, so need to\n  create manual then\\else-like structures:\n   test -f '/path/to/file name' && echo \"exitcode: &?. True\" || echo \"exitcode:\n &?. False\" - test for file existance, then if exit code 0 will execute second \n    echo with True, else (OR) will execute echo with False.\n      &? -  will print exit code of previously executed command, which in this \n    case is 'test' command  Test even can be performed w/o test itself:\n    Example:\n  curl -s google.com | egrep -ci '301 moved' > /dev/null && echo \"file has moved\n\" || echo \"false\" - curl will pass STDOUT to egrep which will try to match \n    string '301 moved', if it succeed exit code will be 0.\n    And first Echo (after &&) will be executed\n    Else, egrep will return 1, and thus second echo (after ||) will be\n    executed  for:\n  regular for loop could be done in command line too  for i in  command ; do echo $i; done\n    backtick -   , does the job, expression between it will be executed in \n    sub shell and its result will be returned, when we have a collection\n    we could iterate it with FOR \n  for line in cat file1`; do echo $line; done  ===Other programs:  elinks:\n  command line browser, with interface similar to MC or aptitude  mc:\n  two-windowed GUI to work with file system  ===Nginx\n  fast web server, could be used as load balancer between servers(docker \n   containers) - reverse proxy server  package name: nginx\n  /etc/nginx - config(as usual in etc dir)\n    ...conf.d - directory for some main configs probably\n    ...default.d - directory for default configs\n    ...sites-available - directory for available sites, convention directory\n    does not exist by default\n    ...sites-enabled - same as previous\n    Read more about sites-* here:\n    https://serverfault.com/questions/527630/what-is-the-different-usages-for-sites-available-vs-the-conf-d-directory-for-ngi  Basically all the configs are included in main config:\n/etc/nginx/nginx.conf\nit includes all the configs from directories mentioned above\nsame settings from different configs are overwritten by later added:\nfirst.conf(a=1, b=2), second.conf(b=4, c=3) will be treated like this:\na=1, b=4, c =3  !!! MAKE SURE to follow syntax !!!\n    'upstream' directive can not be added before http{} section \n    or inside http{} section  So all includes need to be made correctly\nalso not all directives could be added into configs that are included\nbecause it could lead to conflicts and Nginx wont start\n\nIF nginx is not starting check:\njournalctl -xe - for service errors\n\nIF nginx throws errors or behave in not expected way check error log\nits location in nginx.conf file\ndefault location: /var/log/nginx/error.log\n\nNginx could refuse connection:\n(13: Permission denied)\n   this related to SELinux, which also has error log:\nsudo cat /var/log/audit/audit.log | grep nginx | grep denied\n SELinux has bool values that Nginx and Apache and probably others use\n   getsebool -a | grep httpd - will return all bools related to httpd \nand Nginx(it uses them too)\n To allow connections set:\nsetsebool httpd_can_network_connect on - or use 1\n-P - will set it permanently\n\n\nConfig setup, for proxy server:\nOption 1:\ncreate config with following stuff:",
            "title": "lsblk"
        },
        {
            "location": "/linux_cheatsheet/#pool-for-balancing-should-be-after-http-section",
            "text": "upstream containerapp {\n        server localhost:8081; #- not sure whether localhost is fine\n        server localhost:8082;\n}",
            "title": "pool for balancing? should be after http{} section"
        },
        {
            "location": "/linux_cheatsheet/#server-configuration",
            "text": "server {\n        listen *:80; #everything coming to port 80      server_name localhost;\n    index index.html index.htm index.php #match any such file and use it\n\n    access_log /var/log/nginx/localweb.log; #save logs here\n    error_log /var/log/nginx/localerr.log;\n\n#location is root , for some reason\n    location / {\n    #pass traffic coming through port 80 into the pool\n            proxy_pass http://containerapp; \n    }  }   Option 2:\nAdd upstream (with local network IPs) into nginx.config after http{}\nsection\nUpdate server{} like above, but w/o *_log fields\nServer_name also use local network IP\nALso i commented default 'root' not sure if this necessary  ===Networking  networking relies on configuration files:\n/etc/sysconfig/network-scripts/ifcfg-  - this file describes \n    how network interface works, every interface has its own file.\n    Settings of the interface are stored in here\n     Example:\n    DEVICE=\"eth0\"\n    BOOTPROTO=\"dhcp\"\n    HWADDR=\"00:0C:29:3B:44:D3\"\n    IPV6INIT=\"yes\"\n    NM_CONTROLLED=\"yes\"\n    ONBOOT=\"yes\"\n    TYPE=\"Ethernet\"\n    UUID=\"951b47f7-e9c5-4e64-85ce-44c79732d914\"\n/etc/sysconfig/network - general network configuration, probably\n     Example:\n    NETWORKING=yes\n    HOSTNAME=centos7\n    GATEWAY=10.0.0.1\n/etc/resolv.conf - something like 'hosts' in Windows, it seems, store default DNS here\n     Example:\n    nameserver 8.8.8.8   - default google's DNS\nsee this for details on resolving Networking issue on VMWare:\nhttps://superuser.com/questions/901672/centos-7-ping-8-8-8-8-connect-network-is-unreachable/901677  tcptraceroute:\n  traceroute to port \n   Example:\n  sudo tcptraceroute teamcity.tideworks.com 443  ip: \n  link - creates network interface with name and type:\n    also could be used if 'ifconfig' is absent\n    Example:\n     ip link add br10 type bridge - will create network interface with \n    name br10 and of type bridge\n   add - add interface\n   set - sets interface, somehow\n    ip link set br10 up - will enable network interface br10  addr - manages ip addresses. \n    in case 'ifconfig' is not available this comes handy\n   add - adds gateway ip and subnet mask for device with a name\n    Example:\n     addr add 10.10.100.1/24 dev br10 - adds gateway with addres ...100.1 \n    and with subnet mask 255.255.255.0 to the device called br10\n    which is a network interface  Add persistant network interface:\n    /etc/network/interfaces\n    auto  \n    iface   inet static\n        address 10.10.100.1 #main address\n        netmask 255.255.255.0   #class C, like 10.10.100.1/24\n        bridge_ports dummy0 #some mock stuff\n        bridge_stp off\n        bridge fd 0         #frame 0\n====Ports checks\nread more:\nhttps://www.cyberciti.biz/faq/how-do-i-find-out-what-ports-are-listeningopen-on-my-linuxfreebsd-server/\nhttps://www.maketecheasier.com/check-open-ports-linux/  netstat:\n  find open ports\n  Find port on machine:  https://www.tecmint.com/find-open-ports-in-linux/\n   Params:\n  -l / --listen - displays ports that are in LISTEN state (lots of)\n  -vatn - opened ports and established TCP connections\n   - v - ?\n   - a - All - show all sockets\n   - t - TCP connections\n   - u - UDP connections\n   - n - short DNS hostname. will display dns names/ports w/o it\n   - p - show related PID and Program Name\n  Example:\n   netstat -vat - display used porta and established connections\n   sudo netstat -tulw - will display all ports for local machine\n  Check ports 9211, 8685 opened on local machine for tcp\\udp for IPv4 and IPv6:\n   netstat -lntu | egrep \"9211|8685\"  ss:\n  netstat could be outdated on some systems, use it instead\n   Example:\n  sudo ss -tulwn - display all opened ports on local machine  lsof:\n  also checks ports, could display processes that are using\n   port, and files used by the process  Params:\n  -i - displays processes and ports used, and connection\n    with friendly names\n   - 4 - ipv4\n   - tcp - display tcp (cant use 4 and tcp in the same time, it seems)  telnet:\n  check ports open\n  telnet    \n   Example:\n  telnet lwpeartifabld.tideworks.com 5000  nmap:\n  Network Mapped, detects open ports on my system\n   -s - Scan, there are lots of it\n    -sT - scan TCP\n    -sn - disable port scan, just 'ping'\n    -stuff, see 'man nmap'\n   -O - OS detection\n   -T - TCP\n   -U - UDP\n    Example:\n   sudo nmap -sT -O 192.168.42.74  - scan remote machine for opened ports\n   sudo nmap -sT -O localhost - scan local machine\n    Example:\n   nmap artifactory-dev.tideworks.com -p 5000\n     check port 5000 opened on 'artifactory-dev.tideworks.com' machine  NOTE: \n   Read MAN - it has examples and lots of stuff:\n    scanme.nmap.org/24 - will scan 256 IP in C network class of\n    scanme network - CRAZY!!111  Example:\n   nmap -sT -O localhost    - [s]cans [T]CP on localhost  nslookup:\n  looks for domain names by given IP\n  uses /etc/resolv.conf  - where all the DNS servers are\n   resolv.conf:\n  domain twlab.int\n  search customer.int usa.int tideworks.com\n  nameserver 172.19.65.2\n  nameserver 172.19.65.1\n  nameserver 10.253.1.1\n  nameserver 10.253.1.3  ==Security\nLSM: linux security module:\nhttps://www.kernel.org/doc/html/latest/admin-guide/LSM/index.html  Primary user of LSM is Mandatory Access Control(MAC) extensions  LSM loads a bit earlier than UserSpace, and controls user\niteraction with system through policies.\nLSMs exist to apply policies to actions taken by user space, so \nas long as the LSM infrastructure is running by the time user \nspace starts, everything is fine.\nLockdown, though, must act earlier: it needs to be able to \nblock the action of certain types of command-line parameters and \nmust be functional even before a security policy can be loaded. \nSo the patch set starts by creating a new type of \"early security \nmodule\" that is initialized toward the beginning of the boot process. \nAt this point, the module can't do much \u2014 even basic amenities like \nkmalloc() are not available \u2014 but it's enough to register its \nhooks and take control.  Loaded modules - MAC Extensions:\n cat /sys/kernel/security/lsm\n     \n   capability - loaded always\n   then 'minor' modules\n    yama\n   last 'major' module, ONE:\n    SELinux - OEL(RHEL?) OEL extension , might be custom\n    AppArmor - Ubuntu(Debian?) MAC Extension default\n     https://www.kernel.org/doc/html/latest/admin-guide/LSM/apparmor.html\n    Smack\n    Tomoyo\n  Process attributes for MAJOR module location:\n   /proc/.../attr\n    Subfolder with Module Name could be there\n    This is LEGACY for modules that provide subdirectories  AppArmor: apparmor:\ncentered around Tasks and Profiles for tasks\nProfiles are attached to Tasks\nTasks w/o profile controlled only be regular permissions wrx (DAC)  If AppArmor is not the default security module it can be enabled by passing \nsecurity=apparmor on the kernel\u2019s command line.\nIf AppArmor is the default security module it can be disabled by passing \napparmor=0, security=XXXX (where XXXX is valid security module), on the \nkernel\u2019s command line.\nFor AppArmor to enforce any restrictions beyond standard Linux DAC \npermissions policy must be loaded into the kernel from user space \n(see the Documentation and tools links).  Enable\\disable:\n  set CONFIG_SECURITY_APPARMOR=y  Set default:\n  CONFIG_DEFAULT_SECURITY=\"apparmor\"\n  CONFIG_SECURITY_APPARMOR_BOOTPARAM_VALUE=1  Wiki - http://wiki.apparmor.net\nUser space tools - https://gitlab.com/apparmor  Lockdown:\n  https://lwn.net/Articles/791863/\n   Together with UEFI locks who can access kernel\n   only Private Key owner with pair in UEFI\n   or certificate?  Lockdown is loaded much earlier than LSM and User Space, to prevent\ntricky cmd params from doing anything\n  Lockdown adds hooks at early-boot, throughh which operates   There are two lockdown modes. Confidentiality mode prevents user-land processes from extracting confidential information from the kernel. The other mode, Integrity, allows the kernel to switch off features that would allow user-land processes to modify the running kernel. Both of these modes even prevent processes launched by the root user or anyone with sudo privileges from modifying the kernel.  SELinux: selinux:  Statuses:\n1. Enforced : Actions contrary to the policy are blocked and a corresponding event is logged in the audit log.\n2. Permissive : Permissive mode loads the SELinux software, but doesn\u2019t enforce the rules, only logging is performed.\n3. Disabled : The SELinux is disabled entirely.  Read about status change and check:\nhttps://www.thegeekdiary.com/how-to-check-whether-selinux-is-enabled-or-disabled/  getenforce:\n  check status of selinux\n   Example:\n  # getenforce\n  Permissive  setenforce:\n  change enforce\n   Example:\n  # setenforce 0\n  # getenforce\n  Permissive  setstatus:\n  more detailed current status output  change to permissive and configuration info:\nhttps://www.thegeekdiary.com/how-to-disable-or-set-selinux-to-permissive-mode/  cat /etc/selinux/config\nSELINUX=disabled  # to disable  dot in permisisons:\ndrwxr-xr-x.    < this dot(.) means file has Access Control List(ACL) with SELinux.\nthis will not appear if SELinux is disabled  setfacl:\n will or unset ACL for files\\directories etc.  (the dot at the end of permisisons)\nhttps://stackoverflow.com/questions/30594871/what-does-the-dot-at-the-end-of-the-permissions-in-the-output-of-ls-lah-mean  ==Virtualization general\nVirtual box has several different network interfaces  NAT and NATNet networks would not be accessible from outside(and HOST)\nso to SSH into virtual machine, gotta use Port Forwarding \n  and connecto to localhost: \ntable:\n+-----------+-------------+-------------+----------------+----------------+\n|           | VM <-> Host | VM1 <-> VM2 | VM -> Internet | VM <- Internet |\n+-----------+-------------+-------------+----------------+----------------+\n| HostOnly  |     Yes     |     Yes     |      No        |       No       |\n| Internal  |     No      |     Yes     |      No        |       No       |\n| Bridged   |     Yes     |     Yes     |      Yes       |       Yes      |\n| NAT       |     No      |     No      |      Yes       |  Port forward  |\n| NATNet    |     No      |     Yes     |      Yes       |  Port forward  |\n+-----------+-------------+-------------+----------------+----------------+\nBridged will allow your guest to appear as just another PC on your host's network. The host, all the other network PCs, the internet and the guest can all communicate. The guest would default to getting an IP address from your host network's router. The host needs to be connected to an active network to allow guests to use Bridged. And Bridged is not always compatible with Wi-Fi.  Internal makes a private network just for your guests. There is no host communication or internet. There is no DCHP server on an internal network by default, though you can put one on in Virtualbox. Otherwise configure static IP address for the guests within the guest OS's. Or you can make a router guest using a router OS like pfSense. You can make more than one internal network by naming the new networks differently.  Host-Only is a special form of internal network that also includes the host, via a virtual Host-Only network adapter made on your host, defaulting to IP address 192.168.56.1, and there is a default DHCP server handing out IP addresses to the guests starting at 192.168.56.101. You can change the IP address range or make multiple host-only networks in Virtualbox's Network settings in the main GUI. There is no internet in Host-Only.  Virtualbox has two kinds of NAT: regular NAT and a NAT network service. Regular NAT (\"NAT\" in the dropdown) connects each guest to the host's network connection and internet via an independent channel. The host and any other guests cannot connect to the NAT-connected guest, although ports can be opened, just like NAT in a router. (The NAT guest can, however, see any services and shared folders on the host's network.)  The NAT network service (\"NAT network\" in the dropdown) is like a home router, NAT to the outside world and multiple \"LAN\" guest connections. All the guests connected to this \"NAT network\" can see and communicate with each other. Internet is accessible. Ports can be opened just like a real router. You need to make a new NAT network in the main Virtualbox window, File Menu, Preferences, Network. Then attach the guests using the name of the NAT network you made in Preferences.  ===GUI\nGUI on Centos7\nhttps://unix.stackexchange.com/questions/181503/how-to-install-desktop-environments-on-centos-7  GNOME install on Centos7\nyum group list\n  check groups available\n \"Server with GUI\" - this for RHEL\n \"GNOME Desktop\" this is GNOME for Centos\n \"KDE Plasma Workspaces\"  - this is KDE\n \"Graphical Administration Tools\" - extra admin tools for GUI\nyum groupinstall \"GNOME Desktop\" \"Graphical Administration Tools\"\n  this will install two groups for Gnome and admin tools\n  Read more:\n  https://www.itzgeek.com/how-tos/linux/centos-how-tos/install-gnome-gui-on-centos-7-rhel-7.html  gnome:\ngnome-control-center:\n  calls GUI of main settings window\n  gnome version could be viewed somewhere here\n  also version:\n  /usr/share/gnome/gnome-version.xml  gnome-tweaks:\n  calls GUI of advanced? settings window  utilities:\nclamav:\nclamscan:\n      ClamAV \u2013 A command line utility: It is a free, open source, and cross-platform antivirus toolkit for detecting many types of malicious software and viruses.\n    ClamTK \u2013 A Graphical utility: ClamTk is a graphical front-end for the Clam Antivirus. It is designed to be an easy-to-use, lightweight, on-demand antivirus scanner for Linux systems.\n   Install:\n  sudo apt-get install clamav clamav-daemon\n   Execute scan:\n   Example:\n  clamscan --invected --log=clamscan.log --recursive --copy=infected_files --detect-pua=yes /",
            "title": "server configuration"
        },
        {
            "location": "/networking_page/",
            "text": "DNS\n\n\nGuide\n\n\nDomain Name System - resolves human readable hostnames, such as www.example.com, into machine readable IP(Internet Protocol) addresses like 50.16.85.183.  \n\n\nAlso it is a directory of crucial information about domain names such as:  \n\n\n\n\nemail servers (MX records)  \n\n\nsending verification (DKIM, SPF, DMARC)  \n\n\nTXT record verification of domain ownership  \n\n\nSSH fingerprints (SSHFP)\n\n\n\n\nIntelligent DNS could even do something as Load Balancing - it could decide which IP(s) are returned after resolve.\n\n\nWork principles\n\n\nComputer sends requests every time it uses a domain address (example.com).\n\nThis happens every time domain name is used.\n\nFor any reason - web surfing, email, internet radio, API calls etc.\n\n\nSteps: In depth look for query\n\n\n\n\nLocal DNS cache. \n\n\n\n\nLocal computers(TODO: ensure), has \nDNS cache\n, where request could find hostname-IP resolution and this will be enough. If there is no local cache computer performs \nDNS query\n.\n\n\n\n\nRecursive DNS server. Resolver.\n\n\n\n\nUsually resolvers are somewhere in ISP's(internet service provider) network, but there could be even local resolver installed.\n\nResolvers also has their own cache, which is examined for query sent.\n\nIf hostname-IP is resolved(username checks out), this will be enough and results is sent back to the computer.\n\n\n\n\nRoot Name server. (.)\n\n\n\n\nResolver queries other resolvers(for cache) until it comes to the Root Name Servers, which are not cached already, so cache story ends here.\n\nThe Root Name servers forward query to the regular Name Servers, which know the address of the hostname - thus are able to do the resolution.\n\nThe Root name servers are located all around the world, this is kind of hubs on the traffic roads, usually Bunches located in huge cities like Capitals or just megacities.\n\nRoot Name servers are managed by \n13 companies\n such as ICANN or NASA.  \n\n\n\n\nTop Level Domain Name server. (.com. .us. .ua.)\n\n\n\n\nRoot server reads address from right to left, and depending on TLD in the address directs query to that Top Level Domain name server (Root NS knows addresses of all TLD Name servers).\n\nTLD NS knows reads the address queried, and reads next part like example in example.com, and this TLD NS knows address of every such domain (like \nexample\n.com).  \n\n\n\n\nAuthoritative DNS servers (Name Servers)\n\n\n\n\nAuthoritative name servers know everything about specific domain - it is stored in DNS records, such as A Record, CNAME and stuff.\nA Record is what we need, this will be returned.\n\n\n\n\nRetreive the record\n\n\n\n\nrecursive servers save request result from Name Server in its cache for amount of time set in TTL, after TTL expires recursive server will ask again to make sure info is up to date.\n\n\n\n\nReceive the anwser\n\n\n\n\nA record is returned back to the original asking computer by the Recursive server, answer is stored in local cache, reads IP address and passes it to browser. Browser opens the connection to receive the website.\n\n\nMAC addresses:\n\n\nhttps://www.howtogeek.com/169540/what-exactly-is-a-mac-address-used-for/\nhttp://www.linfo.org/mac_address.html\nhttps://www.youtube.com/watch?v=V2SpN-OePzc\n\n\nMAC is the uniq address of Network Interface Card.  \n\n\nMAC is lowest level in networking, below IP protocil.  \n\n\nMAC is used to send network packages, it is located in package header.  \n\n\nNIC will accept only those packages whose header contains NIC's MAC(matches).  \n\n\nRouting is done on IP level.\n\n\nWith MAC packages only could go inside the same network, if need to go through the Router the IP need to be used, wrapping package with MAC address.\n\n\nSteps:\n\n\nWHen computer wants to send a packet he checks whether target IP is in the same network. seems like ARP is used here, allows to ask all the computers in network (via Broadcast MAC ff:ff:ff:ff...) who has the IP address, only the computer with the IP will answer. \n\nAnswer will also contain the MAC address. This MAC will be written in the package and sent to that IP address (next hop). Info will be cached (as in DNS resolvers, seems like)\n\n\nIf IP is out of network, router receives the package with router's MAC in header and target IP address(not router's). Then Router check whether he could reach given target IP, if not, he sends package to next hot(router), and everything is done again.\n\n\nRouter receives packets for its own MAC, and for different(not router's) IP.\n\nThen checks whether he can directly reach the target IP, if not - passes to another router(hop)\n\n\nNAT  (Network Address Translation)\n\n\nhttp://www.internet-computer-security.com/Firewall/NAT.html\n\n\nThere are only 32 bits of addresses or 4 billions in IPv4.\n\nTo workaround it NAT is created, it introduces Private IP addresses.  \n\n\nPrivate IP addresses\n\n\nthere are 3 classes:\n\n\nClass A: 10.0.0.0 - 10.255.255.255\n\n\nClass B: 172.16.0.0 - 172.31.255.255\n\n\nClass C: 192.168.0.0 - 192.168.255.255 (or CIDR block \n192.168.0.0/16\n )\n\n\nWithin the private network - behind the Router(hop) the IP addresses would be unique. In another private network those addresses also would be unique, but could be the same in two different private networks.\n\nHowever as long as they stay private no conflicts appears.  \n\n\nRouters are the Gateways with real IP address purchased from the ISP (internet service provider). And this purchased address would be Public and unique.\n\nRouters are the 'real' sources for other services in the internet, and those services would address their packets only to Router, and Router will then translate (using routing tables) and substitute info in the package to send it to original sender inside the private network.\n\n\nNAT types:\n\n\nStatic NAT\n\n\nNAT device has a pool of Public ip addresses, and devices in network are assigned those addresses when accessing outside world. Static addresses will be given to the device permanently, so even when it goes offline the address still can not be taken by other device. It is useful for servers.\n\n\nDynamic NAT\n\n\nsame as above - pool of IP addresses which are given to the devices going outside, but when devices goes offline IP address is released and could be given to another device. When all the addresses are taken - new device could not be given any IP from the pool hence cant go to the internet.\n\n\nPort Address Translation (PAT)\n\n\nvideo\n\n\nPrivate address connects to a outer address with for instance:\n\n\n192.168.0.3:40213 connects to 40.30.20.10:80\n\n\n\nRouter substracts private ip and port and puts it in Routing Table\n\nand adds its own IP and another random port, changing the connection to:\n\n\n12.13.14.15:50098 connects to 40.30.20.10:80\n\n\n\nServer(or another same NAT router) receives connection, and answers to the Router:\n\n\n40.30.20.10:80 answers to 12.13.14.15:50098\n\n\n\nThen Router takes info from routing table, where matches router_address:random_port to the entry, and takes second pair with private_address:random_port\n\n\n40.30.20.10:80 answers to 192.168.0.3:40213\n\n\n\nSubnet Mask\n\n\nhttps://www.iplocation.net/subnet-mask\n\nhttps://en.wikipedia.org/wiki/Subnetwork  \n\n\ncalculator http://jodies.de/ipcalc\n\n\nProxy\n\n\nhttps://www.youtube.com/watch?v=0OukrSld3sY\n\n\n\n\nForward Proxy\n\n\nOr just Proxy, processes Outgoing requests. Computers in the network connects to it, and it connects to the servers.\n\nSee p4 proxy as example, with its own address, port - it accepts connections and does everything on its own - checks whether it needs to look furhter or not etc.\n\n\nUsual use:\n  - Content filtering:\n\n\n\n\ncensoring some addresses or resources(prevents traffic to coming in)\ntranslation some content before shipping it to computers inside network\n\n  - Caching (see p4 proxy)\n  - logging, monitoring  - what comes in, what comes out, etc.\n  - anonimization\n\nbecause not the computer bu proxy connects to server, it could ship different information\n\n\n\n\nReverse Proxy\n\n\nProcesses Incoming requests. Computers from outside world connects to the Proxy and not to the server itself, which is more secure, more stable(only 1 static address for outer world), more managable.\n\n\nUsual use:\n  - \nStable client endpoint\n whereas servers behind id could change addresses, cease to exist, turn on and off, scale and descale\n  - \nLoad balancing\n: Level 4 (udp/tcp traffic) & Level 8 (http traffic,with headers and other info for better balancing\n  - \nLoad balancing\n: server selection like Green\\Blue schamas or A\\B testing\n  - \nSSL Termination\n - when encryption ends on Proxy and internal traffic to servers and between internal servers is unencrypted and over http\n  - \nCaching\n - frequent requests could be cached w/o going to internal servers at all (during TTL ofc)\n  - \nAuthentication/validation\n - all the auth is done on the Proxy so no further auth is required.\n  - \nTenant throttling/billing\n - deny connection\\answer to computers that made too many requests per second. Or bill such customers, if we are charging by the amount\\frequency of requests done to the servers.\n  - \nDDoS mitigation\n - deny connection if suspicious about DDoS attack for some amount of time (1 hour or 1 day etc)\n\n\nReverse proxy examples:\n\n\nWebservers (such as Nginx)\n\nLoad Balancers\n\nAPI Gateways  \n\n\nMicroservices\n\n\nMonolith split into little parts which are independent one from each other.\nMicroservices have its own:\n\n - Data storage, and exclusive access to it for read and write\n - Technology stack, so different services could be written in different languages and use different storages and stuff  \n\n\nWhy to split:\n - Different technology stacks could be used for different services\n\n - Scalability - some services could be scaled to more instances than the others. Which means that less resources is needed comparing to split of monolith service, where some parts are overscaled and no need at the moment, but still scaled bcs of monolith structure\n\n - Different services(clients) could use and depend on one single service (see it as inheritance, when different classes could be inherited from single base class)\n - Versioning, some services could be upgraded with new featurs, but as long as they are backwards compatible other services could still use them w/o any additional changes\n - Conflicting version dependencies of shared libs in monolith services could be solved by splitting this monolith in microservices so different shared libs goes independent on different services\n\n\nAutoscaling instances\n\n\nOne of the holy grails Cloud apps is Autoscaling, which means changing amount of instances(of same service\\app) running at the same time.\n\n\nAutoscaling is performed automatically by 'orchestrator' - some cloud tool\n\nThere are couple of ways to autoscale:\n\n - Periodical Queue check: It has Queue which is in front of services, if amount of items in queue growth above some threshold for some period of time, the 'orchestrator' starts to scale instances up. When items in queue start to go down, 'orchestrator' will scale instances down.\n - Periodical Resource usage check: Instances could have some metrics being monitored by 'orchestrator' like CPU or RAM or HDD etc usage, and when those instances start to heat, some usage goes up, 'orchestrator' could decide to spin up another instance(scale up), and vice versa when resources goes low, 'orchestrator' could scale down some extra instances(keep n+1 amount anyway). \n\nLoad Balancer also could be used to make sure all the instances getting equialent load amount.\n - Scheduled: just some schedule related to day\\night, weekdays\\weekends\\holidays etc. As a downside - scheduled manner could not always be up to real world, and sometimes there could be too many or too less instances available\n\n\nMessaging\n\n\nMessaging communication \nreactivemanifesto.org\n\n\nBenefits over standard HTTP network calls (like GET/PUT/POST etc) which are converted from Method calls\n\n\nRequest\\Reply pattern Downsides:\n\n\n\n\n\n\nNetwork calls could be sent to a services(instance of a server) by load balancer, but particular service could already be busy doing work, and LB could not know about it.\n\nBut there could be other server instances that are not busy (and LB does not know about it)\n\n\n\n\n\n\nClient could go down - crash or scale down, while waiting for message request.  \n\n\n\n\n\n\nMessaging benefits:\n\n\n\n\n\n\nResource efficient - messages are taken from the queue by instance of a server which is done wokring, so all the instances have work and not overwhelmed with it. \n\n\n\n\n\n\nClient Services are not waiting longer than it could with Request-Reply pattern. So less chance to crash scale down before the answer.\n\n\n\n\n\n\nClient and Server are both talking to Queue service, which is always there and is solid - it wont go down crashing or descaling (actially it could be reverse proxy endpoint hiding queue brokers like zookeper one)\n\n\n\n\n\n\nResilient - if message from queue is taken and service which took it crashes - message will be returned to queue (in some magical way lol), and will be worked by with another service instance. \n\n\n\n\n\n\nQueue should implies idempotency (or Server rather?), when Queue could push out \nunordered\n messages multiple times, it is all need to be in Idempotent fashion.\n\n\n\n\n\n\nMessages are not lost even when consumer is fully offline. Messages will be stored in Queue until consumer comes back. This could be useful if consumer had a but, and was taken offline for fix, so new and fixed consumer now could handle all the messages.\n\n\n\n\n\n\nElastic - Orchestrator could use queue length to determine whether we need to scale  number of instances up or down.\n\n\n\n\n\n\nFault tolerant message processing\n\n\n\n\n\n\nService takes message from a queue. \n\n1.1 Message becomes invisible in the queue, but not deleted.\n\n1.2 Message becomes visible again in X seconds (amount of time expected to process the message)\n1.3 MessageDequeue counter is incremented, counter shows how many times message was dequeued(taken from the queue for processing)\n\n\n\n\n\n\nService takes message again if it is visible, and again increments counter. \n2.1 If counter is greater than threshold, it means the message is poisonous and we need to log it and delete it w/o processing, because it could crash the service(the reason why it has dequeue number over threshold)\n2.2 If message is processed fine the Service asks the Broker to delete the message.\n\n\n\n\n\n\nThis means that messages could be processed out of order(when retried), and could be even processed in parallel (if X seconds are not enough for processing at some point).\n\n\nWhich means that messages processing should be:  \n\n\n\n\nindependent (one message processed should not affect others)  \n\n\nidempotent (2+ message processings should have no ill effect)\n\n\n\n\nMessages features\n\n\n\n\n\n\nMultiple subscribers could receive one same message. So we need to send 1 message and 2+ subscribers could receive it.\n\n\n\n\n\n\nMessages could have TTL and when TTL passes message got deleted from the queue. This could save costs in cloud when we are billed for storing messages, and unprocessed messages could pile up (e.g. noone can process the message at the moment for some period of time)\n\n\n\n\n\n\nInvisibility timeout of message (see previous secion for X value).\n\nBut it should be taken carefully because in case of short X message could be processed second time \nin parallel\n\nAnd in case of too long X it could be invisible long time after unsuccessful processing being completed long ago\nAlso Service could increase or decrease the X which is stored in Broker.\n\n\n\n\n\n\nUpdate message in queue.\n\nIn case message processing is really long, and one Service processed it partially, it could go to Broker and update the message, so in case of crash second Service could not do that part of work which was done by the Service number 1. To do not waste the resources on double work. \nWhich still should be idempotent\n\n\n\n\n\n\nAt-most-once pattern\n\n\nThere is cases when message should be processed 1 or 0 times, it applies to data which is actual now and does not matter as time passes.\n\n\nThis could be accomplished by using TTL of the message, so it will be deleted by the Broker after the TTL expires, and TTL is the amount of time the data in the message \nmatters\n\nAdditionally to add 1 ir 0 times processing, time of message invisibility should be more than TTL, so when service takes message into processing, it will stay invisible long enough for Broker to delete it",
            "title": "Networking page"
        },
        {
            "location": "/networking_page/#dns",
            "text": "Guide  Domain Name System - resolves human readable hostnames, such as www.example.com, into machine readable IP(Internet Protocol) addresses like 50.16.85.183.    Also it is a directory of crucial information about domain names such as:     email servers (MX records)    sending verification (DKIM, SPF, DMARC)    TXT record verification of domain ownership    SSH fingerprints (SSHFP)   Intelligent DNS could even do something as Load Balancing - it could decide which IP(s) are returned after resolve.",
            "title": "DNS"
        },
        {
            "location": "/networking_page/#work-principles",
            "text": "Computer sends requests every time it uses a domain address (example.com). \nThis happens every time domain name is used. \nFor any reason - web surfing, email, internet radio, API calls etc.",
            "title": "Work principles"
        },
        {
            "location": "/networking_page/#steps-in-depth-look-for-query",
            "text": "Local DNS cache.    Local computers(TODO: ensure), has  DNS cache , where request could find hostname-IP resolution and this will be enough. If there is no local cache computer performs  DNS query .   Recursive DNS server. Resolver.   Usually resolvers are somewhere in ISP's(internet service provider) network, but there could be even local resolver installed. \nResolvers also has their own cache, which is examined for query sent. \nIf hostname-IP is resolved(username checks out), this will be enough and results is sent back to the computer.   Root Name server. (.)   Resolver queries other resolvers(for cache) until it comes to the Root Name Servers, which are not cached already, so cache story ends here. \nThe Root Name servers forward query to the regular Name Servers, which know the address of the hostname - thus are able to do the resolution. \nThe Root name servers are located all around the world, this is kind of hubs on the traffic roads, usually Bunches located in huge cities like Capitals or just megacities. \nRoot Name servers are managed by  13 companies  such as ICANN or NASA.     Top Level Domain Name server. (.com. .us. .ua.)   Root server reads address from right to left, and depending on TLD in the address directs query to that Top Level Domain name server (Root NS knows addresses of all TLD Name servers). \nTLD NS knows reads the address queried, and reads next part like example in example.com, and this TLD NS knows address of every such domain (like  example .com).     Authoritative DNS servers (Name Servers)   Authoritative name servers know everything about specific domain - it is stored in DNS records, such as A Record, CNAME and stuff.\nA Record is what we need, this will be returned.   Retreive the record   recursive servers save request result from Name Server in its cache for amount of time set in TTL, after TTL expires recursive server will ask again to make sure info is up to date.   Receive the anwser   A record is returned back to the original asking computer by the Recursive server, answer is stored in local cache, reads IP address and passes it to browser. Browser opens the connection to receive the website.",
            "title": "Steps: In depth look for query"
        },
        {
            "location": "/networking_page/#mac-addresses",
            "text": "https://www.howtogeek.com/169540/what-exactly-is-a-mac-address-used-for/\nhttp://www.linfo.org/mac_address.html\nhttps://www.youtube.com/watch?v=V2SpN-OePzc  MAC is the uniq address of Network Interface Card.    MAC is lowest level in networking, below IP protocil.    MAC is used to send network packages, it is located in package header.    NIC will accept only those packages whose header contains NIC's MAC(matches).    Routing is done on IP level.  With MAC packages only could go inside the same network, if need to go through the Router the IP need to be used, wrapping package with MAC address.",
            "title": "MAC addresses:"
        },
        {
            "location": "/networking_page/#steps",
            "text": "WHen computer wants to send a packet he checks whether target IP is in the same network. seems like ARP is used here, allows to ask all the computers in network (via Broadcast MAC ff:ff:ff:ff...) who has the IP address, only the computer with the IP will answer.  \nAnswer will also contain the MAC address. This MAC will be written in the package and sent to that IP address (next hop). Info will be cached (as in DNS resolvers, seems like)  If IP is out of network, router receives the package with router's MAC in header and target IP address(not router's). Then Router check whether he could reach given target IP, if not, he sends package to next hot(router), and everything is done again.  Router receives packets for its own MAC, and for different(not router's) IP. \nThen checks whether he can directly reach the target IP, if not - passes to another router(hop)",
            "title": "Steps:"
        },
        {
            "location": "/networking_page/#nat-network-address-translation",
            "text": "http://www.internet-computer-security.com/Firewall/NAT.html  There are only 32 bits of addresses or 4 billions in IPv4. \nTo workaround it NAT is created, it introduces Private IP addresses.",
            "title": "NAT  (Network Address Translation)"
        },
        {
            "location": "/networking_page/#private-ip-addresses",
            "text": "there are 3 classes:  Class A: 10.0.0.0 - 10.255.255.255  Class B: 172.16.0.0 - 172.31.255.255  Class C: 192.168.0.0 - 192.168.255.255 (or CIDR block  192.168.0.0/16  )  Within the private network - behind the Router(hop) the IP addresses would be unique. In another private network those addresses also would be unique, but could be the same in two different private networks. \nHowever as long as they stay private no conflicts appears.    Routers are the Gateways with real IP address purchased from the ISP (internet service provider). And this purchased address would be Public and unique. \nRouters are the 'real' sources for other services in the internet, and those services would address their packets only to Router, and Router will then translate (using routing tables) and substitute info in the package to send it to original sender inside the private network.",
            "title": "Private IP addresses"
        },
        {
            "location": "/networking_page/#nat-types",
            "text": "",
            "title": "NAT types:"
        },
        {
            "location": "/networking_page/#static-nat",
            "text": "NAT device has a pool of Public ip addresses, and devices in network are assigned those addresses when accessing outside world. Static addresses will be given to the device permanently, so even when it goes offline the address still can not be taken by other device. It is useful for servers.",
            "title": "Static NAT"
        },
        {
            "location": "/networking_page/#dynamic-nat",
            "text": "same as above - pool of IP addresses which are given to the devices going outside, but when devices goes offline IP address is released and could be given to another device. When all the addresses are taken - new device could not be given any IP from the pool hence cant go to the internet.",
            "title": "Dynamic NAT"
        },
        {
            "location": "/networking_page/#port-address-translation-pat",
            "text": "video  Private address connects to a outer address with for instance:  192.168.0.3:40213 connects to 40.30.20.10:80  Router substracts private ip and port and puts it in Routing Table \nand adds its own IP and another random port, changing the connection to:  12.13.14.15:50098 connects to 40.30.20.10:80  Server(or another same NAT router) receives connection, and answers to the Router:  40.30.20.10:80 answers to 12.13.14.15:50098  Then Router takes info from routing table, where matches router_address:random_port to the entry, and takes second pair with private_address:random_port  40.30.20.10:80 answers to 192.168.0.3:40213",
            "title": "Port Address Translation (PAT)"
        },
        {
            "location": "/networking_page/#subnet-mask",
            "text": "https://www.iplocation.net/subnet-mask \nhttps://en.wikipedia.org/wiki/Subnetwork    calculator http://jodies.de/ipcalc",
            "title": "Subnet Mask"
        },
        {
            "location": "/networking_page/#proxy",
            "text": "https://www.youtube.com/watch?v=0OukrSld3sY",
            "title": "Proxy"
        },
        {
            "location": "/networking_page/#forward-proxy",
            "text": "Or just Proxy, processes Outgoing requests. Computers in the network connects to it, and it connects to the servers. \nSee p4 proxy as example, with its own address, port - it accepts connections and does everything on its own - checks whether it needs to look furhter or not etc.  Usual use:\n  - Content filtering:   censoring some addresses or resources(prevents traffic to coming in)\ntranslation some content before shipping it to computers inside network \n  - Caching (see p4 proxy)\n  - logging, monitoring  - what comes in, what comes out, etc.\n  - anonimization \nbecause not the computer bu proxy connects to server, it could ship different information",
            "title": "Forward Proxy"
        },
        {
            "location": "/networking_page/#reverse-proxy",
            "text": "Processes Incoming requests. Computers from outside world connects to the Proxy and not to the server itself, which is more secure, more stable(only 1 static address for outer world), more managable.  Usual use:\n  -  Stable client endpoint  whereas servers behind id could change addresses, cease to exist, turn on and off, scale and descale\n  -  Load balancing : Level 4 (udp/tcp traffic) & Level 8 (http traffic,with headers and other info for better balancing\n  -  Load balancing : server selection like Green\\Blue schamas or A\\B testing\n  -  SSL Termination  - when encryption ends on Proxy and internal traffic to servers and between internal servers is unencrypted and over http\n  -  Caching  - frequent requests could be cached w/o going to internal servers at all (during TTL ofc)\n  -  Authentication/validation  - all the auth is done on the Proxy so no further auth is required.\n  -  Tenant throttling/billing  - deny connection\\answer to computers that made too many requests per second. Or bill such customers, if we are charging by the amount\\frequency of requests done to the servers.\n  -  DDoS mitigation  - deny connection if suspicious about DDoS attack for some amount of time (1 hour or 1 day etc)  Reverse proxy examples:  Webservers (such as Nginx) \nLoad Balancers \nAPI Gateways",
            "title": "Reverse Proxy"
        },
        {
            "location": "/networking_page/#microservices",
            "text": "Monolith split into little parts which are independent one from each other.\nMicroservices have its own: \n - Data storage, and exclusive access to it for read and write\n - Technology stack, so different services could be written in different languages and use different storages and stuff    Why to split:\n - Different technology stacks could be used for different services \n - Scalability - some services could be scaled to more instances than the others. Which means that less resources is needed comparing to split of monolith service, where some parts are overscaled and no need at the moment, but still scaled bcs of monolith structure \n - Different services(clients) could use and depend on one single service (see it as inheritance, when different classes could be inherited from single base class)\n - Versioning, some services could be upgraded with new featurs, but as long as they are backwards compatible other services could still use them w/o any additional changes\n - Conflicting version dependencies of shared libs in monolith services could be solved by splitting this monolith in microservices so different shared libs goes independent on different services",
            "title": "Microservices"
        },
        {
            "location": "/networking_page/#autoscaling-instances",
            "text": "One of the holy grails Cloud apps is Autoscaling, which means changing amount of instances(of same service\\app) running at the same time.  Autoscaling is performed automatically by 'orchestrator' - some cloud tool \nThere are couple of ways to autoscale: \n - Periodical Queue check: It has Queue which is in front of services, if amount of items in queue growth above some threshold for some period of time, the 'orchestrator' starts to scale instances up. When items in queue start to go down, 'orchestrator' will scale instances down.\n - Periodical Resource usage check: Instances could have some metrics being monitored by 'orchestrator' like CPU or RAM or HDD etc usage, and when those instances start to heat, some usage goes up, 'orchestrator' could decide to spin up another instance(scale up), and vice versa when resources goes low, 'orchestrator' could scale down some extra instances(keep n+1 amount anyway).  \nLoad Balancer also could be used to make sure all the instances getting equialent load amount.\n - Scheduled: just some schedule related to day\\night, weekdays\\weekends\\holidays etc. As a downside - scheduled manner could not always be up to real world, and sometimes there could be too many or too less instances available",
            "title": "Autoscaling instances"
        },
        {
            "location": "/networking_page/#messaging",
            "text": "Messaging communication  reactivemanifesto.org  Benefits over standard HTTP network calls (like GET/PUT/POST etc) which are converted from Method calls  Request\\Reply pattern Downsides:    Network calls could be sent to a services(instance of a server) by load balancer, but particular service could already be busy doing work, and LB could not know about it. \nBut there could be other server instances that are not busy (and LB does not know about it)    Client could go down - crash or scale down, while waiting for message request.      Messaging benefits:    Resource efficient - messages are taken from the queue by instance of a server which is done wokring, so all the instances have work and not overwhelmed with it.     Client Services are not waiting longer than it could with Request-Reply pattern. So less chance to crash scale down before the answer.    Client and Server are both talking to Queue service, which is always there and is solid - it wont go down crashing or descaling (actially it could be reverse proxy endpoint hiding queue brokers like zookeper one)    Resilient - if message from queue is taken and service which took it crashes - message will be returned to queue (in some magical way lol), and will be worked by with another service instance.     Queue should implies idempotency (or Server rather?), when Queue could push out  unordered  messages multiple times, it is all need to be in Idempotent fashion.    Messages are not lost even when consumer is fully offline. Messages will be stored in Queue until consumer comes back. This could be useful if consumer had a but, and was taken offline for fix, so new and fixed consumer now could handle all the messages.    Elastic - Orchestrator could use queue length to determine whether we need to scale  number of instances up or down.",
            "title": "Messaging"
        },
        {
            "location": "/networking_page/#fault-tolerant-message-processing",
            "text": "Service takes message from a queue.  \n1.1 Message becomes invisible in the queue, but not deleted. \n1.2 Message becomes visible again in X seconds (amount of time expected to process the message)\n1.3 MessageDequeue counter is incremented, counter shows how many times message was dequeued(taken from the queue for processing)    Service takes message again if it is visible, and again increments counter. \n2.1 If counter is greater than threshold, it means the message is poisonous and we need to log it and delete it w/o processing, because it could crash the service(the reason why it has dequeue number over threshold)\n2.2 If message is processed fine the Service asks the Broker to delete the message.    This means that messages could be processed out of order(when retried), and could be even processed in parallel (if X seconds are not enough for processing at some point).  Which means that messages processing should be:     independent (one message processed should not affect others)    idempotent (2+ message processings should have no ill effect)",
            "title": "Fault tolerant message processing"
        },
        {
            "location": "/networking_page/#messages-features",
            "text": "Multiple subscribers could receive one same message. So we need to send 1 message and 2+ subscribers could receive it.    Messages could have TTL and when TTL passes message got deleted from the queue. This could save costs in cloud when we are billed for storing messages, and unprocessed messages could pile up (e.g. noone can process the message at the moment for some period of time)    Invisibility timeout of message (see previous secion for X value). \nBut it should be taken carefully because in case of short X message could be processed second time  in parallel \nAnd in case of too long X it could be invisible long time after unsuccessful processing being completed long ago\nAlso Service could increase or decrease the X which is stored in Broker.    Update message in queue. \nIn case message processing is really long, and one Service processed it partially, it could go to Broker and update the message, so in case of crash second Service could not do that part of work which was done by the Service number 1. To do not waste the resources on double work.  Which still should be idempotent",
            "title": "Messages features"
        },
        {
            "location": "/networking_page/#at-most-once-pattern",
            "text": "There is cases when message should be processed 1 or 0 times, it applies to data which is actual now and does not matter as time passes.  This could be accomplished by using TTL of the message, so it will be deleted by the Broker after the TTL expires, and TTL is the amount of time the data in the message  matters \nAdditionally to add 1 ir 0 times processing, time of message invisibility should be more than TTL, so when service takes message into processing, it will stay invisible long enough for Broker to delete it",
            "title": "At-most-once pattern"
        }
    ]
}