{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to MkDocs\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n\n\n\n12 factor apps\n\n\n12factor.net\n\n\n8 Fallacies of Distributed Systems\n\n\nUnderstanding the 8 Fallacies of Distributed Systems \n\n\noriginal pdf\n\n\nThe network is reliable\n\n\nit is not, and packets could be lost, so there is need of Error handling\n\nand retries after the packet lost and error occured\\connection loss\n\n\nLatency is zero\n\n\nlatency is a thing, and computers are not close to each other. Sometimes things could be mirrored in different regions.\n\nNumber of network requests need to be as little as possible, to minimize latency timings - the less requests goes through the network with unknown latency, the faster overall things would be\n\n\nBadnwidth is infinite\n\n\nBandwidth (channel width) is limited, and huge amount of data will take longer to transfer.\n\nAlso when service gets requests from lots of clients, some clients could send lots of requests which will be transformed in network being to busy, so other clients will get no space.\n\nIn this case some throtelling should be applied, and such clients at some moment need to be denied, so there will be network space for others to made a request.\nSend small payloads - do not send huge packets with requests, send small ones so they will get through physicall wire faster, kind of\n\n\nThe network is secure\n\n\nNetwork inside even a single cluster is Insecure, so even the services located in secured network should use authentication.\n\nEspecially in case when services are running some 3rd party code or libraries or stuff like that, which could send some requests over the network, when they should not be allowed to.\n\n\nTopology does not change\n\n\nOrchestrator (tool which will keep all the services ap and running, restarted and scaled) could change topology - some services could be spawned eleswhere, on the different pc, rack or even availability zone, this will affect the Topology\n\n\nThere is one administrator\n\n\nThere are lots of administrators - people and automated tools\\scripts, so something that is considered by you as Immutable (eg. IP:port) could be changed by someone w/o your knowlege.\n\nThings like that need to be monitored, and all the updates should be visible and all the notifications or even automated changes should be done.\n\n\nTransport cost is zero\n\n\nUsually transporting traffic in and out of datacenter costs a performance.\n\nAlso transport could cost money, as datacenters could charge users for that metric\n\n\nThe network is homogeneous\n\n\nThe network could consist of different hardware used such as routers, switches(manufacturer or model or age) and even the wire - fibre or copper etc.\n\nThis could be applied to datacenter itself, or segment between datacenters where user's services are mirrorred, or anywhere on the way\n\n\nMicroservices\n\n\nTo use a mircoservices we need to convert usual methods into network calls,\n\n together with parameters of the methods, which also need to be [de]serializable\n\nand result , returned by the method also need to be serializable.\n\n\nvar result = Method(arg1, arg2);\n\n\n\n\n\nresult\n and \nargs\n need to be serialized and deserialized byt client and server\n\nMethod need to be Explicit, Language agnostic, multiversion API contract.   \n\n\n\n\nWhich means name would be used on different sides(server and client), in different languages, and whole the signature will require lots of changes on both sides.  \n\n\nAlso the method signature will not be shown in IntelliSense anymore because it is not in referenced library anymore, it is just simple Network call (which could be wrapped into library but this will require extra work)\n\n\nSerializable narrows amount of Types we could use (only serializable ones),\n\n and the process of serialization of language-specific object into serializable data type, such as JSON, XML, Avro, YAML, so on, takes CPU and RAM\n\n\nSometimes serialization of types, especially if they support reflection, could use 90+% of the resources used by the service\\app.\n\n\nInprocess method call vs Network request\n\n\n\n\n\n\nPerformance - Worse:\n\nIncreases network congestion, means that it adds to network traffic\n\nUnpredictable timing, in-process call could take microseconds, and Network call could take seconds and could even fail on timeout - need to expect it and be ready to handle.\n\n\n\n\n\n\nUnreliable:\n\nRequires retries, timeouts and circuit breakers.\n\nCircuit breaker is when client tries to communicate with a server, and if it fails to connect\\communicate for X consecutive times(e.g. 5) it will no longer try for Y amount of time, or otherwise it will became a DDoS attacker of own server. This loop(try 5 times, wait 1 minute) will continute until server is available again - then circuit breaker will exit and allow traffic to move on.\n\nCircuit breaker - client logic mostly\n\nBut server also need to count whether it answered, and if yes - do not send same answers for requests made because of circuit breaker still not exited and sending more requests, that already have been answered\n\n\n\n\n\n\nSecurity:\n\nWhen method was in a single process, there was no need of security, authentication and encryption.\n\nBut now Parameters and Results are sent over the network, those need to be encrypted\n\nNow anyone could call this Method, which become Network call, so only authorized clients need to be served\n\n\n\n\n\n\nDiagnostics:\n\nCalls are now are done over the network, instead of being in-process before, so there is latency now.\n\nEvents and logs are now generated by all the tools and apps, like web servers, proxies, encryptors, virtual machines.\n\nCall stack is now split over the network and potentially lots of machines, you cant just view it in Debug mode like before, or go to Method implementation by F12(F11 in VS?)\n\nClocks are not fully synchorized, use UTC in the cluster. \n\nBut even with same UTC some machines could slow down or pace too fast, so there could be situation when Client sent a request at 00.01 by clien's time and server received it by 23.59 by server's time.\n\nSo during logs examination this need to be taken into account.\n\n\n\n\n\n\nIdempotent operation\n\n\nOperation which could be performed 2+ times witn no ill effect.\n\n\nExample:\n\n\n\n\nMethod call to the server, with an image as a parameter, and the server produces thumbnail(preview of the image) and returns it to the client.\n\nThis oparation could be performed 2+ times with the same exact image and the server will return the same exact thumbnail every time. Without any \nill\n effect.\n\n\n\n\nServices MUST implement operations idempotently\n\n\nServers need to make sure that operation is idempotent.\n\n\nExample:\n\n\n\n\nMethod adds $100 to an account by creating a request, it does this in a circuit breaker loop, but after server accepts the call, method sends another one. If server will accept all such calls - account will have more than $100.\n\nSo server should check whether the call was processed, and if so - deny all the same calls being done afterwards.\n\nWhich will be a State corruption.\n\n\n\n\nSo the Client need to send the requests in loop manner, to make sure message is sent and received.\n\n\nThe Server need to make sure idempotentcy - so every request received from the Client will be idempotent - Client could call method 2+ times \nwith no ill effect\n\n\nIdempotant CRUD operations\n\n\n\n\n\n\n\n\n\n\nOperation\n\n\nHTTP Verb\n\n\nWhat to do\n\n\n\n\n\n\n\n\n\n\nC\n\n\nid = Create()\n\n\nPOST\n\n\nsee pattern below\n\n\n\n\n\n\nR\n\n\ndata = Read(id)\n\n\nGET/HEAD/OPTIONS/TRACE\n\n\nNaturally idempotent\n\n\n\n\n\n\nU\n\n\nUpdate(id, data)\n\n\nPUT\n\n\nLast writer wins\n\n\n\n\n\n\nD\n\n\nDelete(id)\n\n\nDELETE\n\n\nif already done OK\n\n\n\n\n\n\n\n\nIdempotency pattern:\n\n\n\n\n\n\nClient: asks server to create unique ID \nor\n client(\ntrusted\n) creates ab ID\n\n\n\n\n\n\nsome unique ID like GUID is created by the server on clients request, this is more trustfull way, but adds network latency.\n\nClient could create ID on its own, w/o network communication, but it needs to be Unique, so Server need to \nTrust\n the client on that matter.\n\nThis could be retried, but only last ID matters, all other previous IDs should be thrown away\n\n\n\n\n\n\nClient: sends ID & desired operation to server\n\n\n\n\n\n\nClient then sends POST asking Server to add $100 to the account.\n\nThis also could be retried, as we are talking about idemtotency here\n\n\n\n\n\n\nServer: if ID is new, do the operation and log the ID, respond OK\n\n\n\n\n\n\nMust be transactioned(atomic) operation\n\nThis is important to keep idempotency: if Client sends operation +$100, and Server checked in the Log for the ID, start doing the operation, and Client sends the retry - Server will start 2nd operation, because 1st is still ongoing and ID is not yet logged.\n\nOn the other hand if Search and then Log the ID, server could crash on the execution of the operation, and will not perform the operation at all.\n\nSo the atomic 'Search-for-ID; Perform-operation; Log-the-ID' operation is important. Otherwise it will not guaranteed to be \nIdempotent\n.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-mkdocs",
            "text": "For full documentation visit  mkdocs.org .",
            "title": "Welcome to MkDocs"
        },
        {
            "location": "/#commands",
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.",
            "title": "Commands"
        },
        {
            "location": "/#project-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "Project layout"
        },
        {
            "location": "/#12-factor-apps",
            "text": "12factor.net",
            "title": "12 factor apps"
        },
        {
            "location": "/#8-fallacies-of-distributed-systems",
            "text": "Understanding the 8 Fallacies of Distributed Systems   original pdf",
            "title": "8 Fallacies of Distributed Systems"
        },
        {
            "location": "/#the-network-is-reliable",
            "text": "it is not, and packets could be lost, so there is need of Error handling \nand retries after the packet lost and error occured\\connection loss",
            "title": "The network is reliable"
        },
        {
            "location": "/#latency-is-zero",
            "text": "latency is a thing, and computers are not close to each other. Sometimes things could be mirrored in different regions. \nNumber of network requests need to be as little as possible, to minimize latency timings - the less requests goes through the network with unknown latency, the faster overall things would be",
            "title": "Latency is zero"
        },
        {
            "location": "/#badnwidth-is-infinite",
            "text": "Bandwidth (channel width) is limited, and huge amount of data will take longer to transfer. \nAlso when service gets requests from lots of clients, some clients could send lots of requests which will be transformed in network being to busy, so other clients will get no space. \nIn this case some throtelling should be applied, and such clients at some moment need to be denied, so there will be network space for others to made a request.\nSend small payloads - do not send huge packets with requests, send small ones so they will get through physicall wire faster, kind of",
            "title": "Badnwidth is infinite"
        },
        {
            "location": "/#the-network-is-secure",
            "text": "Network inside even a single cluster is Insecure, so even the services located in secured network should use authentication. \nEspecially in case when services are running some 3rd party code or libraries or stuff like that, which could send some requests over the network, when they should not be allowed to.",
            "title": "The network is secure"
        },
        {
            "location": "/#topology-does-not-change",
            "text": "Orchestrator (tool which will keep all the services ap and running, restarted and scaled) could change topology - some services could be spawned eleswhere, on the different pc, rack or even availability zone, this will affect the Topology",
            "title": "Topology does not change"
        },
        {
            "location": "/#there-is-one-administrator",
            "text": "There are lots of administrators - people and automated tools\\scripts, so something that is considered by you as Immutable (eg. IP:port) could be changed by someone w/o your knowlege. \nThings like that need to be monitored, and all the updates should be visible and all the notifications or even automated changes should be done.",
            "title": "There is one administrator"
        },
        {
            "location": "/#transport-cost-is-zero",
            "text": "Usually transporting traffic in and out of datacenter costs a performance. \nAlso transport could cost money, as datacenters could charge users for that metric",
            "title": "Transport cost is zero"
        },
        {
            "location": "/#the-network-is-homogeneous",
            "text": "The network could consist of different hardware used such as routers, switches(manufacturer or model or age) and even the wire - fibre or copper etc. \nThis could be applied to datacenter itself, or segment between datacenters where user's services are mirrorred, or anywhere on the way",
            "title": "The network is homogeneous"
        },
        {
            "location": "/#microservices",
            "text": "To use a mircoservices we need to convert usual methods into network calls, \n together with parameters of the methods, which also need to be [de]serializable \nand result , returned by the method also need to be serializable.  var result = Method(arg1, arg2);   result  and  args  need to be serialized and deserialized byt client and server \nMethod need to be Explicit, Language agnostic, multiversion API contract.      Which means name would be used on different sides(server and client), in different languages, and whole the signature will require lots of changes on both sides.    Also the method signature will not be shown in IntelliSense anymore because it is not in referenced library anymore, it is just simple Network call (which could be wrapped into library but this will require extra work)  Serializable narrows amount of Types we could use (only serializable ones), \n and the process of serialization of language-specific object into serializable data type, such as JSON, XML, Avro, YAML, so on, takes CPU and RAM  Sometimes serialization of types, especially if they support reflection, could use 90+% of the resources used by the service\\app.",
            "title": "Microservices"
        },
        {
            "location": "/#inprocess-method-call-vs-network-request",
            "text": "Performance - Worse: \nIncreases network congestion, means that it adds to network traffic \nUnpredictable timing, in-process call could take microseconds, and Network call could take seconds and could even fail on timeout - need to expect it and be ready to handle.    Unreliable: \nRequires retries, timeouts and circuit breakers. \nCircuit breaker is when client tries to communicate with a server, and if it fails to connect\\communicate for X consecutive times(e.g. 5) it will no longer try for Y amount of time, or otherwise it will became a DDoS attacker of own server. This loop(try 5 times, wait 1 minute) will continute until server is available again - then circuit breaker will exit and allow traffic to move on. \nCircuit breaker - client logic mostly \nBut server also need to count whether it answered, and if yes - do not send same answers for requests made because of circuit breaker still not exited and sending more requests, that already have been answered    Security: \nWhen method was in a single process, there was no need of security, authentication and encryption. \nBut now Parameters and Results are sent over the network, those need to be encrypted \nNow anyone could call this Method, which become Network call, so only authorized clients need to be served    Diagnostics: \nCalls are now are done over the network, instead of being in-process before, so there is latency now. \nEvents and logs are now generated by all the tools and apps, like web servers, proxies, encryptors, virtual machines. \nCall stack is now split over the network and potentially lots of machines, you cant just view it in Debug mode like before, or go to Method implementation by F12(F11 in VS?) \nClocks are not fully synchorized, use UTC in the cluster.  \nBut even with same UTC some machines could slow down or pace too fast, so there could be situation when Client sent a request at 00.01 by clien's time and server received it by 23.59 by server's time. \nSo during logs examination this need to be taken into account.",
            "title": "Inprocess method call vs Network request"
        },
        {
            "location": "/#idempotent-operation",
            "text": "Operation which could be performed 2+ times witn no ill effect.  Example:   Method call to the server, with an image as a parameter, and the server produces thumbnail(preview of the image) and returns it to the client. \nThis oparation could be performed 2+ times with the same exact image and the server will return the same exact thumbnail every time. Without any  ill  effect.   Services MUST implement operations idempotently  Servers need to make sure that operation is idempotent.  Example:   Method adds $100 to an account by creating a request, it does this in a circuit breaker loop, but after server accepts the call, method sends another one. If server will accept all such calls - account will have more than $100. \nSo server should check whether the call was processed, and if so - deny all the same calls being done afterwards. \nWhich will be a State corruption.   So the Client need to send the requests in loop manner, to make sure message is sent and received.  The Server need to make sure idempotentcy - so every request received from the Client will be idempotent - Client could call method 2+ times  with no ill effect",
            "title": "Idempotent operation"
        },
        {
            "location": "/#idempotant-crud-operations",
            "text": "Operation  HTTP Verb  What to do      C  id = Create()  POST  see pattern below    R  data = Read(id)  GET/HEAD/OPTIONS/TRACE  Naturally idempotent    U  Update(id, data)  PUT  Last writer wins    D  Delete(id)  DELETE  if already done OK     Idempotency pattern:    Client: asks server to create unique ID  or  client( trusted ) creates ab ID    some unique ID like GUID is created by the server on clients request, this is more trustfull way, but adds network latency. \nClient could create ID on its own, w/o network communication, but it needs to be Unique, so Server need to  Trust  the client on that matter. \nThis could be retried, but only last ID matters, all other previous IDs should be thrown away    Client: sends ID & desired operation to server    Client then sends POST asking Server to add $100 to the account. \nThis also could be retried, as we are talking about idemtotency here    Server: if ID is new, do the operation and log the ID, respond OK    Must be transactioned(atomic) operation \nThis is important to keep idempotency: if Client sends operation +$100, and Server checked in the Log for the ID, start doing the operation, and Client sends the retry - Server will start 2nd operation, because 1st is still ongoing and ID is not yet logged. \nOn the other hand if Search and then Log the ID, server could crash on the execution of the operation, and will not perform the operation at all. \nSo the atomic 'Search-for-ID; Perform-operation; Log-the-ID' operation is important. Otherwise it will not guaranteed to be  Idempotent .",
            "title": "Idempotant CRUD operations"
        },
        {
            "location": "/bash_page/",
            "text": "Intro\n\n\nscripts start from instruction which interpreter to use, first line always must\nbe the directive. Except if script is passed as param into interpreter, then\nthere is no need in that because there already an interpreter in place.\n\n\n#!/path/to/interpreter\n\n\n\ni.e.:\n\n\n#!/bin/bash\n\n\n\nShortcuts:\n  ctrl+a - move to start of line\n  ctrl+e - move to end of line\n  ctrl+b - move one char back\n  alt+b  - move one word back\n  ctrl+f - move one char forth\n  alt+b  - move one word forth\n  ctrl+d - delete one char under cursor\n  ctrl+u - delete from cursor to line start\n  ctrl+k - delete from cursor to line end\n  ctrl+w - delete from cursor to word start\n  alt+bcksp - delete previous word\n  ctrl+y - paste from clipboard(alt+bckspc deleted)\n  ctrl+l - clean screen\n  ctrl+r - reverse search in history\n  ctrl+j - edit found command in search\n  ctrl+p - previous command in history\n  ctrl+n - next command in history\n\n\nBash Scripts accepts data from stdin and its direct redirection or pipe\n\nNote: 'read' command need to be used.  \n\n\nExample: \n\n\n>script:  \n>read -p \"enter any *.txt\"  \n>echo \"${PWD}${REPLY}\"  \n$cd ~ && ls test.txt | ./read_and_expect_txt_files.sh  \n/home/dos/test.txt\n\n\n\nExample:  \n\n\n$echo \"test.txt\" > try.txt  \n$./read_and_expect_txt_files.sh < try.txt  \n/home/dos/test.txt\n\n\n\nDebugging:\n\n\n#!/bin/bash -x\n\n\n\nwill start debugging tracert - with display of all values expanded and\n  lines as bash sees them and will execute them\n\n\nset -x  \necho $var  \nset +x\n\n\n\nwill enable debugging tracert only between set -/+ x displaying only\n commands in between with such debug info\n\n\nshell variables:\n\n\n$FUNCNAME - contains name of the function being executed(like reflection)  \n\n\n$PS4 env var, is Promt String for debugging tracert\n\n$LINENO - is standard variable with line number\n\nPS4=\"$LINENO +\" - will display line number during debugging tracert  \n\n\n$OLDPWD - contains previous workind directory\n\n$RANDOM - contains random number from 1 to 32 767\n\n$$ - current PID of the programs\\script being executed  \n\n\nDefensive programming:\n\n\nMake sure everything is expanded and is executed in right place:\n\n Instead of:  \n\n\n$cd $path  \n$rm *\n\n\n\nUse secure defensive way:  \n\n\n[[ -d \"$path\" ]] && cd \"$path\" && echo rm *\n\n\n\nwhere:  \n\n\n\n\n\n\n-d - make sure path exists and is Directory  \n\n\n\"$path\" - in double quotes to avoid null expansion if var does not exist\n    it will be expanded in empty string in that way - \"\"  \n\n\n&& - will execute next command if previous returned 0 exit code  \n\n\necho rm * - will echo expanded by bash shell wildcard, displaying what\n\n    exactly is going to happen with actual 'rm *'  \n\n\n\n\n\n\nNaming Coventions:\n\n\n\n\nVariable Names:\n\n Lower-case, with underscores to separate words.\n\n Ex: my_variable_name  \n\n\nConstants and Environment Variable Names:\n\n All caps, separated with underscores, declared at the top of the file.\n\n Ex: MY_CONSTANT  \n\n\nGoogle naming convetions:  \n\n    https://google.github.io/styleguide/shell.xml#Naming_Conventions\n\n\n\n\nExecution:\n\n\nbash searches for entered command in \n\n\n\n\nbuilt-ins\n\n\nhashes\n\n\nbash history \n\n\nenv var PATH\n\n   4.1. PATH is split by ':' and every path is searched for the command  \n\n\nnew child process of bash created - fork(), and all ENV copied into it\n\n\nfound command is executed in the child copy of bash and replaces it - execve()  \n\n\nmain bash process executes wait() until the child process ends\n\n\nafter child process ends cleanup starts and destroys the process \n\n\n\n\nSo if script/binary is executed from some(i.e. current) directory and it is\n  not in path , it have to be 'source'd like:  \n\n\n source scriptname\n\n\n\n\n\nor:  \n\n\n\n\n . scriptname\n\n\n\n. is same to source    \n\n\n\n\nor:  \n\n\n\n\n ./scriptname\n\n\n\nthis seems to be \n/scriptname (so basically full path, which is also applies)  \n\n\nExpansion:\n\n\nExpansion is done by shell(i.e. bash) and expanded result is passed to shell\\command to be executed. If something is escaped by escape sequence, it will not be expanded by shell, and \ncould\n be expanded by the program receiving escaped sequence.\n\n expansion works before pass anything further, so:  \n\n\n cp * /some/dir\n\n\n\nwill copy everything from current dir into /some/dir\n\n  first argument * actually will be passed to cp as list of files matched \n  by wildcard, 'cp' will never see * itself\n\n  to check what * will return, execute \necho *\n  \n\n\nvariables expansion:\n\n\nExpansion starts when $ is encountered.\n\n  Escape sequences:\n\n  \\ - will escape 1 symbol to treat it as literal and not meta symbol\n\n  '' - everything between single quotes threated as literals too\n\n  \"\" - will expand variables, but escape spaces etc.\n\n   Variables are expanded before passing them to command so command will only\n  see var's value, not its name.\n\n   Bash creates variable when first mets it, so it will create even empty\n  variables, which could lead to errors:  \n\n\nfoo=\"/some/file\"  \nmv $foo $foo1  # error\n\n\n\n\n\nBash will create $foo1 variable , then mv will receive $foo1 value which\n  is null, and 'mv /some/file \n' will get error as only 1 param is passed\n\n   To rename file to \n'some/file1'\n use \n{}\n:  \n\n\n\n\n    mv $foo {$foo}1\n\n\n\nPositional parameters:\n\n\nBash supports positional params for the script.\n\n  default ones are $0-$9 , other params are listed by ${10} and so on:  \n\n\n\n\n\n\n$0 ---- contains full path of the script being executed\n\n----    does not count as parameter in list $#\n\n----    so [ $# -gt 0 ] will return false if $1 is empty  \n\n\n\n\n\n\n$1+  ---- are actual parameters  \n\n\n\n\n\n\nParameters are passed as usual\n\n\nExample:  \n\n\nscript.sh param1 param2\n\n\n\nin script.sh:  \n\n\necho $0  #/path/to/script.sh\necho $1  #param1\necho $2  #$param2\necho $3  # this would be empty uninitialized param with null value\n\n\n\nParaters could be passed using wildcards like\n\n\nExample:  \n\n\nscript.sh *\n\n\n\nwill pass everything matched, basically it would be a\n        list of files in PWD (same as \necho *\n)\n\n\nParams Arrays:\n\n\n\n\n$# - \n.Count\n, stores number of arguments passed into the script\n\n    does \nnot count $0\n in it, so is not 0 based\n\n\n$* - stores all arguments - ($1 $2 ...), if args has spaces those\n\n    would be treated as separate params, so params \n\n    (\"param1\" \"param with spaces\") would be\n\n\n$1=param1 $2=param $3=with $4=spaces\n  \n\n\n\"$*\" - all params expanded between (\"$1 $2\" ...), with delimiter of first\n    symbol in $IFS variable, it becomes one single parameter:\n\n    (\"param1\" \"param with spaces\") would be\n\n\n$1=\"param1 param with spaces\"\n (leaving quotes behind in the end)  \n\n\n$@ - same as $*  \n\n\n\"$@\" - stores all arguments in quotas (\"$1\" \"$2\" ...), thus handling \n    spaces in variables.\n\n    (\"param1\" \"param with spaces\") would be\n\n\n$1=\"param1\" $2=\"param with spaces\"\n \n\n    See 'variable substitution' for more use - ${@:2} (w/o \" works same)  \n\n\n\n\nshift:\n\n\nshifts params backwards so $2 would be $1 and old $1 would vanish\n  using this command each parameter could be handled one by one\n\n   Example:  \n\n\nwhile [[ $# -gt 0 ]]; do\n echo \"Parameter is $1\"\n shift\ndone\n\n\n\nThis will echo out all parameters given as per shift will consequentially\n  move all parameters into $1 position one by one\n\n\nbasename:\n\n\nreturns file name from latest node in the path\n\n   Example:  \n\n\n$/home/dos/scripts/some_script.sh\nin script >> echo basename $0\nwill return 'some_script.sh'\n\n\n\ndirname:\n\n\nreturns directory name (everything except latest nodein the path)\n\n  antipod of \nbasename\n\n   Example:\n\n\ncd /home/dos/directory/filename\ndirname $PWD\n> /home/dos/directory\n\n\n\nvariables:\n\n\nAKA Global Variables - are visible everywhere in shell environment\n   if 'export'ed - will be visible in subshells created from the shell\n\n   'local' variables are local for funcions - see below\n\n  is converted to 'String' class by default\n\n  Initializate variable by = sign w/o leading or trailing spaces\n\n  Access variable by $  \n\n\nAlso could be int, if used with 'declare':  \n\n\ndeclare -i variable=1\n\n\n\nCOuld be readonly:  \n\n\ndeclare -r READONLY=\"this is constant, in uppercase by convention\"\n\n\n\nVariables definitions:  \n\n\na=z                     # assign string \"z\" to var 'a'\nb=\"a string\"            # assign \"a string\" w/ space to var 'b'\nc=\"a string and $b\"     # expansion works in \"\" so 'c' value \n                        # is \"a string and a string\"\nd=$(ls -l foo.txt)      # result of command is assigned to 'd'\ne=$((5 * 7))            # result of ariphmetic result is assigned to 'e'\nf=\"\\t\\tastring \\n\"      # escaped sequences also works in \"\" here:\n                        # \\t - tab\n                        # \\n - new line\n\n\n\nMore than one variable could be declared in 1 line:  \n\n\na=5 b=\"a string\" # will create two variables with\n                 # values \"5\" and \"a string\", probably 5 is int.ToString()\n\n\n\nTo preserve spaces and stuff use quotas with string vars initialization\n\n   Example:  \n\n\nmy_str_var=\"string w/ spaces goes in quotas\"\necho \"stuff is: $my_str_var\"   # stuff is: string w/ spa.....\necho 'stuff is: $my_str_var'   # stuff is: $my_str_var\n\n\n\narrays:\n\n\nalso variables, indexed from 0, \n\n  could have empty indexes.\n\n   Example declaration:  \n\n\na[10]=Ten   # assign 'Ten' to 11th index in array length of 1\n\n\n\nPowershell-like thing(or vice versa), except w/o @   \n\n\ndays=(Mon Tue Wed Thu Fri Sat Sun)\ndays=([0]=Mon [1]=Tue ... )    # same as above but with strict indexing\nanimals=(\"a dog\" \"a cat\" \"a cow\")\n\n\n\nIf array addressed w/o index, index 0 is displayed, if it is null then nothing\n  displayed EVEN IF THERE ARE OTHER INDEXES\n\n   Example:  \n\n\narr=([1]=one [2]=two)\necho $arr       # nothing returned\narr[14]=14\necho $arr       # nothing returned\narr[0]=0\necho $arr       # 0 is returend\necho ${arr[@]}  # 0 one two 14 is returned\n\n\n\nExample2: assignment of array w/o index specification  \n\n\narr=(a b c d)\necho ${arr[@])  # a b c d - is returned\narr=AAA\necho ${arr[@]}  # AAA b c d - is returned\necho $arr       # AAA - is returned\n\n\n\nExample3: deletion of first index w/o index specification  \n\n\narr=(a b c d)\narr=\necho ${arr[@]}  # b c d - is returned\n\n\n\nIteration:\n\n\nArray variable substitution(see below for strings) arrays could be iterated\n  and counted and checked for empty indexes:\n\n   Iterate array:  \n\n\n${animals[*]} # returns list of values split by space\n${animals[@]} # same as above\n\n\n\nExample:  \n\n\narr3=(\"a first\" \"a second\" 'a third')      # \" or ' is fine\nfor i in ${arr3[*]}; do echo $i ; done\n\n>a          # separate variables\n>first      # also separated by spaces(split)\n>a          # will work fine if no Spaces\n>second\n>a\n>third\n\n\n\n\"${animals[*]}\" - JOIN joins all elements into 1 string\n\n Example:\n\n\n for i in \"${arr3[*]}\"; do echo $i ; done   # only \" , ' will not work\n >a first a second a third                  #joined string\n\n\n\nTODO: CONTINUE FROM HERE\n\n\nBest form to use:\n  \"${animals[@]}\" - returns list of values NOT split by space(real contents)\n    Example:\n   for i in \"${arr3[@]}\"; do echo $i; done\n\n\n\n\na first     # only different values are separated\na second        # spaces are in place\na third     # best form to use\n\n\n\n\nCounting:\n  Array count coun be calculated, and length of value in single index:\n\n\n.Count:\n  ${#array[@]}   \n\n   where:\n  array is var name\n   Example:\n  echo ${#arr3[@]}\n\n\n\n\n3            # there 3 elements 'a first' 'a second' 'a third'\n\n\n\n\narray[i].Length:\n  ${#array[i]}   \n\n   where:\n  array is var name and i is index to measure length of\n   Example:\n  echo ${#arr3[1]}\n\n\n\n\n8            # 'a first' is 8 characters long\n\n\n\n\nCheck indexes numbers:\n   Arrays could have uninitialized or null initialized elements under some\n  indexes.\n   The way of how to check which indexes are taken is:\n  ${!array[@]}      # works same w/ and w/o double quotes\n  ${!array[\n]}      # works same as above w/o quotes, \n            # but differently w/ quote, see example below in NOTE:\n   Example:\n  arr=([1]=\"a one\" [3]=\"a three\" [5]=\"a five\")\n  for i in ${!arr[@]}; do echo $i; done\n\n           \"${!arr[@]}\"         # will do the same\n           ${!arr[\n]}           # will do the same\n\n\n\n\n1\n3\n5\n\n\n\n\nNOTE: @ works same With or WithOut quotes:\n        ${!arr[@]} == \"${!arr[@]}\n   AND\n        * works same as above WithOut quotes\n   BUT\n\n        in double quotes it will JOIN indexes in one string with spaces separators\n   Example:\n  for i in \"${!arr[*]}\"; do echo $i; done\n\n\n\n\n1 3 5                # produces one line of indexes\n\n\n\n\narray.Add:\n  Arrays could have new elements added into the end of it.\n This is made as in Powershell(or powershell take it from bash) using +=\n  Example:\n arr=(a b c)        # echo \"${arr[@]}\" > a b c\n arr+=(f e g)       # echo \"${arr[@]}\" > a b c f e g\n\n\narray.Sort:\n  There is no default sort method, but it could be implemented with\n piping and regular 'sort' command, but with Iteration:\n  arr_sorted=$(for i in \"${arr[@]}\"; do echo $i; done | sort)\n    this will save soreted \"a b c e f g\" into arr_sorted\n   !!!! BUT THIS WILL NOT WORK    !!!\n   !!!!   echo \"${arr[@]}\" | sort !!!\n\n\nDelete array:\n  Deletion of array is done in the same way as deletion of variables\n using 'unset' command.\n DO Not Use '$' in unset.\n  Syntax:\n unset array_name\n\n\nExample:\n arr=(1 2 3)\n echo ${#arr[@]}    # check Count - returns 3\n echo ${arr[@]}     # check contents - returns 1 2 3\n unset arr      # deletes arr, NOTICE no '$' before 'arr' NAME\n echo ${#arr[@]}    # cehck Count - returns 0, as for anything unset\n            #+ because bash initializes on execution call\n echo ${arr[@]}     # returns nothing\n\n\nvariable substitution:\nsubstitutuin:\n   Bash supports various variables substitutions:\n  $a - will be substituted with 'a' value\n  ${a} - same as $a but could be concatenated w/ string w/o spaces:\n    ${a}.txt - will be expanded in a_value.txt\n  ${11} - 11th positional parameter given to script from shell\n  ${var:-word} - if 'variable' is set, the result will be its value\n    if 'variable' is unset - the result will be 'word'\n  $(var:=word} - if variable is set results in its value substituted\n    if variable is unset, it will be assigned to 'word'\n    such assignment will not work for positinal params(see 'shift')\n    and other special variables\n  ${var:?word} - if variable is unset error with reason 'word' will be\n    generated, exit code of such construct will be 1\n  ${var:+word} - if 'variable' is set, the result will be 'word',\n    (but variable's value will not be changed)\n    otherwise result will be EMPTY string\n  Example:\n    $ echo ${variable:-ls} - variable unset - ls used\n    > ls\n    $ export variable=1\n    $ echo ${variable:-ls} - variable is set- its value used\n    > 1\n    $ echo ${variable:+ls} - variable is set - ls used\n    > ls\n    $ echo ${variable1:+ls} - variable unset - empty line used\n    > \n  ${!prefix\n} or ${!prefix@} - returns NAMES of existing variables\n    that starts from 'prefix.\n   Example:\n    $ echo ${!BASH\n}\n    > BASH BASHOPTS BASHPID BASH_ALIASES BASH_ARGC BASH_ARGV BASH_CMDS\n\n\nstring variables substitution:\n  ${#var} - returns length of string in variable's value\n    Example:\n   $ var=123456789   #this could be interpreted as a string too now\n\n\n\n\n9           #string length is 9\n\n\n\n\n${#} or $# or ${#@} or ${#*} - returns number of positional parameters\n    of the script being executed\n\n\n${var:number} - return string from number to the end, spaces trimmed\n          variable is unchanged.\n    Example:\n   $ var=\"This string is to long.\"\n   $ echo ${var:5}         #returns string from 5th symbol\n\n\n\n\nstring is to long.\n    Example: spaces are trimmed:\n   $ echo ${var:5} | wc -c     #count chars\n   $ 19\n   $ echo ${var:4} | wc -c     #return starts from space\n   $ 19                #space is trimmed so same number of chars\n  ${var: -number} - return string from end to number, spaces trimmed\n            NOTE - space between ':' and '-' signs\n    Example:\n   $ echo ${var: -5}\nlong.\n  ${var:number:length} - return string from number till end of lenth\n    Example:\n   $ echo ${var:5:6}\nstring\n\n\n\n\n${var: -number: -length} - return string number between number(from the\n                 end) and length (also from the end)\n                 NOTE: number must be > than length\n    Example:\n   $ echo ${var: -18: -2}    #var is This string is to long.\n\n\n\n\nstring is to lon\n\n\n\n\n${@} - return all values of positional params \n     leaving spaces inside strings (like \"$@\" ) - bcs it know how \n     many arguments script has\n     ${*} is the same form, it seems\n\n\n${@:num} - displays values of positional params but from num\n     $(@:1) - works same as ${@}\n     ${@: -2} works , but starts from the end\n\n\n${@:num:length} - same as with strings but with positional params\n  ${@: -num: -length} - same as with strings but with positional params\n\n\n${param#pattern} - finds shortest match and deletes it (lazy match)\n    Example:\n   foo=\"file.txt.gz\"\n   ${foo#*.}\n\n\n\n\ntxt.gz\n  ${param##pattern} - finds longest match and deletes it (greedy match)\n   Example\n   ${foo##*.}\n.gz\n\n\n\n\n${param%pattern}  - same as #  but deletes from the end of the file\n   Example:\n   foo=file.txt.gz\n   ${foo%.\n}        - note .\n instead of *. in # example\n\n\n\n\nfile.txt\n  ${param%%pattern} - same as ##\n   ${foo%%.*}\nfile\n\n\n\n\nSearch and replace:\n\n\n\n${param/pattern/string} - replaces first occurance of pattern with string\n ${param//pattern/string} - replaces all occurances of pattern with string\n ${param/#pattern/string} - replaces only if at the beginning of the line\n ${param/%pattern/string} - replacesonly if at the end of the line\n\n\n$(())\n  Accepts any valid arithmetic expression \n   Pretty similar to (()) test construct which returns true when result >0\n Accepts any number system:\n\n\nDecimal\n   with base of 10\n  number\n   Example:\n  echo $((10)) - will return 10\n  echo $((10#10)) - will return 10\n\n\nOctal: \n   with base of 8 [0-7]\n  0number\n   Example:\n  echo $((010)) is 8 in decimal, where 10 is the whole base, which is 8\n  echo $((07)) is 7 in decimal\n  echo $((8#10)) is 8 in decimal\n  echo $((08)) - will return error 'value to great'\n        0-7; 10-17 and so on will work\n\n\nHex: Hexademical:\n   with base of 16 [0-9A-F]\n  0xnumber\n   Example:\n  $((0x10)) - is 16 in decimal, which is full base\n  $((16#10)) - is also 16, because of base of 16 numbers\n  $((16#100)) or ((0x100)) is 256 which is 16x16 or 16 in square(^2)\n  $((0xFF)) is F(15) full bases plus F(15) \n        which is ((15x15)==240)+15 == 255\n\n\nCustom:\n    custom base , could be any number [0-9A-Z] and some other]\n    maximum base is 64, seems like\n   base#number\n    Example:\n   echo $((17#10) - is 17, which is full base\n   echo $((64#10)) - is 64, which again is full base\n\n\nArithmetic expressions syntax $((arithmetics go here)):\n\n\nExample:\n\n\n\necho $((2+3)) >> 5\n\n\nIFS - variable that containes field separator, \n    by default is space/tab/new line\n    Could be changed\n\n\nTemporary variables:\n   Bash supports variables change/assignment before command execution\n  Variables assigned before command execution (on the same line) will be\n  changed only for env with which command will be executed:\n   IFS=\":\" read field1 field2 ... <<< \"$line_from_passwd\"\n    Equals to:\n   OLD_IFS=\"$IFS\"\n   IFS=\":\"\n   read field1 field2 .... <<< \"$line_from_passwd\"\n   IFS=\"$OLD_IFS\"\n  !Note:\n   it works weird - only in example from bood, need to research and update\n\n\nUnary operators:\n   Operators that require value on UNE side, contrary to binary operators\n   that reauire values on both sides:\n    + - is for positive numbes: +1\n    - - is for negative number: -1\n\n\nArithmetic operators: Binary operators:\n    there are more binary operators, but those below are used in Arithmetics\n  + - for addition\n  - - for substruction\n  * - for multiplication\n  / - for division\n  \n - for exponentiation (2\n2=4 3\n3=9 4*4=16 and so on, aka ^2 ^3 ^4 ...)\n  % - modulus division \n    5%2=1 which is what left after division - 2, 2 and 1\n      Example:\n    echo $(( 5%2 )) - will return 1\n\n\nAssignment in Arithmetic expressions:\n  echo $((foo=5))  - will assign 5 to $foo\n   NOTE: also could be used in tests like [[...]] and ((...))\n   NOTE: in 'test' or [...] used to \ncompare\n strings, BEWARE\n\n\nAssignment operators:\n   var = value \n    regular assignment operator\n   var += value \n    assignment with value addition      $var = $var + value\n   var -= value\n    assignment with value substruction  $var = $var - value\n   var *= value\n    assignment with value multiplication    $var = $var x value\n   var /= value\n    assignment with value division      $var = $var / value\n   var %= value\n    assignment with value modulus division  $var = $var % value\n\n\nAssignment with C-like style increment decrement:\n   Post - means show\\use first, change after it shown\\used\n  var++ - post increment            $var = $var + 1\n  var-- - post decrement            $var = $var - 1\n   Example:\n   foo=1; echo $((foo++)); echo $foo\n      1        2\n   Pre  - means change\\use first , show after it changed\\used\n  ++var - pre increment             $var = $var + 1\n  --var - pre decrement             $var = $var - 1\n   Example:\n   foo=1; echo $((++foo)); echo $foo\n      2        2\n\n\nBasic logical gates:\n   AND - returns true if both entry are true\n   OR - returs true if at least one (or both) entry is true \n   XOR - returns true if entry differs, \n     both true and both false return false\n   NOT - inverts entry - true is false and false is true\n   NAND - both false are true\n   NOR - returns true if at least one (or both) entry is false\n   XNOR - returns true if not differs,\n      both true or both false return true\n\n\nLogical operators in Arithmetics:\n   Comparison operators less, greater , less than and so on\n  <= - less or equal\n\n\n\n\n= - greater or equal\n  < - less\n- greater\n  == - equal\n  != - not equal\n  && - AND\n  || - OR\n\n\n\n\nexpr1:expr2?expr3 - trinary operator, \n              if expr1 is true \n              then expr2 executed\n              else expr3 is executed\n   Example:\n  a=0\n  ((a<1?++a:--1))   # a+=1 and a-=1 will not work\n  echo $a       # 1\n  ((a<1?(a+=1):(a-=1))) # but this will work just fine\n  echo $a       # 0\n\n\nlocal variables:\n  local variables are declared in functions, and visible only there\n  local variables overwrite globals with the same name\n   Syntax:\n  local variable_name[=optional_value]\n   values are optional , probably, in order to delete some global vars inside\n  a function(i.e. vars are used as standard in\\out params of a command)\n   Example:\n  foo=0\n  func() {\n   local foo\n   foo=1\n   echo \"in func $foo\"\n  }\n  func\n  echo \"in script $foo\"\n   Result:\n\n\n\n\nin func 1\nin script 0\n\n\n\n\nhere doc here docs here-document here script :\n  allows multiline input and it WILL expand even if in ''.\n   But will not expand if escaped - \\$\n   Syntax:\n  command << indicator\n  text\n  indicator\n\n\n<<-  - work as << but accepts TAB before enclosing 'indicator'\n    'space' will still return error\n\n\nwhere:\n  command is a command like, 'cat' or anything else (doesn't work with 'echo')\n  where text between 'indicators' is expanded by shell and sent to STDIN of\n  the command\n   Note that also <<- could be used, with it bash will ignore leading\n  tabs in 'indicator'\n   Note that seconds 'indicator' has to be on separate line and \n  Must have no spaces before or after it, otherwise bash interpreter will \n  conntinue to look for 'indicator' ignoring the one with leading/trailing \n  spaces\n   Value of 'indicator' could be any but 'EOF' is preffered\n\n\nExample:\n  $ foo=\"text\"\n  $ cat << \nEOF\n\n\n\n\n$foo\n\"$foo\"\n'$foo'\n\\$foo\n\nEOF\n\n   Result:\n  text\n  \"text\"\n  'text'\n  $foo\n\n\n\n\nExample of use with 'ftp'\n  ftp -n << EOF\n  open $FTP_SERVER_ADDRESS\n  user anonymous username@hostname\n  cd $FTP_PATH\n  hash\n  get $REMOTE_FILE\n  exit\n  EOF\n  ls -l $REMOTE_FILE - will display downloaded from ftp remote file, all the\n    commands are passed from inside the here-document\n\n\nhere-line:\n  same as here-doc but onliner:\n   $read field1 field2 <<< \"test in put\"\n   $echo $field1  ---> test\n   $echo $field2  ---> in put  < because put is 3rd field, and we have only 2\n                all excess fields goes into last 'field2'\n    Profid of here-line here is that commands with pipe\n   $echo \"test in put\" | read \n\n    will not work because subshell will be created for 'read' where $REPLY\n    will be created and assigned, but after that subshell ends, and\n    subshell can not change parent shell's env, such as REPLY value\n\n\nif:\n  Condition tests in IF should be inside square brackets - [], which are just a\n  reference for commant 'test'. see MAN page for TEST for details.\n   example:\n  if [ 100 -eq 100 ]; then - semicolon is needed in case of single line w/o /n\n   -or-\n  if [ 100 == 100 ] - seems like this also should work\n   -or-\n  if $(test $(echo \"$REPLY\" | grep '^[0-3]$'))\n    tests whether value of REPLY is mathced- if not, grep return 1(error)\n    then test returns 1 and 'if' will not work\n    if value matched then grep returns 0 (good), then test returns 0 and\n    'if' will work\n   -or-\n  if test $(echo \"$REPLY\" | grep '^[0-3]$')\n    same as above, but $() outside 'test' is unnecessary in fact\n    but works in both ways so keep both examples here\n   -or-\n  if [ \n ] && [ \n ] || [ \n ]\n  then\n    \n\n  elif [ \n ]      # else if, onliner gotta be ; ended\n  then\n    \n\n  else              # one liner gotta be ; ended\n    \n\n  fi\n\n\nlots info here:\n  https://ryanstutorials.net/bash-scripting-tutorial/bash-if-statements.php\n\n\n&& ||:\n  allowed inside bash shell logical AND and OR:\n   Example:\n  mkdir test && cd test\n   will create and than change dir to 'test'\n  [ -d test ] || mkdir test && cd test\n   will check whether 'test' exists, returns 0 or 1\n   || continues only if 1 was returned by 'test' and proceeds with mkdir\n   && continues only if 0 was returned by 'mkdir' and proceeds with cd\n\n\ntest:\n  Tests whether expression returns 0 or not\n  after evaluation command 'test' returns 1 if true and 0 if false\n   test 100 -eq 100 - test whether 100 equals 100\n  or [\n]\n   if [ 100 -eq 100 ]\n  used to test conditions\n\n\nLogical operators:\n   -a - AND\n   -o - OR\n   ! - NOT\n    Example:\n  [ ! ( FALSE -o TRUE )   -- (TRUE) but [ FALSE]\n   note the escaped parentheses\n   note that 'false' and 'true' command will behave differently , bcs returns\n  interegs 1 and 0 respectively\n\n\nRequires shell expansion symbols $ \n   Example:\n  if [ $num1 -lt $num2 ]; then echo 'int 1 less than int2'; fi\n\n\nBEWARE!!!!!!\n  shell creates empty variable if it is not defined\n  and $num1 -lt $num2 will return TRUE if both undefined\n\n\nCHECK YOUR VARIABLES BEFORE USE\n\n\nTest Strings\\Ints:\n  Operator  Description\n  ! \n  The EXPRESSION is false.\n  -n str    The length of STRING is greater than zero.\n  -z str    The lengh of STRING is zero (ie it is empty).\n  str1 == str2      STRING1 is equal to STRING2 (= also allowed)\n  str1 != str2      STRING1 is not equal to STRING2\n  str1 > str2       STRING1 is alphabetically greater than STRING2\n  INTEGER1 -eq INTEGER2     INTEGER1 is numerically equal to INTEGER2\n  -ne               integers are not equal\n  -le               are less or equal\n  -ge               greater or equal\n  INTEGER1 -gt INTEGER2     INTEGER1 is numerically greater than INTEGER2\n  INTEGER1 -lt INTEGER2     INTEGER1 is numerically less than INTEGER2\n\n\nTest Files:\n  FILE -ef FILE1 - both files are hard links (point to same inode)\n  FILE -nt FILE1 - FILE newer than FILE1\n  FILE -ot FILE1 - FILE older than FILE1\n  -e FILE   FILE exists.\n  -f FILE   FILE exists and is file\n  -s FILE   FILE exists and its size greater than 0\n  -L FILE   exists and is soft symblic link\n  -d FILE   FILE exists and is a directory.\n  -b FILE   exists and is Block device (i.e /dev/sda1)\n  -c FILE   exists and char device\n  -r FILE   FILE exists and the read permission is granted.\n  -s FILE   FILE exists and it's size is greater than zero (ie. not empty).\n  -w FILE   FILE exists and the write permission is granted.\n  -x FILE   FILE exists and the execute permission is granted.\n  -G FILE   exists and file belongs to existing group\n  -O FILE   exists and belongs to existing user\n\n\nNOTE:\n  == - does a string comparision\n  -eq - does numerical comparison\n\n\nEscaping:\n  symbols < > ( ) need to be escabed by \\ or be between ''\n  because [ ] is just command test, and bash will try to use them as its own \n  meta symbols (< > are stdout\\in redirecton, so could be a MESS)\n\n\n[[ ]]:\n  improved 'test' command but not POSIX compatible\n    Differences\\Features:\n\n\nDoes not need expansion symbol $, expands just fine w/o it\n\n\n\nUNLESS IS A STRING, then use \"$VAR\", to expand and prevent empty values\n    Example:\n   $ if [[ $num1 -lt num2 ]]; then echo \"$num1 less than $num2\"; fi\n\n\n\n\n1 less than 2\n    NOTE:\n   $num1 and num2 are both fine as per expansion works in some other way\n\n\n\n\nBEWARE OF UNDEFINED VARIABLES\n   IF BOTH ARE UNDEFINED 0 WILL BE RETURNED!!\n\n\nCan compare regex.\n\n\n\nSeems like gotta match full line, not just some part.\n   uses Extended Regex (ERE)\n    Requires $ symbol to expand variable value\n    use \"\" to prevent empty values\n   No need to escape bash meta symbols like \\ or *\n    Example:\n   $ if [[ \"$string_var\" =~ \\w+? ]]; then echo \"good\"; fi\n\n\n\n\ngood\n\n\n\n\nCan use wildcards , or similar to it..\n\n\n\nWhen using == equation\n    Example:\n   $ FILE=foo.txt\n   $ if [[ $FILE == foo.* ]]; then echo \"matches pattern\"; fi\n\n\n\n\nmatches pattern\n\n\n\n\nLogical operators:\n   && - AND\n   || - OR\n   ! - NOT\n    Example:\n   if [[ ! ( FALSE || TRUE )]]; --- in (will be TRUE) but ! will invert to FALSE\n    Note that 'false' and 'true' are not commands, commands return 1 and 0\n   and behavios incorrectly\n\n\n(( )):\n  Permits Arithmetic expansion and Evaluation for \n  Works only with Integers (no float dot like in Double or Float types)\n   Note:\n  bash syntax part, not a command\n  also does expansion w/o $ symbol before var name\n\n\nArithmetic expansion examples\n   a=$(( 5 + 3 )) or a=$((5+3)) - assign result of addition to variable 'a'\n   C style manipulations:\n   (( var++ )) or ((var++)) - display then increment by 1\n     $(( var++ )) - increments and saves var but also produses errors\n   (( ++var ))  - increment by 1 then display\n   (( --var ))  - same manner decrement\n   (( var-- ))  - display then decrement\n   Assignment:\n   $((b+=a)) - add 'a' to 'b' and assign result to 'b' - b=$a+$b\n\n\nEvaluation:\n   if ((10>=5)); then echo \"10 greater or equal 5\"; fi\n      ((var1>=var5))  - same byt with variables\n\n\nLogical operators:\n   same as in [[ ]] - && , || , !\n\n\n(()) returns 'true' if integer inside of it greater than 0\n   Example:\n  dos:bash$ if ((1)); then echo \"true\"; else echo \"false\"; fi\n\n\n\n\ntrue\n  dos:bash$ if ((0)); then echo \"true\"; else echo \"false\"; fi\nfalse\n\n\n\n\nLoops:\nfor:\n   Has two forms, first is foreach:\n    Syntax:\n  FOR var IN words; DO \n    commands\n  DONE\n   var - custom name as in regular foreach\n     usually i j k l m - thanks to Fortran where int vars must start\n     with those letters\n   words - is a collection, could be just 1 2 3 4 \n       or result of command\n       or result of wildcard matches list\n       or result of subshell execution\n       if omitted - args passed from commandline\n   do and done are the braces\n   Example:\n  for i in 1 2 3;  do echo \"print $i\"; done\n\n\n\n\nprint 1\nprint 2\nprint 3\n\n\n\n\nExample:\n  for file in test*.txt; do echo $file; done\n\n\n\n\nwill echo all files from current dir matched by globbed(wildcard)\n\n\ntest*.txt mask\n\n\n\n\n\nExample:\n  for i in $(some commands executed in subshell); do....\n\n\nIf optional 'words' component is omitted in 'for' it uses args(params) as\n  collection to iterate through\n\n\nExample:\n  for i; do echo \"command line param 1 is: $1\" shift; done\n   this will echo out all params of bash script one by one (shift command)\n\n\nSecond form of 'for' is c-like (like real 'for' in c# and not 'foreach'):\n    Syntax:\n   for (( expr1; expr2; expr3 )); do\n    commands\n   done\n\n\nwhere \n    expr1-2-3 - are arythmetic expressions\n\n\nExample:\n\n\n\nfor (( i=0; i>5; i++ )); do\n    echo \"i is $i\"      # will output $i from 0 to 4\n   done\n\n\nThis form is equialent to following construction:\n\n\n\n(( expr1 ))          # this is assigned before loop\n   while (( expr2 )); do    # loop goes as long as expr2 is true\n    commands\n    (( expr3 ))         # at the end of each iteration expr3 is\n                #+ reassigned/reevaluated\n   done\n\n\nwhile:\n  infinity loop while exit code of command\\expression is 0 \n  could read from file(stdin) line by line with STDIN redirect or pipe:\n   Syntax:\n  while true; do    | while [[ true ]]; do | while (( 1 )); do\n   commands\n  done\n    Example:\n   while read field1 field2 field3\n    ..\n   do < file_to_read_from.txt\n    Example:\n   cat file_to_read_from.txt | while read field1 field 2 field3; do ... ; done\n\n\nexample:\n\n\n\nwhile read host ...; do... - or like this if in 1 line\n  do\n    ping -c 3 $host\n    if [[ $call_continue ]]; then\n      continue           # will pass further iteration and start over\n    fi\n    if [[ $call_break ]]; then\n      break         # break loop and go out of it at all\n    fi\n  done < myhosts.txt\n\n\nwill read from myhosts.txt while there are lines, each line will reinit $host\n\n\nbreak: will stop loop from execution - just regular break as ususal\n  continue: will pass iteration and start next iteration - also standard one\n\n\nexit status:\n\nexit code:\n  bash scripts ALWAYS returns exit code. 'exit' is optional\n  commands ALWAYS return exit code too. \n  Functions ALWAYS return exit code too. 'return' is optional\n  seems like everything returns exit code.\n  Even bash shell 'exit'ed with number will return this number\n  to manually return exit code:\n  exit nnn - to return nnn\n    !!   nnn - MUST be an INTEGER in range 0-255 !!\n  return nnn - to return nnn but from Function\n  exit $? or exit or omitting the 'exit':\n   with no params exit code is taken from latest command executed in the script\n  Variable that contains exit code:\n  $? - contains latest exit code, overriden after any next execution\n    even 'echo $?' will override it with 0\n    Exit code values:\n  0-255 allowed\n    By convetion\n  0 - OK\n  1-255 - various errors \n\n\nto test Exit Code:\n  echo $? - will return exit code of previous command or value of previously\n        executed script \n  more reading:\n  http://tldp.org/LDP/abs/html/exit-status.html\n\n\ncase:\n  check whether one of condition matches the 'word' in case, if so it\n executes code in particular case and terminates\n  Syntax:\n case $word in      # value of $word will be compared against\n  \n|\n) cmd1 # the \n and if tru command1\n         cmd2   # and command2 will be executed\n         ;;     # this ends particular case block\n  \n)         exit 1     # * match anything, last case 'catch'\n             ;;     # ends 'catch' block values of '$word' either\n            #+ matched by previous cases or this one\n            #+ nothing is left 'case' block unmatched this way\n esac           # this ends whole case, after 1 of the cases \n            #+ matched, or none matched\n  Example:\n while [[ -n $1 ]]; do      #while param is not null\n  case $1 in            #check param\n    -f | --file)    shift   #take $2 as $1 , if -f, \n                #to get file path given with -f option\n            filename=$1\n                #save file path to $filename\n            ;;\n  esac\n Case uses wildcards as pattern:\n ? - one character\n * - anything of any length\n [ABC] - 1 letter - either A, B or C\n [0-9] - 1 number in 0 - 9 range - any digin with length of 1\n a - letter 'a'\n \n.txt - anything that ends with '.txt'\n\n\nGood practice is \n\n\ntrue:\n  command that always return 0\n   Example:\n  exit $(true)   - will return 0 exit code\n   or Function:\n  return $(true) - will return 0 after Function\n   or:\n  true       - will return 0 exit code\n   or Funcion:\n  test() {\n   true\n  }\n  test       - this will return 0 in script, and if last line - script\n           will return this same 0 to caller (i.e. bash shell)\n   Same works for False\n\n\nfalse: \n  command that always return 1\n\n\nhelp:\n  lists all built-in commands\n   help \n\n  will return help for that command\n\n\nshopt:\n  SHell OPTions - built-in command to set\\view shell options\n\n\nshell types\n  See details:\n  https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work\n  Even more detailed:\n  https://askubuntu.com/questions/879364/differentiate-interactive-login-and-non-interactive-non-login-shell\n there are 4 shell types\n  Interactive Login shell\n    either shell opened via SSH\n    or via ctrl+alt+F1(-F6)\n  Interactive Non-Login shell\n    Opening terminal\n    executing 'bash'\n  Non-interactive Non-Login\n    scripts execution\n    or 'bash -c'\n  Non-interactive Login\n    rare stuff, complex.. should be ssh launched w/o command and STDIN of the\n    ssh should has no TTY - echo command | ssh server\n    or 'bash -l -c command'\n\n\nHow to check which shell is:\n   echo $-\n     if output has 'i' - its Interactive:\n\n\n\n\nhimBH\n\n\n\n\n\n\nbash -c 'echo $-'\n\n\n\n\nhBc\n     NOTE: that double quotes - bash -c \"echo $-\" still will start interactive\n    shell.. probably due to Shell Expansion of variables\n\n\n\n\nshopt login_shell\n\n\n\n\nlogin_shell off\n    will mean that it is NOT login shell\nlogin_shell on\n    will mean that is IS login shell\n\n\n\n\nExample for each type:\n\n\nInteractive, non-login shell. Regular terminal\n\n\n$ echo $-; shopt login_shell\nhimBHs\nlogin_shell     off\n\n\nInteractive login shell\n\n\n$ bash -l\n$ echo $-; shopt login_shell\nhimBHs\nlogin_shell     on\n\n\nNon-interactive, non-login shell\n\n\n$ ssh localhost 'echo $-; shopt login_shell'\nhBc\nlogin_shell     off\n\n\nNon-interactive login shell\n\n\n$ echo 'echo $-; shopt login_shell' | ssh localhost\nPseudo-terminal will not be allocated because stdin is not a terminal.\nhBs\nlogin_shell     on    \n\n\nfunctions:\n  shell could have functions, funcions MUST BE declared before its call,\n  otherwise shell will interpret funcion names as regular commands\n   Functions could be executed and calld from shell, added to scripts or \n  bashrc/profile files so will be available in cmd as regular command\n  'return' command in optional\n   Function MUST has at least one command, in order to mock\\stab 'return'\n  command could be used.\n   Basically every script could be converted into a function just by\n  copuint all the contents after she-bang in between function declaration\n  which is in Syntax below. And replace 'exit' with 'return' keywords\n   Variables in the Function could be set as 'local' overriding outer\n  global variables.\n   $0 in fuction contains name of the script being executed\n   $FUNCNAME - contains name of the function being executed\n\n\nSyntax:\n  function name {\n    local VAR\n    local VAR2=value\n    \n\n    return\n  }\n   Or equivalent:\n  name() {\n   \n\n   return\n  }\n\n\nExample:\n\n\n\nscript.sh:\n  function func {\n   echo \"step 2\"\n   return\n  }\n  echo \"step 1\"\n  func\n  echo \"step 3\"\n\n\nResult:\n\n\n\n\nstep 1\nstep 2\nste 3\n\n\n\n\nExample .bashrc:\n  ds() {\n   echo \"disk space utilization for $HOSTNAME\"\n   df -h\n  }\n\n\nResult after shell restarted \n   (after source .bashrc ds looped and crashed shell):\n  $ ds\n\n\n\n\ndisk space utili....\n...output of df -h....\n\n\n\n\n|:\n  pipe character\n  creates new subshell, executes what goes after pipe there, then returns\n  back to parent shell. subshell can not change parent shell - so 'read' command\n  will not work in pipe\n\n\nbreaking line; new line:\n use \\ at the end of the line.\n  NO SPACES OR ANYTHING AFTER THE BACK SLASH.\n Guideline: split the long(80 chars) line into two lines.\n Also break the line everytime there is an &&, | or || characters.\n Example:\ncommand1 \\\n && command2 \\\n || command3 \\\n | command4\n\n\nread:\n  reads line from keyboard input, accepts here-strings, does not work with |\n  By default saves entered value into $REPLY global variable\n   Synopsis:\n  read [-params] [variable1 variable2 variable3 ...]\n\n\nAccepts more than 1 variable to save keyboard input\n   Default separator is space or tab or new line(\\n)\n  If less variables are given to save data, all excessive data (delimited \n  fields) will be saved in last variable (like msbuild do when too many params)\n  When too many variables is given - unpopulated vars remain empty string\n\n\nParams:\n  -a - array, saves input into the array \n  -d \n - delimiter, by default space\\tab\\newline\n  -e - use Readline, behaves like keyboard input in bash\n  -n \n - read num symbols from keyboard input\n  -p \n - display promt befor cursor on the same line (like bash promt)\n  -r - do not inerpret \\ symbols\n  -s - silent, do not display entered data, like in password field\n  -t - time out for wait of entry\n  -u \n - use file with given descriptor as STDIN\n\n\nCommand grouping:\ncommand groups:\n Commands could be groupped, there are two types:\n  Groups:\n   { command1; command2; [command3; ... ] }\n  Subshell:\n   (command1; command2 [; command3 ... ])\n\n\nCommands in Groups are executed in current shell, so ENV variables \n chaned during commands execution are saved into current Shell ENV\n  Subshells are executed in sub shell and current ENV could not be \n modified.\n\n\nExamples:\n ls -l > output.txt\n echo \"listing of foo.txt\" > output.txt\n cat foo.txt > outpu.txt\n  ==\n { ls -l; echo \"listing of foo.txt\"; cat foo.txt; } > output.txt\n  or\n (ls -l; echo \"listing of foo.txt\"; cat foo.txt) > output.txt\n\n\nNOTICE:\n  Groups must have spaces next to {}, and last command should end with ;\n  SUbshells do not need extra spaces and ;\n BUT:\n  subshells are taking longer to be executed because of extra subshell\n need to be created. Also more memory is required. \n  Also no parent shell ENV modifications is possible\n\n\nExample with pipe:\n  { ls-l; echo \"listing of foo.txt\"; cat foo.txt; } | lpr\n this will redirect output through pipe into 'lpr' which is print command.\n  So all the output of 3 commands are redirected at once into 'lpr'\n which is not the same with \n  ls -l | echo \"...\" | cat foo.txt | lpr - will send only foo.txt to\n print\n\n\nProcesses substitution:\n <(commands list)  - processes sending output to STDOUT\n\n\n\n\n(commands list)  - processes getting input to STDIN\n\n\n\n\nSubshell output is interpreted as regular file, and could be redirected\n into another commands as usual.\n\n\nExample:\n  read < <(echo \"foo\")\n  echo $REPLY - will return 'foo'\n where:\n  'read' reads from STDIN, and saves what is read into $REPLY by default\n     if executed in subshell $REPLY will not be available to \n     parent shell\n        echo \"foo\" | read - will not work\n  <     - STDIN redirection, so STDOUT from <() is redirected as STDIN\n      of 'read' command\n  <(echo \"foo\") - Process substitution, where 'ehco' executed in \n          subshell Group () sends its output to STDOUT.\n        In other words output of this part is represented as\n        usual file, which is redirected into 'read' STDIN\n  echo $REPLY - proves that $REPLY is updated in current shell with\n        value redirected out of subshell \n        due to process substitution\n\n\nResult of substituted shell is stored as regular file and could be \n found line this:\n  echo <(echo \"foo\")\n\n\n\n\n/dev/fd/63\n   Its contents could be listed:\n  cat <(echo \"foo\")\nfoo\n\n\n\n\ntrap:\n  Event subscription for system signals (SIG..) like SIGKILL etc.\n there are 64 signals, so some variety of situations.\n  In general this is the way to trigger script when some system sent some\n particular event to the script\n\n\nSyntax:\n trap \n signal [signal ...]\n  Where command is what will be executed in case 'signal' is sent to\n the executing script\n !  Also could be a function.\n\n\nExample:\n #!/bin/bash\n trap \"echo 'i am ignoring you'\" SIGTERM SIGINT\n\n\nthis will return 'i am ...' if SIGTERM (kill -9) or SIGINT (CTRL+C) are\n sent to the script.\n\n\nAlso works with functions, so like lambda\\delegates\n\n\nExample:\n  exit_on_SIGINT () {\n   echo \"script Interrupted\" 2>&1\n   exit 0\n  }\n\n\ntrap exit_on_SIGINT SIGINT\n\n\nthis will call exit_on_SIGINT() function when CTRL+C is received\n   and will redirect 'echo' into both STDERR and STDOUT\n  After event is handled, exit 0, will ensure exit of the script, as it\n  is expected by the SIGnal\n\n\ntemp files:\nrandom:\n  event handling (trap) usually used to delete temp files generated \n during script work. Such files could contain some secure info and stuff.\n  To secure from 'temp race attack' - when temp files being searched\n by 3rd party and read, files need to be named in unobvious way\n which means use random gibberish stuff to mask particular files.\n\n\nRandom could be used like $RANDOM\n $RANDOM var containes random number from 1 to 32 767, but this is not\n  really huge number\n\n\nmktemp:\n  command that creates temporary file with name based on pattern\n  or directory\n   creates \n   Syntax:\n  mktemp \n\n  mktemp [OPTION]... [TEMPLATE]\n\n\nCreate a temporary file or directory, safely, and print its name.\n\n  TEMPLATE must contain at least 3 consecutive 'X's in last  component. \n\n  If TEMPLATE is not specified, use tmp.XXXXXXXXXX, and --tmpdir is \n  implied. \n   Files are created u+rw, and directories  u+rwx,  minus  umask\n\n  restrictions.\n\n\nthis will create file at given path, with every X being replaced\n  with random number or letter(random case)\n   NOTICE: in general convers 4+ X into random letters\\numbers\n  and only at the end of the file name\n   except if it has extention (like .txt)\n\n\nExample:\n  #!/bin/bash\n  tempfile=$(mktemp ~/.tmp/XXX-$(basename $0).$$.XXXXXXXXX) \n  echo $tempfile\n   which will produce:\n  script output: >XXX-test.17306.PRsr7hXuU\n  $ ll XXX*\n  -rw------- 1 dos dos 0 \u043a\u0432\u0456  2 03:07 XXX-test.17306.PRsr7hXuU\n\n\nGenerally it is more safe to use custom temp directory instead of /tmp\n  Example:\n [[ -d ~/.tmp ]] || mkdir ~/.tmp\n  this will check if ~/.tmp exists, if not will create it\n  But in general mktemp also could create directories.. \n\n\nAsynchronous\nAsync async\n Bash could run async commands and tools.\n To run something in background is should ends with &\n  Example:\n\n\n\n\ncopy_script.sh &\n  copyscript will now run in background\n $! - returns PID of last started in background process\n  Example:\npid=$!   - will save PID of running copy_script.sh into pid variable\n\n\n\n\nwait:\n waits for given process to finish, before continue execution\n  Example:\n wait $pid - will wait for copy_script.sh to finish before continue\n\n\nPipe\npipe\n Pipes are the files created in special pipefs filesystem\n  by default unnamed pipe are created for every | symbol in command chain\n in following command 2 pipes will be created:\n  ls -l | grep foo | less\n output of 'ls -l' will be redirected into pipeA\n input into 'grep' will be reead from pipeA\n STDOUT of 'grep' will be written into pipeB\n STDIN of 'less' will be read from pipeB\n\n\nevery command will be executed in subshell, need to clarify more\n\n\nmkfifo:\n  creates Named pipe, which is stored as a file in regular file system\n   Syntax:\n  mkfifo /path/to/file\n   To write into named pipe:\n  command1 > named_pipe\n   To read from named pipe:\n  command2 < named_pipe\n\n\nfile will have attribute 'p':\n      prw-r-----\n   Example:\n  1st Terminal:\n   mkfifo pipe1\n   ls -l > pipe1 - will halt until everything is read out from pipe1\n  2nd Terminal\n   cat < pipe1 - will read everything from pipe1 and display, 1st terminal\n        will continue its work afterwards",
            "title": "Bash page"
        },
        {
            "location": "/bash_page/#intro",
            "text": "scripts start from instruction which interpreter to use, first line always must\nbe the directive. Except if script is passed as param into interpreter, then\nthere is no need in that because there already an interpreter in place.  #!/path/to/interpreter  i.e.:  #!/bin/bash  Shortcuts:\n  ctrl+a - move to start of line\n  ctrl+e - move to end of line\n  ctrl+b - move one char back\n  alt+b  - move one word back\n  ctrl+f - move one char forth\n  alt+b  - move one word forth\n  ctrl+d - delete one char under cursor\n  ctrl+u - delete from cursor to line start\n  ctrl+k - delete from cursor to line end\n  ctrl+w - delete from cursor to word start\n  alt+bcksp - delete previous word\n  ctrl+y - paste from clipboard(alt+bckspc deleted)\n  ctrl+l - clean screen\n  ctrl+r - reverse search in history\n  ctrl+j - edit found command in search\n  ctrl+p - previous command in history\n  ctrl+n - next command in history  Bash Scripts accepts data from stdin and its direct redirection or pipe \nNote: 'read' command need to be used.    Example:   >script:  \n>read -p \"enter any *.txt\"  \n>echo \"${PWD}${REPLY}\"  \n$cd ~ && ls test.txt | ./read_and_expect_txt_files.sh  \n/home/dos/test.txt  Example:    $echo \"test.txt\" > try.txt  \n$./read_and_expect_txt_files.sh < try.txt  \n/home/dos/test.txt",
            "title": "Intro"
        },
        {
            "location": "/bash_page/#debugging",
            "text": "#!/bin/bash -x  will start debugging tracert - with display of all values expanded and\n  lines as bash sees them and will execute them  set -x  \necho $var  \nset +x  will enable debugging tracert only between set -/+ x displaying only\n commands in between with such debug info",
            "title": "Debugging:"
        },
        {
            "location": "/bash_page/#shell-variables",
            "text": "$FUNCNAME - contains name of the function being executed(like reflection)    $PS4 env var, is Promt String for debugging tracert \n$LINENO - is standard variable with line number \nPS4=\"$LINENO +\" - will display line number during debugging tracert    $OLDPWD - contains previous workind directory \n$RANDOM - contains random number from 1 to 32 767 \n$$ - current PID of the programs\\script being executed",
            "title": "shell variables:"
        },
        {
            "location": "/bash_page/#defensive-programming",
            "text": "Make sure everything is expanded and is executed in right place: \n Instead of:    $cd $path  \n$rm *  Use secure defensive way:    [[ -d \"$path\" ]] && cd \"$path\" && echo rm *  where:      -d - make sure path exists and is Directory    \"$path\" - in double quotes to avoid null expansion if var does not exist\n    it will be expanded in empty string in that way - \"\"    && - will execute next command if previous returned 0 exit code    echo rm * - will echo expanded by bash shell wildcard, displaying what \n    exactly is going to happen with actual 'rm *'",
            "title": "Defensive programming:"
        },
        {
            "location": "/bash_page/#naming-coventions",
            "text": "Variable Names: \n Lower-case, with underscores to separate words. \n Ex: my_variable_name    Constants and Environment Variable Names: \n All caps, separated with underscores, declared at the top of the file. \n Ex: MY_CONSTANT    Google naming convetions:   \n    https://google.github.io/styleguide/shell.xml#Naming_Conventions",
            "title": "Naming Coventions:"
        },
        {
            "location": "/bash_page/#execution",
            "text": "bash searches for entered command in    built-ins  hashes  bash history   env var PATH \n   4.1. PATH is split by ':' and every path is searched for the command    new child process of bash created - fork(), and all ENV copied into it  found command is executed in the child copy of bash and replaces it - execve()    main bash process executes wait() until the child process ends  after child process ends cleanup starts and destroys the process    So if script/binary is executed from some(i.e. current) directory and it is\n  not in path , it have to be 'source'd like:     source scriptname   or:      . scriptname  . is same to source       or:      ./scriptname  this seems to be  /scriptname (so basically full path, which is also applies)",
            "title": "Execution:"
        },
        {
            "location": "/bash_page/#expansion",
            "text": "Expansion is done by shell(i.e. bash) and expanded result is passed to shell\\command to be executed. If something is escaped by escape sequence, it will not be expanded by shell, and  could  be expanded by the program receiving escaped sequence. \n expansion works before pass anything further, so:     cp * /some/dir  will copy everything from current dir into /some/dir \n  first argument * actually will be passed to cp as list of files matched \n  by wildcard, 'cp' will never see * itself \n  to check what * will return, execute  echo *",
            "title": "Expansion:"
        },
        {
            "location": "/bash_page/#variables-expansion",
            "text": "Expansion starts when $ is encountered. \n  Escape sequences: \n  \\ - will escape 1 symbol to treat it as literal and not meta symbol \n  '' - everything between single quotes threated as literals too \n  \"\" - will expand variables, but escape spaces etc. \n   Variables are expanded before passing them to command so command will only\n  see var's value, not its name. \n   Bash creates variable when first mets it, so it will create even empty\n  variables, which could lead to errors:    foo=\"/some/file\"  \nmv $foo $foo1  # error   Bash will create $foo1 variable , then mv will receive $foo1 value which\n  is null, and 'mv /some/file  ' will get error as only 1 param is passed \n   To rename file to  'some/file1'  use  {} :         mv $foo {$foo}1",
            "title": "variables expansion:"
        },
        {
            "location": "/bash_page/#positional-parameters",
            "text": "Bash supports positional params for the script. \n  default ones are $0-$9 , other params are listed by ${10} and so on:      $0 ---- contains full path of the script being executed \n----    does not count as parameter in list $# \n----    so [ $# -gt 0 ] will return false if $1 is empty      $1+  ---- are actual parameters",
            "title": "Positional parameters:"
        },
        {
            "location": "/bash_page/#parameters-are-passed-as-usual",
            "text": "Example:    script.sh param1 param2  in script.sh:    echo $0  #/path/to/script.sh\necho $1  #param1\necho $2  #$param2\necho $3  # this would be empty uninitialized param with null value",
            "title": "Parameters are passed as usual"
        },
        {
            "location": "/bash_page/#paraters-could-be-passed-using-wildcards-like",
            "text": "Example:    script.sh *  will pass everything matched, basically it would be a\n        list of files in PWD (same as  echo * )",
            "title": "Paraters could be passed using wildcards like"
        },
        {
            "location": "/bash_page/#params-arrays",
            "text": "$# -  .Count , stores number of arguments passed into the script \n    does  not count $0  in it, so is not 0 based  $* - stores all arguments - ($1 $2 ...), if args has spaces those \n    would be treated as separate params, so params  \n    (\"param1\" \"param with spaces\") would be  $1=param1 $2=param $3=with $4=spaces     \"$*\" - all params expanded between (\"$1 $2\" ...), with delimiter of first\n    symbol in $IFS variable, it becomes one single parameter: \n    (\"param1\" \"param with spaces\") would be  $1=\"param1 param with spaces\"  (leaving quotes behind in the end)    $@ - same as $*    \"$@\" - stores all arguments in quotas (\"$1\" \"$2\" ...), thus handling \n    spaces in variables. \n    (\"param1\" \"param with spaces\") would be  $1=\"param1\" $2=\"param with spaces\"   \n    See 'variable substitution' for more use - ${@:2} (w/o \" works same)",
            "title": "Params Arrays:"
        },
        {
            "location": "/bash_page/#shift",
            "text": "shifts params backwards so $2 would be $1 and old $1 would vanish\n  using this command each parameter could be handled one by one \n   Example:    while [[ $# -gt 0 ]]; do\n echo \"Parameter is $1\"\n shift\ndone  This will echo out all parameters given as per shift will consequentially\n  move all parameters into $1 position one by one",
            "title": "shift:"
        },
        {
            "location": "/bash_page/#basename",
            "text": "returns file name from latest node in the path \n   Example:    $/home/dos/scripts/some_script.sh\nin script >> echo basename $0\nwill return 'some_script.sh'",
            "title": "basename:"
        },
        {
            "location": "/bash_page/#dirname",
            "text": "returns directory name (everything except latest nodein the path) \n  antipod of  basename \n   Example:  cd /home/dos/directory/filename\ndirname $PWD\n> /home/dos/directory",
            "title": "dirname:"
        },
        {
            "location": "/bash_page/#variables",
            "text": "AKA Global Variables - are visible everywhere in shell environment\n   if 'export'ed - will be visible in subshells created from the shell \n   'local' variables are local for funcions - see below \n  is converted to 'String' class by default \n  Initializate variable by = sign w/o leading or trailing spaces \n  Access variable by $    Also could be int, if used with 'declare':    declare -i variable=1  COuld be readonly:    declare -r READONLY=\"this is constant, in uppercase by convention\"  Variables definitions:    a=z                     # assign string \"z\" to var 'a'\nb=\"a string\"            # assign \"a string\" w/ space to var 'b'\nc=\"a string and $b\"     # expansion works in \"\" so 'c' value \n                        # is \"a string and a string\"\nd=$(ls -l foo.txt)      # result of command is assigned to 'd'\ne=$((5 * 7))            # result of ariphmetic result is assigned to 'e'\nf=\"\\t\\tastring \\n\"      # escaped sequences also works in \"\" here:\n                        # \\t - tab\n                        # \\n - new line  More than one variable could be declared in 1 line:    a=5 b=\"a string\" # will create two variables with\n                 # values \"5\" and \"a string\", probably 5 is int.ToString()  To preserve spaces and stuff use quotas with string vars initialization \n   Example:    my_str_var=\"string w/ spaces goes in quotas\"\necho \"stuff is: $my_str_var\"   # stuff is: string w/ spa.....\necho 'stuff is: $my_str_var'   # stuff is: $my_str_var",
            "title": "variables:"
        },
        {
            "location": "/bash_page/#arrays",
            "text": "also variables, indexed from 0,  \n  could have empty indexes. \n   Example declaration:    a[10]=Ten   # assign 'Ten' to 11th index in array length of 1  Powershell-like thing(or vice versa), except w/o @     days=(Mon Tue Wed Thu Fri Sat Sun)\ndays=([0]=Mon [1]=Tue ... )    # same as above but with strict indexing\nanimals=(\"a dog\" \"a cat\" \"a cow\")  If array addressed w/o index, index 0 is displayed, if it is null then nothing\n  displayed EVEN IF THERE ARE OTHER INDEXES \n   Example:    arr=([1]=one [2]=two)\necho $arr       # nothing returned\narr[14]=14\necho $arr       # nothing returned\narr[0]=0\necho $arr       # 0 is returend\necho ${arr[@]}  # 0 one two 14 is returned  Example2: assignment of array w/o index specification    arr=(a b c d)\necho ${arr[@])  # a b c d - is returned\narr=AAA\necho ${arr[@]}  # AAA b c d - is returned\necho $arr       # AAA - is returned  Example3: deletion of first index w/o index specification    arr=(a b c d)\narr=\necho ${arr[@]}  # b c d - is returned  Iteration:  Array variable substitution(see below for strings) arrays could be iterated\n  and counted and checked for empty indexes: \n   Iterate array:    ${animals[*]} # returns list of values split by space\n${animals[@]} # same as above  Example:    arr3=(\"a first\" \"a second\" 'a third')      # \" or ' is fine\nfor i in ${arr3[*]}; do echo $i ; done\n\n>a          # separate variables\n>first      # also separated by spaces(split)\n>a          # will work fine if no Spaces\n>second\n>a\n>third  \"${animals[*]}\" - JOIN joins all elements into 1 string \n Example:   for i in \"${arr3[*]}\"; do echo $i ; done   # only \" , ' will not work\n >a first a second a third                  #joined string  TODO: CONTINUE FROM HERE  Best form to use:\n  \"${animals[@]}\" - returns list of values NOT split by space(real contents)\n    Example:\n   for i in \"${arr3[@]}\"; do echo $i; done   a first     # only different values are separated\na second        # spaces are in place\na third     # best form to use   Counting:\n  Array count coun be calculated, and length of value in single index:  .Count:\n  ${#array[@]}    \n   where:\n  array is var name\n   Example:\n  echo ${#arr3[@]}   3            # there 3 elements 'a first' 'a second' 'a third'   array[i].Length:\n  ${#array[i]}    \n   where:\n  array is var name and i is index to measure length of\n   Example:\n  echo ${#arr3[1]}   8            # 'a first' is 8 characters long   Check indexes numbers:\n   Arrays could have uninitialized or null initialized elements under some\n  indexes.\n   The way of how to check which indexes are taken is:\n  ${!array[@]}      # works same w/ and w/o double quotes\n  ${!array[ ]}      # works same as above w/o quotes, \n            # but differently w/ quote, see example below in NOTE:\n   Example:\n  arr=([1]=\"a one\" [3]=\"a three\" [5]=\"a five\")\n  for i in ${!arr[@]}; do echo $i; done \n           \"${!arr[@]}\"         # will do the same\n           ${!arr[ ]}           # will do the same   1\n3\n5   NOTE: @ works same With or WithOut quotes:\n        ${!arr[@]} == \"${!arr[@]}\n   AND\n        * works same as above WithOut quotes\n   BUT \n        in double quotes it will JOIN indexes in one string with spaces separators\n   Example:\n  for i in \"${!arr[*]}\"; do echo $i; done   1 3 5                # produces one line of indexes   array.Add:\n  Arrays could have new elements added into the end of it.\n This is made as in Powershell(or powershell take it from bash) using +=\n  Example:\n arr=(a b c)        # echo \"${arr[@]}\" > a b c\n arr+=(f e g)       # echo \"${arr[@]}\" > a b c f e g  array.Sort:\n  There is no default sort method, but it could be implemented with\n piping and regular 'sort' command, but with Iteration:\n  arr_sorted=$(for i in \"${arr[@]}\"; do echo $i; done | sort)\n    this will save soreted \"a b c e f g\" into arr_sorted\n   !!!! BUT THIS WILL NOT WORK    !!!\n   !!!!   echo \"${arr[@]}\" | sort !!!  Delete array:\n  Deletion of array is done in the same way as deletion of variables\n using 'unset' command.\n DO Not Use '$' in unset.\n  Syntax:\n unset array_name  Example:\n arr=(1 2 3)\n echo ${#arr[@]}    # check Count - returns 3\n echo ${arr[@]}     # check contents - returns 1 2 3\n unset arr      # deletes arr, NOTICE no '$' before 'arr' NAME\n echo ${#arr[@]}    # cehck Count - returns 0, as for anything unset\n            #+ because bash initializes on execution call\n echo ${arr[@]}     # returns nothing  variable substitution:\nsubstitutuin:\n   Bash supports various variables substitutions:\n  $a - will be substituted with 'a' value\n  ${a} - same as $a but could be concatenated w/ string w/o spaces:\n    ${a}.txt - will be expanded in a_value.txt\n  ${11} - 11th positional parameter given to script from shell\n  ${var:-word} - if 'variable' is set, the result will be its value\n    if 'variable' is unset - the result will be 'word'\n  $(var:=word} - if variable is set results in its value substituted\n    if variable is unset, it will be assigned to 'word'\n    such assignment will not work for positinal params(see 'shift')\n    and other special variables\n  ${var:?word} - if variable is unset error with reason 'word' will be\n    generated, exit code of such construct will be 1\n  ${var:+word} - if 'variable' is set, the result will be 'word',\n    (but variable's value will not be changed)\n    otherwise result will be EMPTY string\n  Example:\n    $ echo ${variable:-ls} - variable unset - ls used\n    > ls\n    $ export variable=1\n    $ echo ${variable:-ls} - variable is set- its value used\n    > 1\n    $ echo ${variable:+ls} - variable is set - ls used\n    > ls\n    $ echo ${variable1:+ls} - variable unset - empty line used\n    > \n  ${!prefix } or ${!prefix@} - returns NAMES of existing variables\n    that starts from 'prefix.\n   Example:\n    $ echo ${!BASH }\n    > BASH BASHOPTS BASHPID BASH_ALIASES BASH_ARGC BASH_ARGV BASH_CMDS  string variables substitution:\n  ${#var} - returns length of string in variable's value\n    Example:\n   $ var=123456789   #this could be interpreted as a string too now   9           #string length is 9   ${#} or $# or ${#@} or ${#*} - returns number of positional parameters\n    of the script being executed  ${var:number} - return string from number to the end, spaces trimmed\n          variable is unchanged.\n    Example:\n   $ var=\"This string is to long.\"\n   $ echo ${var:5}         #returns string from 5th symbol   string is to long.\n    Example: spaces are trimmed:\n   $ echo ${var:5} | wc -c     #count chars\n   $ 19\n   $ echo ${var:4} | wc -c     #return starts from space\n   $ 19                #space is trimmed so same number of chars\n  ${var: -number} - return string from end to number, spaces trimmed\n            NOTE - space between ':' and '-' signs\n    Example:\n   $ echo ${var: -5}\nlong.\n  ${var:number:length} - return string from number till end of lenth\n    Example:\n   $ echo ${var:5:6}\nstring   ${var: -number: -length} - return string number between number(from the\n                 end) and length (also from the end)\n                 NOTE: number must be > than length\n    Example:\n   $ echo ${var: -18: -2}    #var is This string is to long.   string is to lon   ${@} - return all values of positional params \n     leaving spaces inside strings (like \"$@\" ) - bcs it know how \n     many arguments script has\n     ${*} is the same form, it seems  ${@:num} - displays values of positional params but from num\n     $(@:1) - works same as ${@}\n     ${@: -2} works , but starts from the end  ${@:num:length} - same as with strings but with positional params\n  ${@: -num: -length} - same as with strings but with positional params  ${param#pattern} - finds shortest match and deletes it (lazy match)\n    Example:\n   foo=\"file.txt.gz\"\n   ${foo#*.}   txt.gz\n  ${param##pattern} - finds longest match and deletes it (greedy match)\n   Example\n   ${foo##*.}\n.gz   ${param%pattern}  - same as #  but deletes from the end of the file\n   Example:\n   foo=file.txt.gz\n   ${foo%. }        - note .  instead of *. in # example   file.txt\n  ${param%%pattern} - same as ##\n   ${foo%%.*}\nfile   Search and replace:  ${param/pattern/string} - replaces first occurance of pattern with string\n ${param//pattern/string} - replaces all occurances of pattern with string\n ${param/#pattern/string} - replaces only if at the beginning of the line\n ${param/%pattern/string} - replacesonly if at the end of the line  $(())\n  Accepts any valid arithmetic expression \n   Pretty similar to (()) test construct which returns true when result >0\n Accepts any number system:  Decimal\n   with base of 10\n  number\n   Example:\n  echo $((10)) - will return 10\n  echo $((10#10)) - will return 10  Octal: \n   with base of 8 [0-7]\n  0number\n   Example:\n  echo $((010)) is 8 in decimal, where 10 is the whole base, which is 8\n  echo $((07)) is 7 in decimal\n  echo $((8#10)) is 8 in decimal\n  echo $((08)) - will return error 'value to great'\n        0-7; 10-17 and so on will work  Hex: Hexademical:\n   with base of 16 [0-9A-F]\n  0xnumber\n   Example:\n  $((0x10)) - is 16 in decimal, which is full base\n  $((16#10)) - is also 16, because of base of 16 numbers\n  $((16#100)) or ((0x100)) is 256 which is 16x16 or 16 in square(^2)\n  $((0xFF)) is F(15) full bases plus F(15) \n        which is ((15x15)==240)+15 == 255  Custom:\n    custom base , could be any number [0-9A-Z] and some other]\n    maximum base is 64, seems like\n   base#number\n    Example:\n   echo $((17#10) - is 17, which is full base\n   echo $((64#10)) - is 64, which again is full base  Arithmetic expressions syntax $((arithmetics go here)):  Example:  echo $((2+3)) >> 5  IFS - variable that containes field separator, \n    by default is space/tab/new line\n    Could be changed  Temporary variables:\n   Bash supports variables change/assignment before command execution\n  Variables assigned before command execution (on the same line) will be\n  changed only for env with which command will be executed:\n   IFS=\":\" read field1 field2 ... <<< \"$line_from_passwd\"\n    Equals to:\n   OLD_IFS=\"$IFS\"\n   IFS=\":\"\n   read field1 field2 .... <<< \"$line_from_passwd\"\n   IFS=\"$OLD_IFS\"\n  !Note:\n   it works weird - only in example from bood, need to research and update  Unary operators:\n   Operators that require value on UNE side, contrary to binary operators\n   that reauire values on both sides:\n    + - is for positive numbes: +1\n    - - is for negative number: -1  Arithmetic operators: Binary operators:\n    there are more binary operators, but those below are used in Arithmetics\n  + - for addition\n  - - for substruction\n  * - for multiplication\n  / - for division\n    - for exponentiation (2 2=4 3 3=9 4*4=16 and so on, aka ^2 ^3 ^4 ...)\n  % - modulus division \n    5%2=1 which is what left after division - 2, 2 and 1\n      Example:\n    echo $(( 5%2 )) - will return 1  Assignment in Arithmetic expressions:\n  echo $((foo=5))  - will assign 5 to $foo\n   NOTE: also could be used in tests like [[...]] and ((...))\n   NOTE: in 'test' or [...] used to  compare  strings, BEWARE  Assignment operators:\n   var = value \n    regular assignment operator\n   var += value \n    assignment with value addition      $var = $var + value\n   var -= value\n    assignment with value substruction  $var = $var - value\n   var *= value\n    assignment with value multiplication    $var = $var x value\n   var /= value\n    assignment with value division      $var = $var / value\n   var %= value\n    assignment with value modulus division  $var = $var % value  Assignment with C-like style increment decrement:\n   Post - means show\\use first, change after it shown\\used\n  var++ - post increment            $var = $var + 1\n  var-- - post decrement            $var = $var - 1\n   Example:\n   foo=1; echo $((foo++)); echo $foo\n      1        2\n   Pre  - means change\\use first , show after it changed\\used\n  ++var - pre increment             $var = $var + 1\n  --var - pre decrement             $var = $var - 1\n   Example:\n   foo=1; echo $((++foo)); echo $foo\n      2        2  Basic logical gates:\n   AND - returns true if both entry are true\n   OR - returs true if at least one (or both) entry is true \n   XOR - returns true if entry differs, \n     both true and both false return false\n   NOT - inverts entry - true is false and false is true\n   NAND - both false are true\n   NOR - returns true if at least one (or both) entry is false\n   XNOR - returns true if not differs,\n      both true or both false return true  Logical operators in Arithmetics:\n   Comparison operators less, greater , less than and so on\n  <= - less or equal   = - greater or equal\n  < - less\n- greater\n  == - equal\n  != - not equal\n  && - AND\n  || - OR   expr1:expr2?expr3 - trinary operator, \n              if expr1 is true \n              then expr2 executed\n              else expr3 is executed\n   Example:\n  a=0\n  ((a<1?++a:--1))   # a+=1 and a-=1 will not work\n  echo $a       # 1\n  ((a<1?(a+=1):(a-=1))) # but this will work just fine\n  echo $a       # 0  local variables:\n  local variables are declared in functions, and visible only there\n  local variables overwrite globals with the same name\n   Syntax:\n  local variable_name[=optional_value]\n   values are optional , probably, in order to delete some global vars inside\n  a function(i.e. vars are used as standard in\\out params of a command)\n   Example:\n  foo=0\n  func() {\n   local foo\n   foo=1\n   echo \"in func $foo\"\n  }\n  func\n  echo \"in script $foo\"\n   Result:   in func 1\nin script 0   here doc here docs here-document here script :\n  allows multiline input and it WILL expand even if in ''.\n   But will not expand if escaped - \\$\n   Syntax:\n  command << indicator\n  text\n  indicator  <<-  - work as << but accepts TAB before enclosing 'indicator'\n    'space' will still return error  where:\n  command is a command like, 'cat' or anything else (doesn't work with 'echo')\n  where text between 'indicators' is expanded by shell and sent to STDIN of\n  the command\n   Note that also <<- could be used, with it bash will ignore leading\n  tabs in 'indicator'\n   Note that seconds 'indicator' has to be on separate line and \n  Must have no spaces before or after it, otherwise bash interpreter will \n  conntinue to look for 'indicator' ignoring the one with leading/trailing \n  spaces\n   Value of 'indicator' could be any but 'EOF' is preffered  Example:\n  $ foo=\"text\"\n  $ cat <<  EOF   $foo\n\"$foo\"\n'$foo'\n\\$foo EOF \n   Result:\n  text\n  \"text\"\n  'text'\n  $foo   Example of use with 'ftp'\n  ftp -n << EOF\n  open $FTP_SERVER_ADDRESS\n  user anonymous username@hostname\n  cd $FTP_PATH\n  hash\n  get $REMOTE_FILE\n  exit\n  EOF\n  ls -l $REMOTE_FILE - will display downloaded from ftp remote file, all the\n    commands are passed from inside the here-document  here-line:\n  same as here-doc but onliner:\n   $read field1 field2 <<< \"test in put\"\n   $echo $field1  ---> test\n   $echo $field2  ---> in put  < because put is 3rd field, and we have only 2\n                all excess fields goes into last 'field2'\n    Profid of here-line here is that commands with pipe\n   $echo \"test in put\" | read  \n    will not work because subshell will be created for 'read' where $REPLY\n    will be created and assigned, but after that subshell ends, and\n    subshell can not change parent shell's env, such as REPLY value  if:\n  Condition tests in IF should be inside square brackets - [], which are just a\n  reference for commant 'test'. see MAN page for TEST for details.\n   example:\n  if [ 100 -eq 100 ]; then - semicolon is needed in case of single line w/o /n\n   -or-\n  if [ 100 == 100 ] - seems like this also should work\n   -or-\n  if $(test $(echo \"$REPLY\" | grep '^[0-3]$'))\n    tests whether value of REPLY is mathced- if not, grep return 1(error)\n    then test returns 1 and 'if' will not work\n    if value matched then grep returns 0 (good), then test returns 0 and\n    'if' will work\n   -or-\n  if test $(echo \"$REPLY\" | grep '^[0-3]$')\n    same as above, but $() outside 'test' is unnecessary in fact\n    but works in both ways so keep both examples here\n   -or-\n  if [   ] && [   ] || [   ]\n  then\n     \n  elif [   ]      # else if, onliner gotta be ; ended\n  then\n     \n  else              # one liner gotta be ; ended\n     \n  fi  lots info here:\n  https://ryanstutorials.net/bash-scripting-tutorial/bash-if-statements.php  && ||:\n  allowed inside bash shell logical AND and OR:\n   Example:\n  mkdir test && cd test\n   will create and than change dir to 'test'\n  [ -d test ] || mkdir test && cd test\n   will check whether 'test' exists, returns 0 or 1\n   || continues only if 1 was returned by 'test' and proceeds with mkdir\n   && continues only if 0 was returned by 'mkdir' and proceeds with cd  test:\n  Tests whether expression returns 0 or not\n  after evaluation command 'test' returns 1 if true and 0 if false\n   test 100 -eq 100 - test whether 100 equals 100\n  or [ ]\n   if [ 100 -eq 100 ]\n  used to test conditions  Logical operators:\n   -a - AND\n   -o - OR\n   ! - NOT\n    Example:\n  [ ! ( FALSE -o TRUE )   -- (TRUE) but [ FALSE]\n   note the escaped parentheses\n   note that 'false' and 'true' command will behave differently , bcs returns\n  interegs 1 and 0 respectively  Requires shell expansion symbols $ \n   Example:\n  if [ $num1 -lt $num2 ]; then echo 'int 1 less than int2'; fi  BEWARE!!!!!!\n  shell creates empty variable if it is not defined\n  and $num1 -lt $num2 will return TRUE if both undefined  CHECK YOUR VARIABLES BEFORE USE  Test Strings\\Ints:\n  Operator  Description\n  !    The EXPRESSION is false.\n  -n str    The length of STRING is greater than zero.\n  -z str    The lengh of STRING is zero (ie it is empty).\n  str1 == str2      STRING1 is equal to STRING2 (= also allowed)\n  str1 != str2      STRING1 is not equal to STRING2\n  str1 > str2       STRING1 is alphabetically greater than STRING2\n  INTEGER1 -eq INTEGER2     INTEGER1 is numerically equal to INTEGER2\n  -ne               integers are not equal\n  -le               are less or equal\n  -ge               greater or equal\n  INTEGER1 -gt INTEGER2     INTEGER1 is numerically greater than INTEGER2\n  INTEGER1 -lt INTEGER2     INTEGER1 is numerically less than INTEGER2  Test Files:\n  FILE -ef FILE1 - both files are hard links (point to same inode)\n  FILE -nt FILE1 - FILE newer than FILE1\n  FILE -ot FILE1 - FILE older than FILE1\n  -e FILE   FILE exists.\n  -f FILE   FILE exists and is file\n  -s FILE   FILE exists and its size greater than 0\n  -L FILE   exists and is soft symblic link\n  -d FILE   FILE exists and is a directory.\n  -b FILE   exists and is Block device (i.e /dev/sda1)\n  -c FILE   exists and char device\n  -r FILE   FILE exists and the read permission is granted.\n  -s FILE   FILE exists and it's size is greater than zero (ie. not empty).\n  -w FILE   FILE exists and the write permission is granted.\n  -x FILE   FILE exists and the execute permission is granted.\n  -G FILE   exists and file belongs to existing group\n  -O FILE   exists and belongs to existing user  NOTE:\n  == - does a string comparision\n  -eq - does numerical comparison  Escaping:\n  symbols < > ( ) need to be escabed by \\ or be between ''\n  because [ ] is just command test, and bash will try to use them as its own \n  meta symbols (< > are stdout\\in redirecton, so could be a MESS)  [[ ]]:\n  improved 'test' command but not POSIX compatible\n    Differences\\Features:  Does not need expansion symbol $, expands just fine w/o it  UNLESS IS A STRING, then use \"$VAR\", to expand and prevent empty values\n    Example:\n   $ if [[ $num1 -lt num2 ]]; then echo \"$num1 less than $num2\"; fi   1 less than 2\n    NOTE:\n   $num1 and num2 are both fine as per expansion works in some other way   BEWARE OF UNDEFINED VARIABLES\n   IF BOTH ARE UNDEFINED 0 WILL BE RETURNED!!  Can compare regex.  Seems like gotta match full line, not just some part.\n   uses Extended Regex (ERE)\n    Requires $ symbol to expand variable value\n    use \"\" to prevent empty values\n   No need to escape bash meta symbols like \\ or *\n    Example:\n   $ if [[ \"$string_var\" =~ \\w+? ]]; then echo \"good\"; fi   good   Can use wildcards , or similar to it..  When using == equation\n    Example:\n   $ FILE=foo.txt\n   $ if [[ $FILE == foo.* ]]; then echo \"matches pattern\"; fi   matches pattern   Logical operators:\n   && - AND\n   || - OR\n   ! - NOT\n    Example:\n   if [[ ! ( FALSE || TRUE )]]; --- in (will be TRUE) but ! will invert to FALSE\n    Note that 'false' and 'true' are not commands, commands return 1 and 0\n   and behavios incorrectly  (( )):\n  Permits Arithmetic expansion and Evaluation for \n  Works only with Integers (no float dot like in Double or Float types)\n   Note:\n  bash syntax part, not a command\n  also does expansion w/o $ symbol before var name  Arithmetic expansion examples\n   a=$(( 5 + 3 )) or a=$((5+3)) - assign result of addition to variable 'a'\n   C style manipulations:\n   (( var++ )) or ((var++)) - display then increment by 1\n     $(( var++ )) - increments and saves var but also produses errors\n   (( ++var ))  - increment by 1 then display\n   (( --var ))  - same manner decrement\n   (( var-- ))  - display then decrement\n   Assignment:\n   $((b+=a)) - add 'a' to 'b' and assign result to 'b' - b=$a+$b  Evaluation:\n   if ((10>=5)); then echo \"10 greater or equal 5\"; fi\n      ((var1>=var5))  - same byt with variables  Logical operators:\n   same as in [[ ]] - && , || , !  (()) returns 'true' if integer inside of it greater than 0\n   Example:\n  dos:bash$ if ((1)); then echo \"true\"; else echo \"false\"; fi   true\n  dos:bash$ if ((0)); then echo \"true\"; else echo \"false\"; fi\nfalse   Loops:\nfor:\n   Has two forms, first is foreach:\n    Syntax:\n  FOR var IN words; DO \n    commands\n  DONE\n   var - custom name as in regular foreach\n     usually i j k l m - thanks to Fortran where int vars must start\n     with those letters\n   words - is a collection, could be just 1 2 3 4 \n       or result of command\n       or result of wildcard matches list\n       or result of subshell execution\n       if omitted - args passed from commandline\n   do and done are the braces\n   Example:\n  for i in 1 2 3;  do echo \"print $i\"; done   print 1\nprint 2\nprint 3   Example:\n  for file in test*.txt; do echo $file; done",
            "title": "arrays:"
        },
        {
            "location": "/bash_page/#will-echo-all-files-from-current-dir-matched-by-globbedwildcard",
            "text": "test*.txt mask   Example:\n  for i in $(some commands executed in subshell); do....  If optional 'words' component is omitted in 'for' it uses args(params) as\n  collection to iterate through  Example:\n  for i; do echo \"command line param 1 is: $1\" shift; done\n   this will echo out all params of bash script one by one (shift command)  Second form of 'for' is c-like (like real 'for' in c# and not 'foreach'):\n    Syntax:\n   for (( expr1; expr2; expr3 )); do\n    commands\n   done  where \n    expr1-2-3 - are arythmetic expressions  Example:  for (( i=0; i>5; i++ )); do\n    echo \"i is $i\"      # will output $i from 0 to 4\n   done  This form is equialent to following construction:  (( expr1 ))          # this is assigned before loop\n   while (( expr2 )); do    # loop goes as long as expr2 is true\n    commands\n    (( expr3 ))         # at the end of each iteration expr3 is\n                #+ reassigned/reevaluated\n   done  while:\n  infinity loop while exit code of command\\expression is 0 \n  could read from file(stdin) line by line with STDIN redirect or pipe:\n   Syntax:\n  while true; do    | while [[ true ]]; do | while (( 1 )); do\n   commands\n  done\n    Example:\n   while read field1 field2 field3\n    ..\n   do < file_to_read_from.txt\n    Example:\n   cat file_to_read_from.txt | while read field1 field 2 field3; do ... ; done  example:  while read host ...; do... - or like this if in 1 line\n  do\n    ping -c 3 $host\n    if [[ $call_continue ]]; then\n      continue           # will pass further iteration and start over\n    fi\n    if [[ $call_break ]]; then\n      break         # break loop and go out of it at all\n    fi\n  done < myhosts.txt  will read from myhosts.txt while there are lines, each line will reinit $host  break: will stop loop from execution - just regular break as ususal\n  continue: will pass iteration and start next iteration - also standard one  exit status: \nexit code:\n  bash scripts ALWAYS returns exit code. 'exit' is optional\n  commands ALWAYS return exit code too. \n  Functions ALWAYS return exit code too. 'return' is optional\n  seems like everything returns exit code.\n  Even bash shell 'exit'ed with number will return this number\n  to manually return exit code:\n  exit nnn - to return nnn\n    !!   nnn - MUST be an INTEGER in range 0-255 !!\n  return nnn - to return nnn but from Function\n  exit $? or exit or omitting the 'exit':\n   with no params exit code is taken from latest command executed in the script\n  Variable that contains exit code:\n  $? - contains latest exit code, overriden after any next execution\n    even 'echo $?' will override it with 0\n    Exit code values:\n  0-255 allowed\n    By convetion\n  0 - OK\n  1-255 - various errors   to test Exit Code:\n  echo $? - will return exit code of previous command or value of previously\n        executed script \n  more reading:\n  http://tldp.org/LDP/abs/html/exit-status.html  case:\n  check whether one of condition matches the 'word' in case, if so it\n executes code in particular case and terminates\n  Syntax:\n case $word in      # value of $word will be compared against\n   | ) cmd1 # the   and if tru command1\n         cmd2   # and command2 will be executed\n         ;;     # this ends particular case block\n   )         exit 1     # * match anything, last case 'catch'\n             ;;     # ends 'catch' block values of '$word' either\n            #+ matched by previous cases or this one\n            #+ nothing is left 'case' block unmatched this way\n esac           # this ends whole case, after 1 of the cases \n            #+ matched, or none matched\n  Example:\n while [[ -n $1 ]]; do      #while param is not null\n  case $1 in            #check param\n    -f | --file)    shift   #take $2 as $1 , if -f, \n                #to get file path given with -f option\n            filename=$1\n                #save file path to $filename\n            ;;\n  esac\n Case uses wildcards as pattern:\n ? - one character\n * - anything of any length\n [ABC] - 1 letter - either A, B or C\n [0-9] - 1 number in 0 - 9 range - any digin with length of 1\n a - letter 'a'\n  .txt - anything that ends with '.txt'  Good practice is   true:\n  command that always return 0\n   Example:\n  exit $(true)   - will return 0 exit code\n   or Function:\n  return $(true) - will return 0 after Function\n   or:\n  true       - will return 0 exit code\n   or Funcion:\n  test() {\n   true\n  }\n  test       - this will return 0 in script, and if last line - script\n           will return this same 0 to caller (i.e. bash shell)\n   Same works for False  false: \n  command that always return 1  help:\n  lists all built-in commands\n   help  \n  will return help for that command  shopt:\n  SHell OPTions - built-in command to set\\view shell options  shell types\n  See details:\n  https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work\n  Even more detailed:\n  https://askubuntu.com/questions/879364/differentiate-interactive-login-and-non-interactive-non-login-shell\n there are 4 shell types\n  Interactive Login shell\n    either shell opened via SSH\n    or via ctrl+alt+F1(-F6)\n  Interactive Non-Login shell\n    Opening terminal\n    executing 'bash'\n  Non-interactive Non-Login\n    scripts execution\n    or 'bash -c'\n  Non-interactive Login\n    rare stuff, complex.. should be ssh launched w/o command and STDIN of the\n    ssh should has no TTY - echo command | ssh server\n    or 'bash -l -c command'  How to check which shell is:\n   echo $-\n     if output has 'i' - its Interactive:   himBH    bash -c 'echo $-'   hBc\n     NOTE: that double quotes - bash -c \"echo $-\" still will start interactive\n    shell.. probably due to Shell Expansion of variables   shopt login_shell   login_shell off\n    will mean that it is NOT login shell\nlogin_shell on\n    will mean that is IS login shell   Example for each type:",
            "title": "will echo all files from current dir matched by globbed(wildcard)"
        },
        {
            "location": "/bash_page/#interactive-non-login-shell-regular-terminal",
            "text": "$ echo $-; shopt login_shell\nhimBHs\nlogin_shell     off",
            "title": "Interactive, non-login shell. Regular terminal"
        },
        {
            "location": "/bash_page/#interactive-login-shell",
            "text": "$ bash -l\n$ echo $-; shopt login_shell\nhimBHs\nlogin_shell     on",
            "title": "Interactive login shell"
        },
        {
            "location": "/bash_page/#non-interactive-non-login-shell",
            "text": "$ ssh localhost 'echo $-; shopt login_shell'\nhBc\nlogin_shell     off",
            "title": "Non-interactive, non-login shell"
        },
        {
            "location": "/bash_page/#non-interactive-login-shell",
            "text": "$ echo 'echo $-; shopt login_shell' | ssh localhost\nPseudo-terminal will not be allocated because stdin is not a terminal.\nhBs\nlogin_shell     on      functions:\n  shell could have functions, funcions MUST BE declared before its call,\n  otherwise shell will interpret funcion names as regular commands\n   Functions could be executed and calld from shell, added to scripts or \n  bashrc/profile files so will be available in cmd as regular command\n  'return' command in optional\n   Function MUST has at least one command, in order to mock\\stab 'return'\n  command could be used.\n   Basically every script could be converted into a function just by\n  copuint all the contents after she-bang in between function declaration\n  which is in Syntax below. And replace 'exit' with 'return' keywords\n   Variables in the Function could be set as 'local' overriding outer\n  global variables.\n   $0 in fuction contains name of the script being executed\n   $FUNCNAME - contains name of the function being executed  Syntax:\n  function name {\n    local VAR\n    local VAR2=value\n     \n    return\n  }\n   Or equivalent:\n  name() {\n    \n   return\n  }  Example:  script.sh:\n  function func {\n   echo \"step 2\"\n   return\n  }\n  echo \"step 1\"\n  func\n  echo \"step 3\"  Result:   step 1\nstep 2\nste 3   Example .bashrc:\n  ds() {\n   echo \"disk space utilization for $HOSTNAME\"\n   df -h\n  }  Result after shell restarted \n   (after source .bashrc ds looped and crashed shell):\n  $ ds   disk space utili....\n...output of df -h....   |:\n  pipe character\n  creates new subshell, executes what goes after pipe there, then returns\n  back to parent shell. subshell can not change parent shell - so 'read' command\n  will not work in pipe  breaking line; new line:\n use \\ at the end of the line.\n  NO SPACES OR ANYTHING AFTER THE BACK SLASH.\n Guideline: split the long(80 chars) line into two lines.\n Also break the line everytime there is an &&, | or || characters.\n Example:\ncommand1 \\\n && command2 \\\n || command3 \\\n | command4  read:\n  reads line from keyboard input, accepts here-strings, does not work with |\n  By default saves entered value into $REPLY global variable\n   Synopsis:\n  read [-params] [variable1 variable2 variable3 ...]  Accepts more than 1 variable to save keyboard input\n   Default separator is space or tab or new line(\\n)\n  If less variables are given to save data, all excessive data (delimited \n  fields) will be saved in last variable (like msbuild do when too many params)\n  When too many variables is given - unpopulated vars remain empty string  Params:\n  -a - array, saves input into the array \n  -d   - delimiter, by default space\\tab\\newline\n  -e - use Readline, behaves like keyboard input in bash\n  -n   - read num symbols from keyboard input\n  -p   - display promt befor cursor on the same line (like bash promt)\n  -r - do not inerpret \\ symbols\n  -s - silent, do not display entered data, like in password field\n  -t - time out for wait of entry\n  -u   - use file with given descriptor as STDIN  Command grouping:\ncommand groups:\n Commands could be groupped, there are two types:\n  Groups:\n   { command1; command2; [command3; ... ] }\n  Subshell:\n   (command1; command2 [; command3 ... ])  Commands in Groups are executed in current shell, so ENV variables \n chaned during commands execution are saved into current Shell ENV\n  Subshells are executed in sub shell and current ENV could not be \n modified.  Examples:\n ls -l > output.txt\n echo \"listing of foo.txt\" > output.txt\n cat foo.txt > outpu.txt\n  ==\n { ls -l; echo \"listing of foo.txt\"; cat foo.txt; } > output.txt\n  or\n (ls -l; echo \"listing of foo.txt\"; cat foo.txt) > output.txt  NOTICE:\n  Groups must have spaces next to {}, and last command should end with ;\n  SUbshells do not need extra spaces and ;\n BUT:\n  subshells are taking longer to be executed because of extra subshell\n need to be created. Also more memory is required. \n  Also no parent shell ENV modifications is possible  Example with pipe:\n  { ls-l; echo \"listing of foo.txt\"; cat foo.txt; } | lpr\n this will redirect output through pipe into 'lpr' which is print command.\n  So all the output of 3 commands are redirected at once into 'lpr'\n which is not the same with \n  ls -l | echo \"...\" | cat foo.txt | lpr - will send only foo.txt to\n print  Processes substitution:\n <(commands list)  - processes sending output to STDOUT   (commands list)  - processes getting input to STDIN   Subshell output is interpreted as regular file, and could be redirected\n into another commands as usual.  Example:\n  read < <(echo \"foo\")\n  echo $REPLY - will return 'foo'\n where:\n  'read' reads from STDIN, and saves what is read into $REPLY by default\n     if executed in subshell $REPLY will not be available to \n     parent shell\n        echo \"foo\" | read - will not work\n  <     - STDIN redirection, so STDOUT from <() is redirected as STDIN\n      of 'read' command\n  <(echo \"foo\") - Process substitution, where 'ehco' executed in \n          subshell Group () sends its output to STDOUT.\n        In other words output of this part is represented as\n        usual file, which is redirected into 'read' STDIN\n  echo $REPLY - proves that $REPLY is updated in current shell with\n        value redirected out of subshell \n        due to process substitution  Result of substituted shell is stored as regular file and could be \n found line this:\n  echo <(echo \"foo\")   /dev/fd/63\n   Its contents could be listed:\n  cat <(echo \"foo\")\nfoo   trap:\n  Event subscription for system signals (SIG..) like SIGKILL etc.\n there are 64 signals, so some variety of situations.\n  In general this is the way to trigger script when some system sent some\n particular event to the script  Syntax:\n trap   signal [signal ...]\n  Where command is what will be executed in case 'signal' is sent to\n the executing script\n !  Also could be a function.  Example:\n #!/bin/bash\n trap \"echo 'i am ignoring you'\" SIGTERM SIGINT  this will return 'i am ...' if SIGTERM (kill -9) or SIGINT (CTRL+C) are\n sent to the script.  Also works with functions, so like lambda\\delegates  Example:\n  exit_on_SIGINT () {\n   echo \"script Interrupted\" 2>&1\n   exit 0\n  }  trap exit_on_SIGINT SIGINT  this will call exit_on_SIGINT() function when CTRL+C is received\n   and will redirect 'echo' into both STDERR and STDOUT\n  After event is handled, exit 0, will ensure exit of the script, as it\n  is expected by the SIGnal  temp files:\nrandom:\n  event handling (trap) usually used to delete temp files generated \n during script work. Such files could contain some secure info and stuff.\n  To secure from 'temp race attack' - when temp files being searched\n by 3rd party and read, files need to be named in unobvious way\n which means use random gibberish stuff to mask particular files.  Random could be used like $RANDOM\n $RANDOM var containes random number from 1 to 32 767, but this is not\n  really huge number  mktemp:\n  command that creates temporary file with name based on pattern\n  or directory\n   creates \n   Syntax:\n  mktemp  \n  mktemp [OPTION]... [TEMPLATE]  Create a temporary file or directory, safely, and print its name. \n  TEMPLATE must contain at least 3 consecutive 'X's in last  component.  \n  If TEMPLATE is not specified, use tmp.XXXXXXXXXX, and --tmpdir is \n  implied. \n   Files are created u+rw, and directories  u+rwx,  minus  umask \n  restrictions.  this will create file at given path, with every X being replaced\n  with random number or letter(random case)\n   NOTICE: in general convers 4+ X into random letters\\numbers\n  and only at the end of the file name\n   except if it has extention (like .txt)  Example:\n  #!/bin/bash\n  tempfile=$(mktemp ~/.tmp/XXX-$(basename $0).$$.XXXXXXXXX) \n  echo $tempfile\n   which will produce:\n  script output: >XXX-test.17306.PRsr7hXuU\n  $ ll XXX*\n  -rw------- 1 dos dos 0 \u043a\u0432\u0456  2 03:07 XXX-test.17306.PRsr7hXuU  Generally it is more safe to use custom temp directory instead of /tmp\n  Example:\n [[ -d ~/.tmp ]] || mkdir ~/.tmp\n  this will check if ~/.tmp exists, if not will create it\n  But in general mktemp also could create directories..   Asynchronous\nAsync async\n Bash could run async commands and tools.\n To run something in background is should ends with &\n  Example:   copy_script.sh &\n  copyscript will now run in background\n $! - returns PID of last started in background process\n  Example:\npid=$!   - will save PID of running copy_script.sh into pid variable   wait:\n waits for given process to finish, before continue execution\n  Example:\n wait $pid - will wait for copy_script.sh to finish before continue  Pipe\npipe\n Pipes are the files created in special pipefs filesystem\n  by default unnamed pipe are created for every | symbol in command chain\n in following command 2 pipes will be created:\n  ls -l | grep foo | less\n output of 'ls -l' will be redirected into pipeA\n input into 'grep' will be reead from pipeA\n STDOUT of 'grep' will be written into pipeB\n STDIN of 'less' will be read from pipeB  every command will be executed in subshell, need to clarify more  mkfifo:\n  creates Named pipe, which is stored as a file in regular file system\n   Syntax:\n  mkfifo /path/to/file\n   To write into named pipe:\n  command1 > named_pipe\n   To read from named pipe:\n  command2 < named_pipe  file will have attribute 'p':\n      prw-r-----\n   Example:\n  1st Terminal:\n   mkfifo pipe1\n   ls -l > pipe1 - will halt until everything is read out from pipe1\n  2nd Terminal\n   cat < pipe1 - will read everything from pipe1 and display, 1st terminal\n        will continue its work afterwards",
            "title": "Non-interactive login shell"
        },
        {
            "location": "/devops_conf_2019/",
            "text": "[Gianluca Abezzano] DevOps never sleeps. What we learned from influxDB v1 to v2\n\n\n\n\nDocker is pretty layered system, which is split even more recently after ContainerD is moved into OpenSource by Docker development team.\n\n\nThe layers if Docker infrastructure are:\n\n\n\n\nCLI\n\n\ncommand line interface for accessing Docker daemon  \n\n\n\n\n\n\nREST\n\n\nrestfull interface for accessing daemon. TODO: check whether CLI also uses it\n\n\n\n\n\n\ndockerd\n\n\ndocker service that accepts CLI and REST  calls and translates them further tocontainerd service(daemon)\n\n\nDocker Engine\n\n\n\n\n\n\ncontainerd\n\n\nservice that is moved into separate project. main purpose is to made this \nCore\n service to be fully crossplatform. \ncontainerd daemon exposes its API that is  used by dockerd and others parties interested in communication witn containerd in order to use services it provides.\nIt is currently used everywhere where containers and docker are  used - AWS, Azure and all the cloud providers that adopted docker as engine for containerization. \nAlso this is the core that runs on multiple OS such as: Linux, MacOS, Windows etc.  \n\n\n\n\n\n\nRunC\n\n\nTODO: write RunC description, and probably add more layers..\n\n\n\n\n\n\n\n\nTODO: which of above uses Linux Socket\\TCP Socket in order of communication with outer world.\n\n\nJenkins and k8s\n\n\nMain point in usage of docker infrastructure by '3rd parties' is in acquiring access to docker socket.\n\nIt could be illustrated by starting docker container with docker(sub) running inside it.\n\nSub docker could manipulate parent docker if it has access to parent docker's socker, for instance by mounting socket file to sub docker's container:  \n\n\ndocker run -v /var/run/docker.sock:/var/run/docker.sock -ti docker bash\n\n\n\nin this example docker socket is passed from Host system into the container so docker running in the container could manipulate Host's one.\n\n\nThis is  how Jenkins plugin and k8s work.\n\n\nTracing and effective logs monitoring\n\n\nIn order to trace different important events out of all the system information, unimportant warnings and errors information need to be correctly logged, aggregated and filtered.\n\nMetrics need to be used and calculated, and tracking of events throught different logs need to be applied to that. \n\n\nAfter all the logs are gathered, together with metrics and tracking applied to it, some particular thing could be Traced.\n\n\nMetrics alone is just like slap to the face, it is sudden and only  has shock effect.\n\n\nLogs in raw view are useless, the sence comes after aggregation and ability to filter it.\n\n\nState of the system.\n\n\nMonitoring of the system could be done by monitoring its State:\n\n\n\n\nHardware charasteritsitcs like CPU usage\n\n\nprocesses rununing and resource usage\n\n\nI\\O activities \n\n\netc\n\n\n\n\nBut just snapshot is usually not enough. \n\nLike 'ps -lte' - it could return actual and correct info but it will not gave any meaningful info in order which process works ineffectively or wrong.\n\nFor more efficient monitoring 'top' program will give much more info about processes and how they are utilizing system's resources.\n\nBut this gives realtime results and requres constant attention, it order to find which process does what.\n\nSo for effective monitor we need a tool that will save changes into history and ability to filter and even visualize the history.\n\n\nBut w/o any context even atual, filtered and visualized info makes no sence:\n\n\n\n\nNormal State.\n\n\n\n\nWhen everything in the system runs OK, and everybody is satisfied we could call it the Normal State of the system.\n\nThis state need to be saved with all the metrics calculated and attached, to be used later when need to determine how system works at given point of time.\n\n\n\n\nCurrent State\n\n\n\n\nAt any point of time we could apply all the filters to logs, calculate metrics and visualize statistics (even of some time period). This will give us the Current State.\n\nComparing this \nCurrent State\n with a 'golden standard' like \nNormal State\n will give the answer to the main question:\n\n\nWhether everything is fine with our system Now?\n\n\nDesign applications correctly in order to monitor them later\n\n\nIn order to monitor system state effectively we need the info in first place.\n\nInfo is the logs generated by apps which state we want to monitor.  \n\n\nWhen Application is developed logs generation should be keep in mind. In ideal way app should has API to access logging functionality from outside by other systems: \n\n\n[App [[Logs func + stuff ] API for logs access\\creation]]\n\n\n\nin that way some logs\\events storage facility could use API to ask app for some particular logs an particular point of time.\n\n\nCode Instrumentation (hello Bullseye)\n\n\nExisting code  also could be instrumented (i.e. during compilation)\n\n\nLater on during runtime this added parts will fire and write logs\\events somewhere ( i.e. into some stream). Also this added part could provide API access for other tools.  \n\n\nTools \\ Links\n\n\n\n\nELK(B)\n\n\nPrometheus(google)\n\n\nJaeger\n\n\nbullseye. not related to article, just example\n\n\nOpenCensus(google\\open source)\n\n\nmount-bind docker socket\n\n\n\n\n\n\n[Philipp Krenn - Elastic] Hands-On ModSecurity and Logging\n\n\nBasically this talk is about security breaches that could be logged and monitored uneffectively, thus alerting user too late and with lack of general information.\n\nTool that is demonstrated in order to prevent both breaches and bad monitoring is ModSecurity\n\n\nSecurity issues\n\n\n\n\nthe link\n\n\nThere could be gaps in app with security\n\n\nGaps could be covered outside the app with tools, that filter out traffic analizing its inner parts, like requests (i.e. request having 'update' sql directive will be filtered out in some case, and 'delete' almost everytime)\n\n\nSome traffic could be sniffed by the \nModSecurity\n, and it will make a decision of what to do , whether block traffic completely (nevertheless notifying the logs \n) or just log info\\warning\\error.  \n\n\n\n\nModSecurity is an \nopen source\n, \ncross-platform\n web application firewall(WAF) module.\n\n\nIt enables web pplication defenders to gain visibility into HTTP(S) traffic and provides a power \nrules language\n and \nAPI\n to implement advanced protections.\n\n\n\n\nSo in case ModSec decided that traffic package has some malwarish things this request to the web server could be blocked preventing anything from happening and end user will get 503 error or something similar to it\n\n\nLogging an monitoring issues\n\n\nThe app itself could write logs, the instrumented app could write logs, and even apps running atof of main app could add logging\n\n\nLogs are generated by ModSecutiry, then are gathered by  File Beat and fed into Logstash, which applies filters to match particular fields of the log to be filtered by or displayed in Kibana.  \n\n\nBasically after ModSecurity is turned on and set up, it generates logs which could be viewed in ELK, containing all the info from where package came from, when, what it had, and in general full statistics could be gathered to understand whole picture.\n\n\n\n\nLogs Enreachment\n\n\n\n\nLike taking IP from the logs and adding where it is from (country) to add meaning and context fo the logged info, w/o which it is pretty useless.  \n\n\nTools \\ Links\n\n\n\n\nElastic Stack: Beats\n\n\nSqlmap\n - tool to check website db for various vulnurabilities  \n\n\nModSecurity\n  \n\n\nModSecurity & Logging demo\n  \n\n\nterraform+ansible+aws+apache+modsecurity\n\n\n\n\n\n\npresentation\n local place: img/devops_conf_2019/presentations/ModSecLogging.pdf\n\n\n\n\n\n\n[Nikita Procenko - Netflix] Infrastructure-as-code: bringing the gap between Devs and Ops\n\n\nthe talk mostly about approaches, and terraform:\n\n\nImmutability of infrastructure\n\n\nTODO: write down about immutability - easier to recreate instancec than patch it, to avoid regression and stuff\n\n\nDeclarative vs Imperative\n\n\nImperative\n approach is when everything is explicitly designed. This is the list of detailed instructions of HOW TO do something.  \n\n\n\n\nbash, aws cli\n\n\n\n\nDeclarative\n on the other hand is abstract form where only END STATE is given. Basically Declarative approah is just a wrapper around Imperative code. Imperative part here is incapsulated out of sight of regular user.\n\nTake Ansible - the only user do is state that Foo program need to be on the system. And Ansible will ensure it - install, update, or do nothing if Foo already existts.  \n\n\n\n\nterraform, ansible\n\n\n\n\nExample:\n\n\nImperative C#:\n\n\nList<int> collection = new List<int> { 1, 2, 3, 4, 5 };\n\n# Imperative: get only Odd numbers\nList<int> results = new List<int>();\nforeach(var num in collection)\n{\n    if (num % 2 != 0)\n          results.Add(num);\n}\n\n# The code below has detailed instructions of what to do step-by-step\n\n# Declarative: get only Odd numbers  \nvar results = collection.Where( num => num % 2 != 0);\n\n# or even more Declarative:\nfrom item in collection where item%2 != 0 select item\n\n\n\n\n\ndeclarative vs imperative is more about \ndeclaring what you want to have\n happen vs. explaining exactly \nhow it needs to occur\n.\n\n\n\n\nTerraform, the same as Ansible are the higher level abstraction tools that are Declarative.\n\nThe user only declares what he wants - tool inistalled or EC2 Instance created from specific image within specific subnet with a specific name etc. and the Tool will ensure for the User that this happened.\n\n\nBash scripts on the other hand are Imperative. \n\nWhere everything is explicitly and in a detailed way is explained to the interpreter which will go step by step doing what it is told, and at the and we need to ensure that everything is in place as we Wanted it.\n\n\nread more\n\n\nIaC Demo:\n\n\nhttps://github.com/iac-demo\n\n\n\n\nOther at all:\n\n\ntech interview presentation\n starting #10 slide",
            "title": "Devops conf 2019"
        },
        {
            "location": "/devops_conf_2019/#gianluca-abezzano-devops-never-sleeps-what-we-learned-from-influxdb-v1-to-v2",
            "text": "Docker is pretty layered system, which is split even more recently after ContainerD is moved into OpenSource by Docker development team.  The layers if Docker infrastructure are:   CLI  command line interface for accessing Docker daemon      REST  restfull interface for accessing daemon. TODO: check whether CLI also uses it    dockerd  docker service that accepts CLI and REST  calls and translates them further tocontainerd service(daemon)  Docker Engine    containerd  service that is moved into separate project. main purpose is to made this  Core  service to be fully crossplatform. \ncontainerd daemon exposes its API that is  used by dockerd and others parties interested in communication witn containerd in order to use services it provides.\nIt is currently used everywhere where containers and docker are  used - AWS, Azure and all the cloud providers that adopted docker as engine for containerization. \nAlso this is the core that runs on multiple OS such as: Linux, MacOS, Windows etc.      RunC  TODO: write RunC description, and probably add more layers..     TODO: which of above uses Linux Socket\\TCP Socket in order of communication with outer world.",
            "title": "[Gianluca Abezzano] DevOps never sleeps. What we learned from influxDB v1 to v2"
        },
        {
            "location": "/devops_conf_2019/#jenkins-and-k8s",
            "text": "Main point in usage of docker infrastructure by '3rd parties' is in acquiring access to docker socket. \nIt could be illustrated by starting docker container with docker(sub) running inside it. \nSub docker could manipulate parent docker if it has access to parent docker's socker, for instance by mounting socket file to sub docker's container:    docker run -v /var/run/docker.sock:/var/run/docker.sock -ti docker bash  in this example docker socket is passed from Host system into the container so docker running in the container could manipulate Host's one.  This is  how Jenkins plugin and k8s work.",
            "title": "Jenkins and k8s"
        },
        {
            "location": "/devops_conf_2019/#tracing-and-effective-logs-monitoring",
            "text": "In order to trace different important events out of all the system information, unimportant warnings and errors information need to be correctly logged, aggregated and filtered. \nMetrics need to be used and calculated, and tracking of events throught different logs need to be applied to that.   After all the logs are gathered, together with metrics and tracking applied to it, some particular thing could be Traced.  Metrics alone is just like slap to the face, it is sudden and only  has shock effect.  Logs in raw view are useless, the sence comes after aggregation and ability to filter it.",
            "title": "Tracing and effective logs monitoring"
        },
        {
            "location": "/devops_conf_2019/#state-of-the-system",
            "text": "Monitoring of the system could be done by monitoring its State:   Hardware charasteritsitcs like CPU usage  processes rununing and resource usage  I\\O activities   etc   But just snapshot is usually not enough.  \nLike 'ps -lte' - it could return actual and correct info but it will not gave any meaningful info in order which process works ineffectively or wrong. \nFor more efficient monitoring 'top' program will give much more info about processes and how they are utilizing system's resources. \nBut this gives realtime results and requres constant attention, it order to find which process does what. \nSo for effective monitor we need a tool that will save changes into history and ability to filter and even visualize the history.  But w/o any context even atual, filtered and visualized info makes no sence:   Normal State.   When everything in the system runs OK, and everybody is satisfied we could call it the Normal State of the system. \nThis state need to be saved with all the metrics calculated and attached, to be used later when need to determine how system works at given point of time.   Current State   At any point of time we could apply all the filters to logs, calculate metrics and visualize statistics (even of some time period). This will give us the Current State. \nComparing this  Current State  with a 'golden standard' like  Normal State  will give the answer to the main question:  Whether everything is fine with our system Now?",
            "title": "State of the system."
        },
        {
            "location": "/devops_conf_2019/#design-applications-correctly-in-order-to-monitor-them-later",
            "text": "In order to monitor system state effectively we need the info in first place. \nInfo is the logs generated by apps which state we want to monitor.    When Application is developed logs generation should be keep in mind. In ideal way app should has API to access logging functionality from outside by other systems:   [App [[Logs func + stuff ] API for logs access\\creation]]  in that way some logs\\events storage facility could use API to ask app for some particular logs an particular point of time.",
            "title": "Design applications correctly in order to monitor them later"
        },
        {
            "location": "/devops_conf_2019/#code-instrumentation-hello-bullseye",
            "text": "Existing code  also could be instrumented (i.e. during compilation)  Later on during runtime this added parts will fire and write logs\\events somewhere ( i.e. into some stream). Also this added part could provide API access for other tools.",
            "title": "Code Instrumentation (hello Bullseye)"
        },
        {
            "location": "/devops_conf_2019/#tools-links",
            "text": "ELK(B)  Prometheus(google)  Jaeger  bullseye. not related to article, just example  OpenCensus(google\\open source)  mount-bind docker socket",
            "title": "Tools \\ Links"
        },
        {
            "location": "/devops_conf_2019/#philipp-krenn-elastic-hands-on-modsecurity-and-logging",
            "text": "Basically this talk is about security breaches that could be logged and monitored uneffectively, thus alerting user too late and with lack of general information. \nTool that is demonstrated in order to prevent both breaches and bad monitoring is ModSecurity",
            "title": "[Philipp Krenn - Elastic] Hands-On ModSecurity and Logging"
        },
        {
            "location": "/devops_conf_2019/#security-issues",
            "text": "the link  There could be gaps in app with security  Gaps could be covered outside the app with tools, that filter out traffic analizing its inner parts, like requests (i.e. request having 'update' sql directive will be filtered out in some case, and 'delete' almost everytime)  Some traffic could be sniffed by the  ModSecurity , and it will make a decision of what to do , whether block traffic completely (nevertheless notifying the logs \n) or just log info\\warning\\error.     ModSecurity is an  open source ,  cross-platform  web application firewall(WAF) module.  It enables web pplication defenders to gain visibility into HTTP(S) traffic and provides a power  rules language  and  API  to implement advanced protections.   So in case ModSec decided that traffic package has some malwarish things this request to the web server could be blocked preventing anything from happening and end user will get 503 error or something similar to it",
            "title": "Security issues"
        },
        {
            "location": "/devops_conf_2019/#logging-an-monitoring-issues",
            "text": "The app itself could write logs, the instrumented app could write logs, and even apps running atof of main app could add logging  Logs are generated by ModSecutiry, then are gathered by  File Beat and fed into Logstash, which applies filters to match particular fields of the log to be filtered by or displayed in Kibana.    Basically after ModSecurity is turned on and set up, it generates logs which could be viewed in ELK, containing all the info from where package came from, when, what it had, and in general full statistics could be gathered to understand whole picture.   Logs Enreachment   Like taking IP from the logs and adding where it is from (country) to add meaning and context fo the logged info, w/o which it is pretty useless.",
            "title": "Logging an monitoring issues"
        },
        {
            "location": "/devops_conf_2019/#tools-links_1",
            "text": "Elastic Stack: Beats  Sqlmap  - tool to check website db for various vulnurabilities    ModSecurity     ModSecurity & Logging demo     terraform+ansible+aws+apache+modsecurity    presentation  local place: img/devops_conf_2019/presentations/ModSecLogging.pdf",
            "title": "Tools \\ Links"
        },
        {
            "location": "/devops_conf_2019/#nikita-procenko-netflix-infrastructure-as-code-bringing-the-gap-between-devs-and-ops",
            "text": "the talk mostly about approaches, and terraform:",
            "title": "[Nikita Procenko - Netflix] Infrastructure-as-code: bringing the gap between Devs and Ops"
        },
        {
            "location": "/devops_conf_2019/#immutability-of-infrastructure",
            "text": "TODO: write down about immutability - easier to recreate instancec than patch it, to avoid regression and stuff",
            "title": "Immutability of infrastructure"
        },
        {
            "location": "/devops_conf_2019/#declarative-vs-imperative",
            "text": "Imperative  approach is when everything is explicitly designed. This is the list of detailed instructions of HOW TO do something.     bash, aws cli   Declarative  on the other hand is abstract form where only END STATE is given. Basically Declarative approah is just a wrapper around Imperative code. Imperative part here is incapsulated out of sight of regular user. \nTake Ansible - the only user do is state that Foo program need to be on the system. And Ansible will ensure it - install, update, or do nothing if Foo already existts.     terraform, ansible   Example:  Imperative C#:  List<int> collection = new List<int> { 1, 2, 3, 4, 5 };\n\n# Imperative: get only Odd numbers\nList<int> results = new List<int>();\nforeach(var num in collection)\n{\n    if (num % 2 != 0)\n          results.Add(num);\n}\n\n# The code below has detailed instructions of what to do step-by-step\n\n# Declarative: get only Odd numbers  \nvar results = collection.Where( num => num % 2 != 0);\n\n# or even more Declarative:\nfrom item in collection where item%2 != 0 select item   declarative vs imperative is more about  declaring what you want to have  happen vs. explaining exactly  how it needs to occur .   Terraform, the same as Ansible are the higher level abstraction tools that are Declarative. \nThe user only declares what he wants - tool inistalled or EC2 Instance created from specific image within specific subnet with a specific name etc. and the Tool will ensure for the User that this happened.  Bash scripts on the other hand are Imperative.  \nWhere everything is explicitly and in a detailed way is explained to the interpreter which will go step by step doing what it is told, and at the and we need to ensure that everything is in place as we Wanted it.  read more",
            "title": "Declarative vs Imperative"
        },
        {
            "location": "/devops_conf_2019/#iac-demo",
            "text": "https://github.com/iac-demo   Other at all:  tech interview presentation  starting #10 slide",
            "title": "IaC Demo:"
        },
        {
            "location": "/interview/",
            "text": "Question: Do you have any question:\n\n\nMy Possible questions:\n\n\nLooks Good  \n\n\n\n\n\n\nthings\\skills that team is missing that manager is going to fill with me  \n\n\n\n\n\n\nare there documentation\n\n  how it is created and updated\n\n  how is it maintained up to date\n\n\n\n\n\n\nwhat is the current biggest point\\priority\n  what is the biggest challanges team is going to face in the next year\n\n\n\n\n\n\nwhat is the career growth paths in the company\n\n\n\n\n\n\nwhat does success look like for this position\n\n\n\n\n\n\nLooks normal  \n\n\n\n\n\n\nwhat in your opinion is DevOps \\ what DevOps is to them\n\n  how aligned the company with the said opinion\n\n\n\n\n\n\nhow team deals with production incidents\n\n\n\n\n\n\nwhat is favorite thing working here besides the peole\n\n\n\n\n\n\nwhat frsutrates you about working here\n\n\n\n\n\n\nGeneral questions:\n\n\n\n\nhow is performance feedback done\n\n  how is 1 on 1\n\n\n\n\nBetter not to ask:",
            "title": "Interview"
        },
        {
            "location": "/interview/#question-do-you-have-any-question",
            "text": "My Possible questions:  Looks Good      things\\skills that team is missing that manager is going to fill with me      are there documentation \n  how it is created and updated \n  how is it maintained up to date    what is the current biggest point\\priority\n  what is the biggest challanges team is going to face in the next year    what is the career growth paths in the company    what does success look like for this position    Looks normal      what in your opinion is DevOps \\ what DevOps is to them \n  how aligned the company with the said opinion    how team deals with production incidents    what is favorite thing working here besides the peole    what frsutrates you about working here    General questions:   how is performance feedback done \n  how is 1 on 1   Better not to ask:",
            "title": "Question: Do you have any question:"
        },
        {
            "location": "/networking_page/",
            "text": "DNS\n\n\nGuide\n\n\nDomain Name System - resolves human readable hostnames, such as www.example.com, into machine readable IP(Internet Protocol) addresses like 50.16.85.183.  \n\n\nAlso it is a directory of crucial information about domain names such as:  \n\n\n\n\nemail servers (MX records)  \n\n\nsending verification (DKIM, SPF, DMARC)  \n\n\nTXT record verification of domain ownership  \n\n\nSSH fingerprints (SSHFP)\n\n\n\n\nIntelligent DNS could even do something as Load Balancing - it could decide which IP(s) are returned after resolve.\n\n\nWork principles\n\n\nComputer sends requests every time it uses a domain address (example.com).\n\nThis happens every time domain name is used.\n\nFor any reason - web surfing, email, internet radio, API calls etc.\n\n\nSteps: In depth look for query\n\n\n\n\nLocal DNS cache. \n\n\n\n\nLocal computers(TODO: ensure), has \nDNS cache\n, where request could find hostname-IP resolution and this will be enough. If there is no local cache computer performs \nDNS query\n.\n\n\n\n\nRecursive DNS server. Resolver.\n\n\n\n\nUsually resolvers are somewhere in ISP's(internet service provider) network, but there could be even local resolver installed.\n\nResolvers also has their own cache, which is examined for query sent.\n\nIf hostname-IP is resolved(username checks out), this will be enough and results is sent back to the computer.\n\n\n\n\nRoot Name server. (.)\n\n\n\n\nResolver queries other resolvers(for cache) until it comes to the Root Name Servers, which are not cached already, so cache story ends here.\n\nThe Root Name servers forward query to the regular Name Servers, which know the address of the hostname - thus are able to do the resolution.\n\nThe Root name servers are located all around the world, this is kind of hubs on the traffic roads, usually Bunches located in huge cities like Capitals or just megacities.\n\nRoot Name servers are managed by \n13 companies\n such as ICANN or NASA.  \n\n\n\n\nTop Level Domain Name server. (.com. .us. .ua.)\n\n\n\n\nRoot server reads address from right to left, and depending on TLD in the address directs query to that Top Level Domain name server (Root NS knows addresses of all TLD Name servers).\n\nTLD NS knows reads the address queried, and reads next part like example in example.com, and this TLD NS knows address of every such domain (like \nexample\n.com).  \n\n\n\n\nAuthoritative DNS servers (Name Servers)\n\n\n\n\nAuthoritative name servers know everything about specific domain - it is stored in DNS records, such as A Record, CNAME and stuff.\nA Record is what we need, this will be returned.\n\n\n\n\nRetreive the record\n\n\n\n\nrecursive servers save request result from Name Server in its cache for amount of time set in TTL, after TTL expires recursive server will ask again to make sure info is up to date.\n\n\n\n\nReceive the anwser\n\n\n\n\nA record is returned back to the original asking computer by the Recursive server, answer is stored in local cache, reads IP address and passes it to browser. Browser opens the connection to receive the website.\n\n\nMAC addresses:\n\n\nhttps://www.howtogeek.com/169540/what-exactly-is-a-mac-address-used-for/\nhttp://www.linfo.org/mac_address.html\nhttps://www.youtube.com/watch?v=V2SpN-OePzc\n\n\nMAC is the uniq address of Network Interface Card.  \n\n\nMAC is lowest level in networking, below IP protocil.  \n\n\nMAC is used to send network packages, it is located in package header.  \n\n\nNIC will accept only those packages whose header contains NIC's MAC(matches).  \n\n\nRouting is done on IP level.\n\n\nWith MAC packages only could go inside the same network, if need to go through the Router the IP need to be used, wrapping package with MAC address.\n\n\nSteps:\n\n\nWHen computer wants to send a packet he checks whether target IP is in the same network. seems like ARP is used here, allows to ask all the computers in network (via Broadcast MAC ff:ff:ff:ff...) who has the IP address, only the computer with the IP will answer. \n\nAnswer will also contain the MAC address. This MAC will be written in the package and sent to that IP address (next hop). Info will be cached (as in DNS resolvers, seems like)\n\n\nIf IP is out of network, router receives the package with router's MAC in header and target IP address(not router's). Then Router check whether he could reach given target IP, if not, he sends package to next hot(router), and everything is done again.\n\n\nRouter receives packets for its own MAC, and for different(not router's) IP.\n\nThen checks whether he can directly reach the target IP, if not - passes to another router(hop)\n\n\nNAT  (Network Address Translation)\n\n\nhttp://www.internet-computer-security.com/Firewall/NAT.html\n\n\nThere are only 32 bits of addresses or 4 billions in IPv4.\n\nTo workaround it NAT is created, it introduces Private IP addresses.  \n\n\nPrivate IP addresses\n\n\nthere are 3 classes:\n\n\nClass A: 10.0.0.0 - 10.255.255.255\n\n\nClass B: 172.16.0.0 - 172.31.255.255\n\n\nClass C: 192.168.0.0 - 192.168.255.255 (or CIDR block \n192.168.0.0/16\n )\n\n\nWithin the private network - behind the Router(hop) the IP addresses would be unique. In another private network those addresses also would be unique, but could be the same in two different private networks.\n\nHowever as long as they stay private no conflicts appears.  \n\n\nRouters are the Gateways with real IP address purchased from the ISP (internet service provider). And this purchased address would be Public and unique.\n\nRouters are the 'real' sources for other services in the internet, and those services would address their packets only to Router, and Router will then translate (using routing tables) and substitute info in the package to send it to original sender inside the private network.\n\n\nNAT types:\n\n\nStatic NAT\n\n\nNAT device has a pool of Public ip addresses, and devices in network are assigned those addresses when accessing outside world. Static addresses will be given to the device permanently, so even when it goes offline the address still can not be taken by other device. It is useful for servers.\n\n\nDynamic NAT\n\n\nsame as above - pool of IP addresses which are given to the devices going outside, but when devices goes offline IP address is released and could be given to another device. When all the addresses are taken - new device could not be given any IP from the pool hence cant go to the internet.\n\n\nPort Address Translation (PAT)\n\n\nvideo\n\n\nPrivate address connects to a outer address with for instance:\n\n\n192.168.0.3:40213 connects to 40.30.20.10:80\n\n\n\nRouter substracts private ip and port and puts it in Routing Table\n\nand adds its own IP and another random port, changing the connection to:\n\n\n12.13.14.15:50098 connects to 40.30.20.10:80\n\n\n\nServer(or another same NAT router) receives connection, and answers to the Router:\n\n\n40.30.20.10:80 answers to 12.13.14.15:50098\n\n\n\nThen Router takes info from routing table, where matches router_address:random_port to the entry, and takes second pair with private_address:random_port\n\n\n40.30.20.10:80 answers to 192.168.0.3:40213\n\n\n\nSubnet Mask\n\n\nhttps://www.iplocation.net/subnet-mask\n\nhttps://en.wikipedia.org/wiki/Subnetwork  \n\n\ncalculator http://jodies.de/ipcalc\n\n\nProxy\n\n\nhttps://www.youtube.com/watch?v=0OukrSld3sY\n\n\n\n\nForward Proxy\n\n\nOr just Proxy, processes Outgoing requests. Computers in the network connects to it, and it connects to the servers.\n\nSee p4 proxy as example, with its own address, port - it accepts connections and does everything on its own - checks whether it needs to look furhter or not etc.\n\n\nUsual use:\n  - Content filtering:\n\n\n\n\ncensoring some addresses or resources(prevents traffic to coming in)\ntranslation some content before shipping it to computers inside network\n\n  - Caching (see p4 proxy)\n  - logging, monitoring  - what comes in, what comes out, etc.\n  - anonimization\n\nbecause not the computer bu proxy connects to server, it could ship different information\n\n\n\n\nReverse Proxy\n\n\nProcesses Incoming requests. Computers from outside world connects to the Proxy and not to the server itself, which is more secure, more stable(only 1 static address for outer world), more managable.\n\n\nUsual use:\n  - \nStable client endpoint\n whereas servers behind id could change addresses, cease to exist, turn on and off, scale and descale\n  - \nLoad balancing\n: Level 4 (udp/tcp traffic) & Level 8 (http traffic,with headers and other info for better balancing\n  - \nLoad balancing\n: server selection like Green\\Blue schamas or A\\B testing\n  - \nSSL Termination\n - when encryption ends on Proxy and internal traffic to servers and between internal servers is unencrypted and over http\n  - \nCaching\n - frequent requests could be cached w/o going to internal servers at all (during TTL ofc)\n  - \nAuthentication/validation\n - all the auth is done on the Proxy so no further auth is required.\n  - \nTenant throttling/billing\n - deny connection\\answer to computers that made too many requests per second. Or bill such customers, if we are charging by the amount\\frequency of requests done to the servers.\n  - \nDDoS mitigation\n - deny connection if suspicious about DDoS attack for some amount of time (1 hour or 1 day etc)\n\n\nReverse proxy examples:\n\n\nWebservers (such as Nginx)\n\nLoad Balancers\n\nAPI Gateways  \n\n\nMicroservices\n\n\nMonolith split into little parts which are independent one from each other.\nMicroservices have its own:\n\n - Data storage, and exclusive access to it for read and write\n - Technology stack, so different services could be written in different languages and use different storages and stuff  \n\n\nWhy to split:\n - Different technology stacks could be used for different services\n\n - Scalability - some services could be scaled to more instances than the others. Which means that less resources is needed comparing to split of monolith service, where some parts are overscaled and no need at the moment, but still scaled bcs of monolith structure\n\n - Different services(clients) could use and depend on one single service (see it as inheritance, when different classes could be inherited from single base class)\n - Versioning, some services could be upgraded with new featurs, but as long as they are backwards compatible other services could still use them w/o any additional changes\n - Conflicting version dependencies of shared libs in monolith services could be solved by splitting this monolith in microservices so different shared libs goes independent on different services\n\n\nAutoscaling instances\n\n\nOne of the holy grails Cloud apps is Autoscaling, which means changing amount of instances(of same service\\app) running at the same time.\n\n\nAutoscaling is performed automatically by 'orchestrator' - some cloud tool\n\nThere are couple of ways to autoscale:\n\n - Periodical Queue check: It has Queue which is in front of services, if amount of items in queue growth above some threshold for some period of time, the 'orchestrator' starts to scale instances up. When items in queue start to go down, 'orchestrator' will scale instances down.\n - Periodical Resource usage check: Instances could have some metrics being monitored by 'orchestrator' like CPU or RAM or HDD etc usage, and when those instances start to heat, some usage goes up, 'orchestrator' could decide to spin up another instance(scale up), and vice versa when resources goes low, 'orchestrator' could scale down some extra instances(keep n+1 amount anyway). \n\nLoad Balancer also could be used to make sure all the instances getting equialent load amount.\n - Scheduled: just some schedule related to day\\night, weekdays\\weekends\\holidays etc. As a downside - scheduled manner could not always be up to real world, and sometimes there could be too many or too less instances available\n\n\nMessaging\n\n\nMessaging communication \nreactivemanifesto.org\n\n\nBenefits over standard HTTP network calls (like GET/PUT/POST etc) which are converted from Method calls\n\n\nRequest\\Reply pattern Downsides:\n\n\n\n\n\n\nNetwork calls could be sent to a services(instance of a server) by load balancer, but particular service could already be busy doing work, and LB could not know about it.\n\nBut there could be other server instances that are not busy (and LB does not know about it)\n\n\n\n\n\n\nClient could go down - crash or scale down, while waiting for message request.  \n\n\n\n\n\n\nMessaging benefits:\n\n\n\n\n\n\nResource efficient - messages are taken from the queue by instance of a server which is done wokring, so all the instances have work and not overwhelmed with it. \n\n\n\n\n\n\nClient Services are not waiting longer than it could with Request-Reply pattern. So less chance to crash scale down before the answer.\n\n\n\n\n\n\nClient and Server are both talking to Queue service, which is always there and is solid - it wont go down crashing or descaling (actially it could be reverse proxy endpoint hiding queue brokers like zookeper one)\n\n\n\n\n\n\nResilient - if message from queue is taken and service which took it crashes - message will be returned to queue (in some magical way lol), and will be worked by with another service instance. \n\n\n\n\n\n\nQueue should implies idempotency (or Server rather?), when Queue could push out \nunordered\n messages multiple times, it is all need to be in Idempotent fashion.\n\n\n\n\n\n\nMessages are not lost even when consumer is fully offline. Messages will be stored in Queue until consumer comes back. This could be useful if consumer had a but, and was taken offline for fix, so new and fixed consumer now could handle all the messages.\n\n\n\n\n\n\nElastic - Orchestrator could use queue length to determine whether we need to scale  number of instances up or down.\n\n\n\n\n\n\nFault tolerant message processing\n\n\n\n\n\n\nService takes message from a queue. \n\n1.1 Message becomes invisible in the queue, but not deleted.\n\n1.2 Message becomes visible again in X seconds (amount of time expected to process the message)\n1.3 MessageDequeue counter is incremented, counter shows how many times message was dequeued(taken from the queue for processing)\n\n\n\n\n\n\nService takes message again if it is visible, and again increments counter. \n2.1 If counter is greater than threshold, it means the message is poisonous and we need to log it and delete it w/o processing, because it could crash the service(the reason why it has dequeue number over threshold)\n2.2 If message is processed fine the Service asks the Broker to delete the message.\n\n\n\n\n\n\nThis means that messages could be processed out of order(when retried), and could be even processed in parallel (if X seconds are not enough for processing at some point).\n\n\nWhich means that messages processing should be:  \n\n\n\n\nindependent (one message processed should not affect others)  \n\n\nidempotent (2+ message processings should have no ill effect)\n\n\n\n\nMessages features\n\n\n\n\n\n\nMultiple subscribers could receive one same message. So we need to send 1 message and 2+ subscribers could receive it.\n\n\n\n\n\n\nMessages could have TTL and when TTL passes message got deleted from the queue. This could save costs in cloud when we are billed for storing messages, and unprocessed messages could pile up (e.g. noone can process the message at the moment for some period of time)\n\n\n\n\n\n\nInvisibility timeout of message (see previous secion for X value).\n\nBut it should be taken carefully because in case of short X message could be processed second time \nin parallel\n\nAnd in case of too long X it could be invisible long time after unsuccessful processing being completed long ago\nAlso Service could increase or decrease the X which is stored in Broker.\n\n\n\n\n\n\nUpdate message in queue.\n\nIn case message processing is really long, and one Service processed it partially, it could go to Broker and update the message, so in case of crash second Service could not do that part of work which was done by the Service number 1. To do not waste the resources on double work. \nWhich still should be idempotent\n\n\n\n\n\n\nAt-most-once pattern\n\n\nThere is cases when message should be processed 1 or 0 times, it applies to data which is actual now and does not matter as time passes.\n\n\nThis could be accomplished by using TTL of the message, so it will be deleted by the Broker after the TTL expires, and TTL is the amount of time the data in the message \nmatters\n\nAdditionally to add 1 ir 0 times processing, time of message invisibility should be more than TTL, so when service takes message into processing, it will stay invisible long enough for Broker to delete it",
            "title": "Networking page"
        },
        {
            "location": "/networking_page/#dns",
            "text": "Guide  Domain Name System - resolves human readable hostnames, such as www.example.com, into machine readable IP(Internet Protocol) addresses like 50.16.85.183.    Also it is a directory of crucial information about domain names such as:     email servers (MX records)    sending verification (DKIM, SPF, DMARC)    TXT record verification of domain ownership    SSH fingerprints (SSHFP)   Intelligent DNS could even do something as Load Balancing - it could decide which IP(s) are returned after resolve.",
            "title": "DNS"
        },
        {
            "location": "/networking_page/#work-principles",
            "text": "Computer sends requests every time it uses a domain address (example.com). \nThis happens every time domain name is used. \nFor any reason - web surfing, email, internet radio, API calls etc.",
            "title": "Work principles"
        },
        {
            "location": "/networking_page/#steps-in-depth-look-for-query",
            "text": "Local DNS cache.    Local computers(TODO: ensure), has  DNS cache , where request could find hostname-IP resolution and this will be enough. If there is no local cache computer performs  DNS query .   Recursive DNS server. Resolver.   Usually resolvers are somewhere in ISP's(internet service provider) network, but there could be even local resolver installed. \nResolvers also has their own cache, which is examined for query sent. \nIf hostname-IP is resolved(username checks out), this will be enough and results is sent back to the computer.   Root Name server. (.)   Resolver queries other resolvers(for cache) until it comes to the Root Name Servers, which are not cached already, so cache story ends here. \nThe Root Name servers forward query to the regular Name Servers, which know the address of the hostname - thus are able to do the resolution. \nThe Root name servers are located all around the world, this is kind of hubs on the traffic roads, usually Bunches located in huge cities like Capitals or just megacities. \nRoot Name servers are managed by  13 companies  such as ICANN or NASA.     Top Level Domain Name server. (.com. .us. .ua.)   Root server reads address from right to left, and depending on TLD in the address directs query to that Top Level Domain name server (Root NS knows addresses of all TLD Name servers). \nTLD NS knows reads the address queried, and reads next part like example in example.com, and this TLD NS knows address of every such domain (like  example .com).     Authoritative DNS servers (Name Servers)   Authoritative name servers know everything about specific domain - it is stored in DNS records, such as A Record, CNAME and stuff.\nA Record is what we need, this will be returned.   Retreive the record   recursive servers save request result from Name Server in its cache for amount of time set in TTL, after TTL expires recursive server will ask again to make sure info is up to date.   Receive the anwser   A record is returned back to the original asking computer by the Recursive server, answer is stored in local cache, reads IP address and passes it to browser. Browser opens the connection to receive the website.",
            "title": "Steps: In depth look for query"
        },
        {
            "location": "/networking_page/#mac-addresses",
            "text": "https://www.howtogeek.com/169540/what-exactly-is-a-mac-address-used-for/\nhttp://www.linfo.org/mac_address.html\nhttps://www.youtube.com/watch?v=V2SpN-OePzc  MAC is the uniq address of Network Interface Card.    MAC is lowest level in networking, below IP protocil.    MAC is used to send network packages, it is located in package header.    NIC will accept only those packages whose header contains NIC's MAC(matches).    Routing is done on IP level.  With MAC packages only could go inside the same network, if need to go through the Router the IP need to be used, wrapping package with MAC address.",
            "title": "MAC addresses:"
        },
        {
            "location": "/networking_page/#steps",
            "text": "WHen computer wants to send a packet he checks whether target IP is in the same network. seems like ARP is used here, allows to ask all the computers in network (via Broadcast MAC ff:ff:ff:ff...) who has the IP address, only the computer with the IP will answer.  \nAnswer will also contain the MAC address. This MAC will be written in the package and sent to that IP address (next hop). Info will be cached (as in DNS resolvers, seems like)  If IP is out of network, router receives the package with router's MAC in header and target IP address(not router's). Then Router check whether he could reach given target IP, if not, he sends package to next hot(router), and everything is done again.  Router receives packets for its own MAC, and for different(not router's) IP. \nThen checks whether he can directly reach the target IP, if not - passes to another router(hop)",
            "title": "Steps:"
        },
        {
            "location": "/networking_page/#nat-network-address-translation",
            "text": "http://www.internet-computer-security.com/Firewall/NAT.html  There are only 32 bits of addresses or 4 billions in IPv4. \nTo workaround it NAT is created, it introduces Private IP addresses.",
            "title": "NAT  (Network Address Translation)"
        },
        {
            "location": "/networking_page/#private-ip-addresses",
            "text": "there are 3 classes:  Class A: 10.0.0.0 - 10.255.255.255  Class B: 172.16.0.0 - 172.31.255.255  Class C: 192.168.0.0 - 192.168.255.255 (or CIDR block  192.168.0.0/16  )  Within the private network - behind the Router(hop) the IP addresses would be unique. In another private network those addresses also would be unique, but could be the same in two different private networks. \nHowever as long as they stay private no conflicts appears.    Routers are the Gateways with real IP address purchased from the ISP (internet service provider). And this purchased address would be Public and unique. \nRouters are the 'real' sources for other services in the internet, and those services would address their packets only to Router, and Router will then translate (using routing tables) and substitute info in the package to send it to original sender inside the private network.",
            "title": "Private IP addresses"
        },
        {
            "location": "/networking_page/#nat-types",
            "text": "",
            "title": "NAT types:"
        },
        {
            "location": "/networking_page/#static-nat",
            "text": "NAT device has a pool of Public ip addresses, and devices in network are assigned those addresses when accessing outside world. Static addresses will be given to the device permanently, so even when it goes offline the address still can not be taken by other device. It is useful for servers.",
            "title": "Static NAT"
        },
        {
            "location": "/networking_page/#dynamic-nat",
            "text": "same as above - pool of IP addresses which are given to the devices going outside, but when devices goes offline IP address is released and could be given to another device. When all the addresses are taken - new device could not be given any IP from the pool hence cant go to the internet.",
            "title": "Dynamic NAT"
        },
        {
            "location": "/networking_page/#port-address-translation-pat",
            "text": "video  Private address connects to a outer address with for instance:  192.168.0.3:40213 connects to 40.30.20.10:80  Router substracts private ip and port and puts it in Routing Table \nand adds its own IP and another random port, changing the connection to:  12.13.14.15:50098 connects to 40.30.20.10:80  Server(or another same NAT router) receives connection, and answers to the Router:  40.30.20.10:80 answers to 12.13.14.15:50098  Then Router takes info from routing table, where matches router_address:random_port to the entry, and takes second pair with private_address:random_port  40.30.20.10:80 answers to 192.168.0.3:40213",
            "title": "Port Address Translation (PAT)"
        },
        {
            "location": "/networking_page/#subnet-mask",
            "text": "https://www.iplocation.net/subnet-mask \nhttps://en.wikipedia.org/wiki/Subnetwork    calculator http://jodies.de/ipcalc",
            "title": "Subnet Mask"
        },
        {
            "location": "/networking_page/#proxy",
            "text": "https://www.youtube.com/watch?v=0OukrSld3sY",
            "title": "Proxy"
        },
        {
            "location": "/networking_page/#forward-proxy",
            "text": "Or just Proxy, processes Outgoing requests. Computers in the network connects to it, and it connects to the servers. \nSee p4 proxy as example, with its own address, port - it accepts connections and does everything on its own - checks whether it needs to look furhter or not etc.  Usual use:\n  - Content filtering:   censoring some addresses or resources(prevents traffic to coming in)\ntranslation some content before shipping it to computers inside network \n  - Caching (see p4 proxy)\n  - logging, monitoring  - what comes in, what comes out, etc.\n  - anonimization \nbecause not the computer bu proxy connects to server, it could ship different information",
            "title": "Forward Proxy"
        },
        {
            "location": "/networking_page/#reverse-proxy",
            "text": "Processes Incoming requests. Computers from outside world connects to the Proxy and not to the server itself, which is more secure, more stable(only 1 static address for outer world), more managable.  Usual use:\n  -  Stable client endpoint  whereas servers behind id could change addresses, cease to exist, turn on and off, scale and descale\n  -  Load balancing : Level 4 (udp/tcp traffic) & Level 8 (http traffic,with headers and other info for better balancing\n  -  Load balancing : server selection like Green\\Blue schamas or A\\B testing\n  -  SSL Termination  - when encryption ends on Proxy and internal traffic to servers and between internal servers is unencrypted and over http\n  -  Caching  - frequent requests could be cached w/o going to internal servers at all (during TTL ofc)\n  -  Authentication/validation  - all the auth is done on the Proxy so no further auth is required.\n  -  Tenant throttling/billing  - deny connection\\answer to computers that made too many requests per second. Or bill such customers, if we are charging by the amount\\frequency of requests done to the servers.\n  -  DDoS mitigation  - deny connection if suspicious about DDoS attack for some amount of time (1 hour or 1 day etc)  Reverse proxy examples:  Webservers (such as Nginx) \nLoad Balancers \nAPI Gateways",
            "title": "Reverse Proxy"
        },
        {
            "location": "/networking_page/#microservices",
            "text": "Monolith split into little parts which are independent one from each other.\nMicroservices have its own: \n - Data storage, and exclusive access to it for read and write\n - Technology stack, so different services could be written in different languages and use different storages and stuff    Why to split:\n - Different technology stacks could be used for different services \n - Scalability - some services could be scaled to more instances than the others. Which means that less resources is needed comparing to split of monolith service, where some parts are overscaled and no need at the moment, but still scaled bcs of monolith structure \n - Different services(clients) could use and depend on one single service (see it as inheritance, when different classes could be inherited from single base class)\n - Versioning, some services could be upgraded with new featurs, but as long as they are backwards compatible other services could still use them w/o any additional changes\n - Conflicting version dependencies of shared libs in monolith services could be solved by splitting this monolith in microservices so different shared libs goes independent on different services",
            "title": "Microservices"
        },
        {
            "location": "/networking_page/#autoscaling-instances",
            "text": "One of the holy grails Cloud apps is Autoscaling, which means changing amount of instances(of same service\\app) running at the same time.  Autoscaling is performed automatically by 'orchestrator' - some cloud tool \nThere are couple of ways to autoscale: \n - Periodical Queue check: It has Queue which is in front of services, if amount of items in queue growth above some threshold for some period of time, the 'orchestrator' starts to scale instances up. When items in queue start to go down, 'orchestrator' will scale instances down.\n - Periodical Resource usage check: Instances could have some metrics being monitored by 'orchestrator' like CPU or RAM or HDD etc usage, and when those instances start to heat, some usage goes up, 'orchestrator' could decide to spin up another instance(scale up), and vice versa when resources goes low, 'orchestrator' could scale down some extra instances(keep n+1 amount anyway).  \nLoad Balancer also could be used to make sure all the instances getting equialent load amount.\n - Scheduled: just some schedule related to day\\night, weekdays\\weekends\\holidays etc. As a downside - scheduled manner could not always be up to real world, and sometimes there could be too many or too less instances available",
            "title": "Autoscaling instances"
        },
        {
            "location": "/networking_page/#messaging",
            "text": "Messaging communication  reactivemanifesto.org  Benefits over standard HTTP network calls (like GET/PUT/POST etc) which are converted from Method calls  Request\\Reply pattern Downsides:    Network calls could be sent to a services(instance of a server) by load balancer, but particular service could already be busy doing work, and LB could not know about it. \nBut there could be other server instances that are not busy (and LB does not know about it)    Client could go down - crash or scale down, while waiting for message request.      Messaging benefits:    Resource efficient - messages are taken from the queue by instance of a server which is done wokring, so all the instances have work and not overwhelmed with it.     Client Services are not waiting longer than it could with Request-Reply pattern. So less chance to crash scale down before the answer.    Client and Server are both talking to Queue service, which is always there and is solid - it wont go down crashing or descaling (actially it could be reverse proxy endpoint hiding queue brokers like zookeper one)    Resilient - if message from queue is taken and service which took it crashes - message will be returned to queue (in some magical way lol), and will be worked by with another service instance.     Queue should implies idempotency (or Server rather?), when Queue could push out  unordered  messages multiple times, it is all need to be in Idempotent fashion.    Messages are not lost even when consumer is fully offline. Messages will be stored in Queue until consumer comes back. This could be useful if consumer had a but, and was taken offline for fix, so new and fixed consumer now could handle all the messages.    Elastic - Orchestrator could use queue length to determine whether we need to scale  number of instances up or down.",
            "title": "Messaging"
        },
        {
            "location": "/networking_page/#fault-tolerant-message-processing",
            "text": "Service takes message from a queue.  \n1.1 Message becomes invisible in the queue, but not deleted. \n1.2 Message becomes visible again in X seconds (amount of time expected to process the message)\n1.3 MessageDequeue counter is incremented, counter shows how many times message was dequeued(taken from the queue for processing)    Service takes message again if it is visible, and again increments counter. \n2.1 If counter is greater than threshold, it means the message is poisonous and we need to log it and delete it w/o processing, because it could crash the service(the reason why it has dequeue number over threshold)\n2.2 If message is processed fine the Service asks the Broker to delete the message.    This means that messages could be processed out of order(when retried), and could be even processed in parallel (if X seconds are not enough for processing at some point).  Which means that messages processing should be:     independent (one message processed should not affect others)    idempotent (2+ message processings should have no ill effect)",
            "title": "Fault tolerant message processing"
        },
        {
            "location": "/networking_page/#messages-features",
            "text": "Multiple subscribers could receive one same message. So we need to send 1 message and 2+ subscribers could receive it.    Messages could have TTL and when TTL passes message got deleted from the queue. This could save costs in cloud when we are billed for storing messages, and unprocessed messages could pile up (e.g. noone can process the message at the moment for some period of time)    Invisibility timeout of message (see previous secion for X value). \nBut it should be taken carefully because in case of short X message could be processed second time  in parallel \nAnd in case of too long X it could be invisible long time after unsuccessful processing being completed long ago\nAlso Service could increase or decrease the X which is stored in Broker.    Update message in queue. \nIn case message processing is really long, and one Service processed it partially, it could go to Broker and update the message, so in case of crash second Service could not do that part of work which was done by the Service number 1. To do not waste the resources on double work.  Which still should be idempotent",
            "title": "Messages features"
        },
        {
            "location": "/networking_page/#at-most-once-pattern",
            "text": "There is cases when message should be processed 1 or 0 times, it applies to data which is actual now and does not matter as time passes.  This could be accomplished by using TTL of the message, so it will be deleted by the Broker after the TTL expires, and TTL is the amount of time the data in the message  matters \nAdditionally to add 1 ir 0 times processing, time of message invisibility should be more than TTL, so when service takes message into processing, it will stay invisible long enough for Broker to delete it",
            "title": "At-most-once pattern"
        }
    ]
}